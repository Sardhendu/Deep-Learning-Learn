{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports are doe Here:\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now as always we get the data we stored in the disk.\n",
    "cleaned_dataset_path = '/Users/sam/All-Program/App-DataSet/Deep-Learning/Datapreparation/dataset_cleaned.p'\n",
    "\n",
    "with open(cleaned_dataset_path, 'rb') as f:\n",
    "    fnl_dataset = pickle.load(f)\n",
    "    training_dataset = (fnl_dataset['training_dataset'])\n",
    "    training_labels = (fnl_dataset['training_labels'])\n",
    "    test_dataset = (fnl_dataset['test_dataset'])\n",
    "    test_labels = (fnl_dataset['test_labels'])\n",
    "    crossvalid_dataset = (fnl_dataset['crossvalid_dataset'])\n",
    "    crossvalid_labels = (fnl_dataset['crossvalid_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Cross Validation set (9810, 784) (9810, 10)\n",
      "Test set (7709, 784) (7709, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "no_of_labels = 10\n",
    "\n",
    "\n",
    "# tensor flow takes the labels input as binary code, where Alphabet A whose binary value is 0 will turn to a array\n",
    "# with elements [1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0] and B becomes [0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "def reshape_data(dataset, labels, sample_size=None):\n",
    "    aaaai = 0\n",
    "    if sample_size:\n",
    "        dataset = dataset[:sample_size].reshape(sample_size, image_size * image_size) # To reshape the  \n",
    "        # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "        labels = (np.arange(no_of_labels) == labels[:,None]).astype(np.float32)\n",
    "    else:\n",
    "        dataset = dataset.reshape(len(dataset), image_size * image_size) # To reshape the  \n",
    "        # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "        labels = (np.arange(no_of_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "# We just reshape the image so that 1 image defines a row\n",
    "\n",
    "# training_dataset[:].reshapeshape\n",
    "\n",
    "training_dataset_, training_labels_ = reshape_data(training_dataset, training_labels)\n",
    "crossvalid_dataset_, crossvalid_labels_ = reshape_data(crossvalid_dataset, crossvalid_labels)\n",
    "test_dataset_, test_labels_ = reshape_data(test_dataset, test_labels)\n",
    "print('Training set', training_dataset_.shape, training_labels_.shape)\n",
    "print('Cross Validation set', crossvalid_dataset_.shape, crossvalid_labels_.shape)\n",
    "print('Test set', test_dataset_.shape, test_labels_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(10000, 784), dtype=float32)\n",
      "[array([-0.44868904, -1.47481346, -0.29837644, -1.57433665, -0.72958857,\n",
      "       -1.429842  , -1.38927901,  1.21846449, -0.33064666, -0.46466389], dtype=float32), array([-0.76749641, -0.85480499,  1.02278805,  1.65393543,  1.28201938,\n",
      "       -0.37156007,  1.32641888, -0.98374391,  1.51335979,  0.2569578 ], dtype=float32), array([-0.6264593 , -0.1319638 ,  0.46596131,  1.13371265, -1.0247426 ,\n",
      "        0.89986873, -0.75644928,  0.42960027, -1.55639887, -0.89282936], dtype=float32)]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Now we build Our first Neural Net Using TensorFlow:\n",
    "sample_size = 10000\n",
    "\n",
    "   # Loading interactive Tensor flow session to print tesorflow objects\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "def start_session():\n",
    "    init = tf.initialize_all_variables()\n",
    "    session = tf.Session()\n",
    "    session.run(init)\n",
    "    return session\n",
    "    \n",
    "with graph.as_default():\n",
    "    # We load all the training data, test data and crossvalid data into the contants\n",
    "    tf_training_dataset = tf.constant(training_dataset_[:sample_size, :])\n",
    "    tf_training_labels = tf.constant(training_labels_[:sample_size, :])\n",
    "    tf_crossvalid_dataset = tf.constant(crossvalid_dataset_)\n",
    "    tf_crossvalid_labels = tf.constant(crossvalid_labels_)\n",
    "    tf_test_dataset = tf.constant(test_dataset_)\n",
    "    tf_test_labels = tf.constant(test_labels_)\n",
    "    \n",
    "    print (tf_training_dataset)\n",
    "    # Weight Initialization: In weight Initialization the weights are randomly initialized from a normal distribution\n",
    "    # One weight for each pixel and for each output label plus one 1 bais term.\n",
    "    weight_matrix = tf.Variable(tf.truncated_normal([image_size*image_size, no_of_labels]))\n",
    "    print ([w for no, w in enumerate(start_session().run(weight_matrix)) if no<=2])\n",
    "    biases = tf.Variable(tf.zeros([no_of_labels]))\n",
    "    print ([w for w in start_session().run(biases)])\n",
    "    \n",
    "    \n",
    "    # We have now obtained our random weights and x inputs, now lets train our model \n",
    "    # We multiply our weight to X's and add the baises term.\n",
    "    logit_fnc = tf.matmul(tf_training_dataset, weight_matrix) + biases\n",
    "    \n",
    "    # The next step after the logit function is to compute the softmax and then the perform the cross-entropy. \n",
    "    # In Tensor flow both the steps are achieved with a single function.\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logit_fnc, tf_training_labels))\n",
    "#     print ([w for no, w in enumerate(start_session().run(loss)) if no<=2])\n",
    "    \n",
    "    # Now we use Gradient Descet as our optimization technique to find the mnimum point\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "     \n",
    "    # The optimizer function findes the minimum point and so now we want to\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
