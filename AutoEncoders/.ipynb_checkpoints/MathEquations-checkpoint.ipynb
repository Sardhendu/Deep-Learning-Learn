{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NETWORK CONFIGURATION\n",
    "----------\n",
    "Imagine we have a total of 3 layers in the network and we have only one training input. We use a sigmoid activation units in the output layer. The range of values that a sigmoid would take is [0,1], so it is a good idea to scale out inputs in [0,1]. If we had used a tanh activation unit then it is wise to scale out input in [-1,1]. Note, we are not restricted to use sigmoid units or tanh units or any other bounding activation, we can even simply use a linear output unit. In that case scalling can be done any way we wish.   \n",
    "\n",
    "* **Input Layer (x)** has 3 units or three features ($x_1, x_2, x_3$) shape = 1x3\n",
    "* **Hidden Layer (h1) - sigmoid activation** has 2 units two latent features ($h_1, h_2$) shape = 1x2\n",
    "* **Output Layer (y) - sigmoid activation** has 3 units or three features ($y_1, y_2, y_3$) shape = 1x3\n",
    "* **Input to Output weights W1** shape = 3x2\n",
    "   * Weights ($w_1, w_4$) are the weights from $x_1$ to $h_1$ and $h_2$\n",
    "   * Weights ($w_2, w_5$) are the weights from $x_2$ to $h_1$ and $h_2$\n",
    "   * Weights ($w_3, w_6$) are the weights from $x_3$ to $h_1$ and $h_2$\n",
    "* **Hidden to Output weights W2** shape = 2x3\n",
    "   * Weights ($w_7, w_9, w_{11}$) are the weights from $h_1$ to $y_1$, $y_2$, $y_3$\n",
    "   * Weights ($w_8, w_{10}, w_{12}$) are the weights from $h_2$ to $y_1$, $y_2$, $y_3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTATION\n",
    "-------------\n",
    "* $x$ : Input Layer\n",
    "   * $x_1, x_2, x_3$ : Specific units of the input layer\n",
    "* $h1$ : Hidden Unit 1 , The derivates computed for $h1$ can be easily extended to any number of hidden units \n",
    "   * $h1_1, h1_2$ : Specific units of the hidden layer  \n",
    "* $y$ : Output Unit\n",
    "   * $y_1, y_2, y_3$ : Specific units of the output layer  \n",
    "* $z(h1)$ : Linear output of the hidden unit 1\n",
    "   * $z(h1_1), z(h1_2)$ : Specific Linear outputs of each hidden unit   \n",
    "* $out(h1)$ : Hidden State: Sigmoid output of the hidden unit 1\n",
    "   * $out(h1_1), out(h1_2)$ : Specific Sigmoid outputs of each hidden unit\n",
    "* $z(y)$ : Linear output of the output unit\n",
    "   * $z(y_1), z(y_2), z(y_3))$ : Specific Linear outputs of each output unit   \n",
    "* $out(y)$ : Output State: Sigmoid output of the output unit \n",
    "   * $out(y_1), out(y_2), out(y_3)$ : Specific Sigmoid outputs of each output unit\n",
    "* $b1$ : Bias to the hidden layer 1 \n",
    "   * $b1_1, b1_2$ : Specific Bias to each hidden units\n",
    "* $b2$ : Bias to the output layer 1 \n",
    "   * $b2_1, b2_2$ : Specific Bias to each output units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Note:\n",
    "-----------------\n",
    "When we continue with the forward propagation and the backward propagation, you'll see that we perform element wise matrix operation when we do computation on a particular layer such as the hidden layer or output layer. But when we do computation between two differnt we use dot products. This is an important thing to keep in mind because we we write the code to actually to the Foraward propagation and backward propagation, it will be easiler for us to do matrix manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FORWARD PROPAGATION:\n",
    "-------------------\n",
    "\n",
    "#### Hidden layer\n",
    " \n",
    "* $ z(h1) = x.W1 + b1$ : **shape = [1x3][3x2] + [1x2] = [1x2]**: *We perform dot product since we move from input layer to hidden layer*\n",
    "   * $ z(h1_1) = \\sum_{i=1}^3 w_ix_i +b1_i $   --> similar to doing a linear regression\n",
    "   * $ z(h1_1) = (w_1x_1+b1_1) + (w_2x_2+b1_2) + (w_3x_3+b1_3) $ \n",
    "   \n",
    "* $ out(h1) = \\frac{1}{1+\\exp^{-z(h1_1)}} = \\frac{1}{1+\\exp^{-(x.W1 + b1)}} $ : **shape = [1x2]**: *We perform element wise matrix division since we do the operation in the hidden layer.*\n",
    "   * $ out(h1_1) = \\frac{1}{1+\\exp^{z(h1_1)}} = \\frac{1}{1+\\exp^{- (w_1x_1+b1_1)+(w_2x_2+b1_2)+(w_3x_3+b1_3)}} $  --> Just Bounds $z(h_1)$ by probability\n",
    " \n",
    "*Adding a Regularizer:*\n",
    "   * $ reg1 = \\sum_{i\\subset{numHidWghts}} W1_i^2$\n",
    "   * $ reg1 = \\sum_{i=1}^{6} W_i^2$\n",
    "   \n",
    "#### Hidden to Output Layer\n",
    "\n",
    "* $ z(y) = out(h1).W2 + b2$ **shape = [1x2][2x3] + [1x3] = [1x3]**: *We perform dot product since we move from hidden layer to output layer*\n",
    "   * $ z(y_1) = (w_7out(h1_1)+b2_1) + (w_8out(h1_2)+b2_2) $\n",
    "\n",
    "* $ out(y) = \\frac{1}{1+\\exp^{-z(y)}} = \\frac{1}{1+\\exp^{-(out(h1).W2 + b2)}} $ *We perform element wise matrix division since we do the operation in the hidden layer.*\n",
    "   * $ out(y_1) = \\frac{1}{1+\\exp^{z(y_1)}} = \\frac{1}{1+\\exp^{-((w_7out(h1_1)+b2_1) + (w_8out(h1_2)+b2_2))}} $ \n",
    "\n",
    "*Adding a Regularizer:*\n",
    "   * $ reg2 = \\sum_{i\\subset{numOutWghts}} W2_i^2$\n",
    "   * $ reg2 = \\sum_{i=7}^{12} W2_i^2$\n",
    " \n",
    " Squared error: $E^{tot} = \\frac{1}{2} \\sum_{i=1}^3 (x_i - out(y_i))^2 $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COST FUNCTION:\n",
    "-----------------\n",
    "The $out(y)$ computed in the forward propagation is basically the estimation and now we would like to make this estimation as close to the actual output y. So we would like to minimize the sum of squared error between the two terms. When we are asked to minimize any function we do some thing like Gradient descent which includes derivatives. \n",
    "\n",
    "The cost function can be written as:\n",
    "\n",
    "$ J(w,b,x,y) = \\frac{1}{2m} \\sum_{j=1}^m (y_j - out(y_j))^2 + \\frac{\\lambda}{2} [\\sum_{i\\subset{numHidWghts}} W1_i^2 + \\sum_{i\\subset{numOutWghts}} W2_i^2] $\n",
    "\n",
    "$ J(w,b,x,y) = \\frac{1}{2*3} \\sum_{i=1}^3 ( - out(y_i))^2 + \\frac{\\lambda}{2} [\\sum_{i=1}^{6} W1_i^2 + \\sum_{i=7}^{12} W2_i^2] $\n",
    "\n",
    " The division by 2 in the regularization term is just given so that when we take the derivative of the regularization then the 2 in numerator will cancel out the 2 in denominator.\n",
    " \n",
    " Now we want to minimize cost $J(w,b,x,y)$ by making changes to the weights. So we want to change weights $W1$ and $W2$ in such a way that the cost function $J(w,b,x,y)$ is minimized at every iteration and reaches its minimum. Hence we take the derivative of the cost function w.r.t the Weights and we get the below. \n",
    " \n",
    "............................................................................................................................\n",
    "\n",
    " $ \\frac{d}{d(W1,W2)}J(w,b,x,y) = \\frac{1}{2m} \\sum_{j=1}^m \\frac{d}{d(W1,W2)} (y_j - out(y_j))^2 + \\frac{\\lambda}{2} [\\sum_{i\\subset{numHidWghts}} \\frac{d}{d(W1,W2)} W1_i^2 + \\sum_{i\\subset{numOutWghts}} \\frac{d}{d(W1,W2)} W2_i^2] $\n",
    " \n",
    " $ \\frac{d}{d(W1,W2)}J(w,b,x,y) = \\frac{1}{2m} \\sum_{j=1}^m \\frac{d}{d(W1,W2)} (J(W1,W2,b1.b2,x,y)) + \\lambda[\\sum_{i\\subset{numHidWghts}}W1_i + \\sum_{i\\subset{numOutWghts}}W2_i] $\n",
    " \n",
    "............................................................................................................................."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPUTING DERIVATIVES (GRADIENTS):\n",
    "------------------\n",
    "\n",
    "Let's for simplicity call out total error (cost) as $E_{tot}$. \n",
    "\n",
    "* $ \\frac{d}{d(out(y))} E_{tot} = \\frac{d}{d(out(y1))} \\frac{1}{2} \\sum_{j=1}^m (y_j - out(y_j))^2 $ \n",
    "  \n",
    "     $ = \\sum_{j=1}^m y_j - out(y_j)$\n",
    "     \n",
    "     $ = -(y - out(y)) $   (**shape = [1x3]-[1x3] = [1x3]** element wise subtraction)\n",
    " \n",
    "     For example:\n",
    "     $\\frac{1}{2} \\frac{d}{d(out(y_1))} (y_1 - out(y_1))^2 + (y_2 - out(y_2))^2 + (y_3 - out(y_3))^2) $\n",
    "     $= -(y_1 - out(y_1))$\n",
    "\n",
    "* $ \\frac{d}{d(z(y))} out(y) = \\frac{d}{d(-z(y))} (\\frac{1}{1+\\exp^{-z(y)}}) $\n",
    " \n",
    "     $ =  \\frac{d}{d(-z(y))} (1+\\exp^{-z(y)})^{-1} $\n",
    "     \n",
    "     $ =  -1*(1+\\exp^{-z(y)})^{-2} \\frac{d}{d(-z(y))} (1+\\exp^{-z(y)})$  \n",
    "     \n",
    "     $ = -1(1+\\exp^{z(y)})^{-2} \\exp^{-z(y)} $\n",
    "     \n",
    "     $ = \\frac{-\\exp^{-z(y)}}{(1-\\exp^{-z(y)})} * (\\frac{1}{1-\\exp^{-z(y}}) $\n",
    "     \n",
    "     $ = (1 - \\frac{1}{1-\\exp^{-z(y)}}) (\\frac{1}{1-\\exp^{-z(y)}}) $ \n",
    "     \n",
    "     $ = (1-out(y))out(y) $  (**shape = [1x3]x[1x3] = [1x3]  element wise multiplication**)\n",
    "     \n",
    "* $ \\frac{d}{d(out(h1))} z(y) = \\frac{d}{d(out(h1))} out(h1).W2 + b2$\n",
    "\n",
    "  $  = W2 $ (**shape = [2x3]**) \n",
    " \n",
    "     For example:\n",
    "     $\\frac{d}{d(out(h1_1))} z(y_1) = \\frac{d}{d(out(h1_1))} w_7out(h1_1)+b2_1) + (w_8out(h1_2)+b2_2) = w_7$\n",
    "     \n",
    "* $ \\frac{d}{d(z(h1))} out(h1) = \\frac{d}{d(z(h1))} (\\frac{1}{1+\\exp^{-z(h1)}}) $\n",
    " \n",
    "     $ =  \\frac{d}{d(z(h1))} (1+\\exp^{-z(h1)})^{-1} $\n",
    "     \n",
    "     $ =  -1(1+\\exp^{-z(h1)})^{-2} \\frac{d}{d(z(h1))} (1+\\exp^{-z(h1)})$  \n",
    "     \n",
    "     $ = -1(1+\\exp^{-z(h1)})^{-2} * \\exp^{-z(h1)} $\n",
    "     \n",
    "     $ = \\frac{-\\exp^{-z(h1)}}{(1-\\exp^{-z(h1)})} * (\\frac{1}{1-\\exp^{-z(h1}}) $\n",
    "     \n",
    "     $ = (1 - \\frac{1}{1-\\exp^{-z(h1)}}) (\\frac{1}{1-\\exp^{-z(h1)}}) $ \n",
    "     \n",
    "     $ = (1-out(h1))out(h1) $  (**shape = [1x2]x[1x2] = [1x2]  element wise multiplication**)\n",
    "     \n",
    "* $ \\frac{d}{d(x)} z(h1) = \\frac{d}{d(x)} x.W1 + b1$\n",
    "\n",
    "  $  = W1 $ (**shape = [3x2]**) \n",
    " \n",
    "     For example:\n",
    "     $\\frac{d}{d(x_1)} z(h_1) = \\frac{d}{d(x_1)} (w_1x_1+b1_1) + (w_2x2+b1_2) + (w_3x3+b2_3)  = w_1 $\n",
    "     \n",
    "\n",
    "* $ \\frac{d}{d(W2)} z(y) = \\frac{d}{d(W2)} out(h1).W2 + b2 $\n",
    "   \n",
    "  $ = out(h1) $ (**shape = [1x2]**)\n",
    "  \n",
    "* $ \\frac{d}{d(W1)} z(h1) = \\frac{d}{d(W1)} x.W1 + b1 $\n",
    "   \n",
    "  $ = x $ (**shape = [1x3]**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Backward Propagation:\n",
    "-----------------\n",
    " \n",
    "In Backward propagation all we do to compute the derivative term of the cost function which is $\\frac{d}{d(W1,W2)} (J(W1,W2,b1.b2,x,y))$. The concept of backward propogation is that how much the cost function changes while we change the weights a little bit. If the cost increases then we may need to decrease the weights by changing them in the negative direction and if the cost function decreases then we are already doing it correctly and we need to continue it for the following iteration.\n",
    "\n",
    "We will devide this section into 4 parts \n",
    "\n",
    "* Output layer gradients      : $\\delta{out(y)}$\n",
    "* Hidden layer gradients      : $\\delta{out(h1)}$\n",
    "* Computing Weight gradients  : $\\delta{W1}$\n",
    "* Computing new Weights       : $\\delta{W2}$\n",
    "\n",
    "\n",
    "For now onwards, We will build formulas for each section and then combine them to one last formula:\n",
    "\n",
    "\n",
    "### Output Layer Gradients:\n",
    "-------------------------\n",
    "\n",
    "* $\\delta{out(y)} = \\frac{d}{d(out(y))} E_{tot} * \\frac{d}{d(z(y)} out(y)$ \n",
    "     \n",
    "  $ = -(y - out(y)) * (1-out(y))out(y) $ \n",
    "  \n",
    "  **Shape= [1x3]x[1x3] = [1x3] element wise multiplication**\n",
    "  \n",
    "### Hidden layer gradients:\n",
    "--------------------\n",
    "\n",
    "* $\\delta{out(h1)} = [\\frac{d}{d(out(y))} E_{tot} * \\frac{d}{d(z(y)}out(y) * \\frac{d}{d(out(h1))} z(y) * \\frac{d}{d(z(h1))} out(h1)] $ \n",
    "     \n",
    "     * $ frac{d}{d(out(y))} E_{tot} * \\frac{d}{d(z(y)}out(y) = -(y - out(y)) * (1-out(y))out(y) $   \n",
    "  \n",
    "       **Shape : [1x3]**\n",
    "  \n",
    "     * $ \\frac{d}{d(out(h1))} z(y) = W2 $ Here we go from one layer to another i.e the gradient is computed betwee outptu layer and hidden layer, so we nee to do a dot product.\n",
    "  \n",
    "        **W2 Shape : [2x3]**\n",
    "        \n",
    "     * $ \\frac{d}{d(z(h1))} out(h1) = (1-out(h1))out(h1) $\n",
    "         \n",
    "        **Shape = [1x2]**\n",
    "  \n",
    "  = $ [-(y - out(y)) * (1-out(y))out(y)].W2^T * (1-out(h1))out(h1) $ \n",
    "    \n",
    "    **shape = [[1x3]x[1x3]]x[3x2] x [1x2] = [1x2] both dot product and element wise multiplication**\n",
    "    \n",
    "    \n",
    "### Computing Weight gradients inclusing regularization because regularization contains weight terms:\n",
    "---------------\n",
    "This computes the change in the total error term when their is a small change in the weights\n",
    "\n",
    "* $\\delta{W2} = \\frac{d}{d(W2)} E_{tot} $\n",
    "\n",
    "  $ = \\frac{d}{d(out(y))} E_{tot} * \\frac{d}{d(z(y)} out(y) * \\frac{d}{d(W2)} z(y)$ \n",
    "  \n",
    "  $ = \\delta{out(y)} * \\frac{d}{d(W2)} z(y)$ \n",
    "\n",
    "   * $\\delta{out(y)} = -(y - out(y)) * (1-out(y))out(y)$ **shape=[1x3]**\n",
    "   * $\\frac{d}{d(W2)} z(y) = out(h1)$ **shape= [1x2]**\n",
    "   \n",
    "   $ =  (out(h1))^T.[-(y - out(y)) * (1-out(y))out(y)]$ \n",
    "   \n",
    "   **Shape= [2x1]x[1x3] = [2x3]**\n",
    "   \n",
    "* $\\delta{W1} = \\frac{d}{d(W1)} E_{tot} $\n",
    "\n",
    "  $ = [\\frac{d}{d(out(y))} E_{tot} * \\frac{d}{d(z(y)}out(y) * \\frac{d}{d(out(h1))} z(y) * \\frac{d}{d(z(h1))} out(h1)] * \\frac{d}{d(W1)} z(h1)$ \n",
    "  \n",
    "  $ = \\delta{out(y)} * \\delta{out(h1)} * \\frac{d}{d(W1)} z(h1)$ \n",
    "\n",
    "   * $\\delta{out(h1)} z(y) = [-(y - out(y)) * (1-out(y))out(y)].W2^T * (1-out(h1))out(h1)$ **shape= [1x2]**\n",
    "   * $\\frac{d}{d(W1)} z(h1) = x $ **shape= [1x3]**\n",
    "   \n",
    "   $ =  x^T.[[-(y - out(y)) * (1-out(y))out(y)].W2^T * (1-out(h1))out(h1)]$ \n",
    "   \n",
    "   **Shape= [3x1]x[1x2]= [3x2] **\n",
    "   \n",
    "In the actual cost function we also remember having a $\\frac{1}{m}$ which sort of takes the average of all the error and we also want to do the derivate for the regularization term. Using the Cost function from above we can write the weight gradients as:\n",
    "\n",
    "* $ reg2 = \\frac{d}{d(W2)} \\frac{\\lambda}{2} [\\sum_{i\\subset{numHidWghts}} W1_i^2 + \\sum_{i\\subset{numOutWghts}} W2_i^2] $\n",
    "\n",
    "   * $ = \\lambda W2$\n",
    "\n",
    "* $ reg1 = \\frac{d}{d(W2)} \\frac{\\lambda}{2} [\\sum_{i\\subset{numHidWghts}} W1_i^2 + \\sum_{i\\subset{numOutWghts}} W2_i^2] $\n",
    "\n",
    "   * $ = \\lambda W2$\n",
    "\n",
    "#### $ \\delta{W2} = \\frac{1}{m} (out(h1))^T.[-(y - out(y)) * (1-out(y))out(y)] +  \\lambda W2  $    (**shape = [2x3 ]+[2x3] = [2x3]**)\n",
    "#### $ \\delta{W1} = \\frac{1}{m} (x^T.[[-(y - out(y)) * (1-out(y))out(y)].W2^T * (1-out(h1))out(h1)] +  \\lambda W2  $   (**shape = [3x2]+[3x2]=[3x2]]**)\n",
    "\n",
    "### Compute New Weights:\n",
    "----------------\n",
    "\n",
    "$ W2_new = W2 - \\alpha*\\delta{W2} $\n",
    "\n",
    "$ W1_new = W1 - \\alpha*\\delta{W1} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " \n",
    "## Sparse Encoding:\n",
    "--------------------\n",
    "\n",
    "Let us first write the Vanila Neural Network Formulas we saw previously:\n",
    "\n",
    "#### Cost Function:\n",
    "\n",
    "$ cost = \\frac{1}{2} \\sum_{i=1}^3 (x_i - out(y_i))^2 + \\frac{\\lambda}{2} [\\sum_{i=1}^{6} W_i^2 + \\sum_{i=7}^{12} W_i^2] $\n",
    "\n",
    "**What basically is a sparse encoding?**\n",
    "\n",
    "Autoencoders can be thaught of as a factor model, where the latent features are the hidden layer of a neural net. For an autoencoder the input and the output are the same because its a unsupervised approach where we do not have any labels. The idea of the autoencoder is pretty similar to PCA (Proncipal component analysis) where we find the eigen vectors that explains most of the variation in th data set. In autoencoders the hidden unit can be thaught of as eigen vectors. In short, while running the network using forward and backward propagation, we would like our hidden units to learn interesting factors about the input data. \n",
    "\n",
    "A typical case could be where we have n inputs and n hidden units and n output. We would not want such a case, because if thats the case then after many iteration of forward and backward propagation we might reach a scenario where the hidden layers have completly learned the input representation. So. in the real world when we get a training point that comes from a very different distribution from ones on which the network was trained, the model will not do well for such cases. The typical problem of \"Overfitting\". So we want our model to learn well but no learn completely.\n",
    "\n",
    "Sparse Encoding typically mean that for a given training input we would force many of our hidden unit to be closer to zero (no active). In this case not every unit will learn about the input features, some units will learn about input features while other will learn about different features. \n",
    "\n",
    "\n",
    "**How do we force out unit activation to be as close as zero**\n",
    "\n",
    "We introduce a term called as $\\rho$ and set $\\rho$ = 0.05. Let's say that the output of the hidden unit is $\\rho^{hat}$. What we would like to do here is to make $\\rho^{hat}$ diverge less from $\\rho$. In simple words we want the average activation of the hidden layer $\\rho^{hat}$ to be closer to rho. When we do so, we basically say that only few units (ones that have very high activation also the ones that are more adept in caturing the complexities in the input feature) are allowed to be greater than 0 or rather active. \n",
    "\n",
    "Imagine rho and $\\rho^{hat}$ being two different distribution and we want calculate the divergence between them. So we use KL divergence to compute the divergence. Now we know how to compute the divergence but what do we do with this. We want to minimize this divergence for rho_hat to be closer to rho, why not add the KL divergence term to the cost function and minimize the cost function as we do for squared error.\n",
    "\n",
    "Sparsity term KL divergence = $ \\sum_{j=1}^{numUnits} \\rho log\\frac{\\rho}{\\rho^{hat}} + (1-\\rho)log\\frac{1-\\rho}{1-\\rho^{hat}}$\n",
    "\n",
    "\n",
    "#### New Cost Function after adding the KL divergence term:\n",
    "\n",
    "cost = $\\frac{1}{2} \\sum_{i=1}^3 (x_i - out(y_i))^2 + \\frac{\\lambda}{2} [\\sum_{i=1}^{6} W_i^2 + \\sum_{i=7}^{12} W_i^2] + \\sum_{j=1}^{numUnits=2} \\rho log\\frac{\\rho}{\\rho^{hat}} + (1-\\rho)log\\frac{1-\\rho}{1-\\rho^{hat}}$\n",
    "\n",
    "\n",
    "This part is only meant while using a sparse encoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-60b725f10c9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
