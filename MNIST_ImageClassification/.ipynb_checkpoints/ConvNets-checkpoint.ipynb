{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports the packages Here\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (200000, 28, 28) (200000,)\n",
      "Cross Validation: (9810, 28, 28) (9810,)\n",
      "Testing: (7709, 28, 28) (7709,)\n"
     ]
    }
   ],
   "source": [
    "# Here we make a call to the dataset and retrieve the training, valid and test data.\n",
    "cleaned_dataset_path = '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/MNIST_ImageClassification/DataPreparation/dataset_cleaned.p'\n",
    "\n",
    "with open(cleaned_dataset_path, 'rb') as f:\n",
    "    fnl_dataset = pickle.load(f)\n",
    "    training_dataset = (fnl_dataset['training_dataset'])\n",
    "    training_labels = (fnl_dataset['training_labels'])\n",
    "    test_dataset = (fnl_dataset['test_dataset'])\n",
    "    test_labels = (fnl_dataset['test_labels'])\n",
    "    crossvalid_dataset = (fnl_dataset['crossvalid_dataset'])\n",
    "    crossvalid_labels = (fnl_dataset['crossvalid_labels'])\n",
    "    \n",
    "print('Training:', training_dataset.shape, training_labels.shape)\n",
    "print('Cross Validation:', crossvalid_dataset.shape, crossvalid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "1. For a convolution Neural Network we need a Tensor of shape (batchSize, imageSize, imageSize, numChannels). For a gray scale image the numChannels would be 1 for a RBG it would be 3. The below code will convert the input array into the tensor of said shape. The numChannels would even work without having it. But we have it because it will help us do the matrix multiplication with the specified depth (kernel) for the convolutional filter. Plus Tensor flow requires it this way.\n",
    "\n",
    "\n",
    "2. For labels Tensor flow takes the labels as binary input where 1 indicated the activation of the class of the training instance and 0 for all other class. For example Alphabet A whose binary value is 0 will turn to an array with elements [1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0] and B becomes [0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28, 1) (200000, 10)\n",
      "Cross Validation set (9810, 28, 28, 1) (9810, 10)\n",
      "Test set (7709, 28, 28, 1) (7709, 10)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the data, the input is in the form of tensor of shape [200000x28x28], Here we convert them into a single row\n",
    "imageSize = 28\n",
    "numChannels = 1 # For Grayscale image.\n",
    "numLabels = 10\n",
    "\n",
    "def reshape_data(dataset, labels, sample_size=None):\n",
    "    if sample_size:\n",
    "        dataset = dataset[:sample_size].reshape((-1,imageSize,imageSize,numChannels)) # To reshape the\n",
    "        # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "        labels = (np.arange(numLabels) == labels[:,None]).astype(np.float32)\n",
    "    else:\n",
    "        dataset = dataset.reshape((-1,imageSize,imageSize,numChannels)) # To reshape the  \n",
    "        # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "        labels = (np.arange(numLabels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "# We just reshape the image so that 1 image defines a row\n",
    "\n",
    "# training_dataset[:].reshapeshape\n",
    "trainData_, trainLabels_ = reshape_data(training_dataset, training_labels)\n",
    "validData_, validLabels_ = reshape_data(crossvalid_dataset, crossvalid_labels)\n",
    "testData_, testLabels_ = reshape_data(test_dataset, test_labels)\n",
    "print('Training set', trainData_.shape, trainLabels_.shape)\n",
    "print('Cross Validation set', validData_.shape, validLabels_.shape)\n",
    "print('Test set', testData_.shape, testLabels_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_graph():  # Reset the graph\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BuildConvNet():\n",
    "    def __init__(self):\n",
    "        self.batchSize = 128\n",
    "        self.imageSize = 28\n",
    "        self.numLabels = 10\n",
    "        self.numChannel = 1              # numChannels -> For grayscale =1, for RGB =3\n",
    "        self.numHidden = 1000            # numHiddenUnits for the fully connected layer\n",
    "\n",
    "        self.conv1Kernel = 5             # Size of kernel for the first convolution layer\n",
    "        self.conv2Kernel = 5             # Size of kernel for the second convolution layer\n",
    "        self.conv1Depth = 32             # Number of kernels for the first convolution layer\n",
    "        self.conv2Depth = 64             # Number of kernels for the second convolution layer\n",
    "        self.conv1Stride = 1             # Strides for the first convolution layer filter  \n",
    "        self.conv2Stride = 1             # Strides for the second convolution layer filter\n",
    "\n",
    "        self.pool1Kernel = 2             # Size of kernel for the first Pooling layer\n",
    "        self.pool2Kernel = 2             # Size of kernel for the second Pooling layer  \n",
    "        self.pool1Stride = 2             # Strides for the first Pooling layer filter\n",
    "        self.pool2Stride = 2             # Strides for the second Pooling layer filter\n",
    "        \n",
    "#         self.dropout = 0.75               # Drop out a weight with a probability of 0.5\n",
    "\n",
    "        self.alpha = 0.001               # The learning Rate, A high Learning Rate will would be inefficient. If using higher\n",
    "                                         # learning rate then use regularizer else, the model will overfitt\n",
    "        self.m = 0.9\n",
    "        \n",
    "        self.weights = {\n",
    "            'cv1_wght': tf.Variable(tf.random_normal([self.conv1Kernel, self.conv1Kernel, self.numChannel, self.conv1Depth])),\n",
    "            'cv2_wgth': tf.Variable(tf.random_normal([self.conv2Kernel, self.conv2Kernel, self.conv1Depth, self.conv2Depth])),\n",
    "            'out_wght': tf.Variable(tf.random_normal([self.numHidden, self.numLabels]))\n",
    "        }\n",
    "\n",
    "        self.biases = {\n",
    "            'cv1_bias': tf.Variable(tf.random_normal([self.conv1Depth])),\n",
    "            'cv2_bias': tf.Variable(tf.random_normal([self.conv2Depth])),\n",
    "            'fc1_bias': tf.Variable(tf.random_normal([self.numHidden])),\n",
    "            'out_bias': tf.Variable(tf.random_normal([self.numLabels]))\n",
    "        }\n",
    "\n",
    "\n",
    "    def convLayer(self, x, w, b, s=1, nlModel='RELU'):\n",
    "        x = tf.nn.conv2d(x, w, [1,s,s,1], padding='SAME')\n",
    "        x = tf.nn.bias_add(x, b)\n",
    "        if nlModel == 'RELU':\n",
    "            return tf.nn.relu(x)\n",
    "        elif nlModel == 'LOGIT':\n",
    "            return tf.sigmoid(x)\n",
    "\n",
    "\n",
    "    def poolLayer(self, x, k=2, s=2, poolType='MAX'):\n",
    "        if poolType=='MAX':\n",
    "            return tf.nn.max_pool(x, ksize=[1,k,k,1], strides=[1,s,s,1], padding='SAME')\n",
    "        elif poolType == 'AVG':\n",
    "            return tf.nn.avg_pool(data, ksize=[1,k,k,1], strides=[1,s,s,1], padding='SAME')\n",
    "\n",
    "        \n",
    "    def fcLayer(self, x, w, b, nModel='RELU'):\n",
    "        x = tf.matmul(x, w) + b\n",
    "        if nModel=='RELU':\n",
    "            return tf.nn.relu(x)\n",
    "        if nModel=='LOGIT':\n",
    "            return tf.sigmoid(x)\n",
    "        \n",
    "        \n",
    "    def outputLayer(self, x, w, b):\n",
    "        x = tf.matmul(x, w) + b\n",
    "        return x, tf.nn.softmax(x)\n",
    "            \n",
    "                         \n",
    "    # Create the entire Model\n",
    "    def conv_net(self):\n",
    "        trainData = tf.placeholder(tf.float32, [None, self.imageSize, self.imageSize, self.numChannel])\n",
    "        trainLabels = tf.placeholder(tf.float32, [None, self.numLabels])\n",
    "\n",
    "\n",
    "        # Convolution Layer 1\n",
    "        conv1 = self.convLayer(trainData, self.weights['cv1_wght'], self.biases['cv1_bias'], s=self.conv1Stride)\n",
    "        conv1 = self.poolLayer(conv1, k=self.pool1Kernel)\n",
    "\n",
    "        # Convolution Layer 2\n",
    "        conv2 = self.convLayer(conv1, self.weights['cv2_wgth'], self.biases['cv2_bias'], self.conv2Stride)\n",
    "        conv2 = self.poolLayer(conv2, k=self.pool2Kernel)\n",
    "        poolSize = conv2.get_shape().as_list()[1]\n",
    "\n",
    "        # Fully connected layer : Reshape conv2 output to fit fully connected layer input\n",
    "        flattenedPoolSize = poolSize*poolSize*self.conv2Depth\n",
    "        self.weights['fc1_wght'] =  tf.Variable(tf.random_normal([flattenedPoolSize, self.numHidden]))\n",
    "        fc1State = tf.reshape(conv2, [-1, flattenedPoolSize])\n",
    "        fc1State = self.fcLayer(fc1State, self.weights['fc1_wght'], self.biases['fc1_bias'])\n",
    "\n",
    "        # Using DropOut Regularizer\n",
    "#         fc1State = tf.nn.dropout(fc1State, self.dropout)\n",
    "\n",
    "        # Output, class prediction\n",
    "        pred, outputState = self.outputLayer(fc1State, self.weights['out_wght'], self.biases['out_bias'])\n",
    "        lossCE = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=trainLabels))\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.alpha).minimize(lossCE)\n",
    "#         optimizer = tf.train.MomentumOptimizer(self.alpha, \n",
    "#                                             self.m, \n",
    "#                                             use_locking=False, \n",
    "#                                             name='Momentum', \n",
    "#                                             use_nesterov=False).minimize(lossCE)\n",
    "\n",
    "        # Evaluate model\n",
    "        correct_pred = tf.equal(tf.argmax(outputState, 1), tf.argmax(trainLabels, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "        \n",
    "        return dict(\n",
    "            trainData=trainData, \n",
    "            trainLabels=trainLabels, \n",
    "            optimizer=optimizer, \n",
    "            lossCE=lossCE, \n",
    "            accuracy=accuracy\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1280, Minibatch Loss= 34799.027344, Training Accuracy= 0.19531\n",
      "Iter 2560, Minibatch Loss= 11949.522461, Training Accuracy= 0.45312\n",
      "Iter 3840, Minibatch Loss= 10554.169922, Training Accuracy= 0.57812\n",
      "Iter 5120, Minibatch Loss= 9428.657227, Training Accuracy= 0.64062\n",
      "Iter 6400, Minibatch Loss= 8560.914062, Training Accuracy= 0.68750\n",
      "Iter 7680, Minibatch Loss= 7913.112793, Training Accuracy= 0.66406\n",
      "Iter 8960, Minibatch Loss= 8458.326172, Training Accuracy= 0.66406\n",
      "Iter 10240, Minibatch Loss= 6793.362305, Training Accuracy= 0.72656\n",
      "Iter 11520, Minibatch Loss= 5856.997070, Training Accuracy= 0.76562\n",
      "Iter 12800, Minibatch Loss= 7365.856445, Training Accuracy= 0.70312\n",
      "Iter 14080, Minibatch Loss= 4816.152344, Training Accuracy= 0.79688\n",
      "Iter 15360, Minibatch Loss= 4008.965332, Training Accuracy= 0.80469\n",
      "Iter 16640, Minibatch Loss= 4870.905273, Training Accuracy= 0.78125\n",
      "Iter 17920, Minibatch Loss= 5972.165527, Training Accuracy= 0.74219\n",
      "Iter 19200, Minibatch Loss= 4097.473633, Training Accuracy= 0.76562\n",
      "Iter 20480, Minibatch Loss= 5758.646484, Training Accuracy= 0.70312\n",
      "Iter 21760, Minibatch Loss= 6074.434570, Training Accuracy= 0.79688\n",
      "Iter 23040, Minibatch Loss= 5008.565918, Training Accuracy= 0.77344\n",
      "Iter 24320, Minibatch Loss= 5391.771484, Training Accuracy= 0.78906\n",
      "Iter 25600, Minibatch Loss= 6404.445312, Training Accuracy= 0.71094\n",
      "Iter 26880, Minibatch Loss= 4163.135742, Training Accuracy= 0.77344\n",
      "Iter 28160, Minibatch Loss= 4867.678223, Training Accuracy= 0.79688\n",
      "Iter 29440, Minibatch Loss= 4511.546875, Training Accuracy= 0.71875\n",
      "Iter 30720, Minibatch Loss= 2866.852539, Training Accuracy= 0.82031\n",
      "Iter 32000, Minibatch Loss= 4425.925781, Training Accuracy= 0.78906\n",
      "Iter 33280, Minibatch Loss= 4670.867188, Training Accuracy= 0.70312\n",
      "Iter 34560, Minibatch Loss= 2811.868652, Training Accuracy= 0.78906\n",
      "Iter 35840, Minibatch Loss= 5806.003906, Training Accuracy= 0.71875\n",
      "Iter 37120, Minibatch Loss= 3394.519531, Training Accuracy= 0.78125\n",
      "Iter 38400, Minibatch Loss= 2710.634766, Training Accuracy= 0.80469\n",
      "Iter 39680, Minibatch Loss= 3791.210693, Training Accuracy= 0.78906\n",
      "Iter 40960, Minibatch Loss= 3128.906250, Training Accuracy= 0.78906\n",
      "Iter 42240, Minibatch Loss= 3542.209473, Training Accuracy= 0.77344\n",
      "Iter 43520, Minibatch Loss= 3989.032959, Training Accuracy= 0.78125\n",
      "Iter 44800, Minibatch Loss= 4201.449219, Training Accuracy= 0.80469\n",
      "Iter 46080, Minibatch Loss= 4565.369629, Training Accuracy= 0.80469\n",
      "Iter 47360, Minibatch Loss= 3830.440430, Training Accuracy= 0.75000\n",
      "Iter 48640, Minibatch Loss= 3480.465820, Training Accuracy= 0.76562\n",
      "Iter 49920, Minibatch Loss= 4108.283203, Training Accuracy= 0.76562\n",
      "Iter 51200, Minibatch Loss= 4972.566406, Training Accuracy= 0.75000\n",
      "Iter 52480, Minibatch Loss= 3316.342773, Training Accuracy= 0.79688\n",
      "Iter 53760, Minibatch Loss= 3503.546387, Training Accuracy= 0.76562\n",
      "Iter 55040, Minibatch Loss= 2093.952148, Training Accuracy= 0.82031\n",
      "Iter 56320, Minibatch Loss= 3474.315186, Training Accuracy= 0.78125\n",
      "Iter 57600, Minibatch Loss= 2670.303955, Training Accuracy= 0.82031\n",
      "Iter 58880, Minibatch Loss= 2202.887939, Training Accuracy= 0.81250\n",
      "Iter 60160, Minibatch Loss= 2551.411133, Training Accuracy= 0.81250\n",
      "Iter 61440, Minibatch Loss= 2787.333984, Training Accuracy= 0.81250\n",
      "Iter 62720, Minibatch Loss= 3634.980469, Training Accuracy= 0.82812\n",
      "Iter 64000, Minibatch Loss= 3459.940674, Training Accuracy= 0.76562\n",
      "Iter 65280, Minibatch Loss= 1501.895752, Training Accuracy= 0.86719\n",
      "Iter 66560, Minibatch Loss= 3050.866455, Training Accuracy= 0.78125\n",
      "Iter 67840, Minibatch Loss= 2651.455078, Training Accuracy= 0.82812\n",
      "Iter 69120, Minibatch Loss= 2604.490234, Training Accuracy= 0.79688\n",
      "Iter 70400, Minibatch Loss= 2103.350586, Training Accuracy= 0.84375\n",
      "Iter 71680, Minibatch Loss= 3565.695312, Training Accuracy= 0.80469\n",
      "Iter 72960, Minibatch Loss= 3183.444336, Training Accuracy= 0.81250\n",
      "Iter 74240, Minibatch Loss= 2340.904297, Training Accuracy= 0.83594\n",
      "Iter 75520, Minibatch Loss= 2229.770264, Training Accuracy= 0.78906\n",
      "Iter 76800, Minibatch Loss= 1929.638672, Training Accuracy= 0.82031\n",
      "Iter 78080, Minibatch Loss= 3686.626953, Training Accuracy= 0.78906\n",
      "Iter 79360, Minibatch Loss= 2018.368286, Training Accuracy= 0.83594\n",
      "Iter 80640, Minibatch Loss= 3081.045654, Training Accuracy= 0.81250\n",
      "Iter 81920, Minibatch Loss= 3492.925293, Training Accuracy= 0.82031\n",
      "Iter 83200, Minibatch Loss= 3579.544434, Training Accuracy= 0.78125\n",
      "Iter 84480, Minibatch Loss= 1708.161499, Training Accuracy= 0.86719\n",
      "Iter 85760, Minibatch Loss= 1615.889893, Training Accuracy= 0.80469\n",
      "Iter 87040, Minibatch Loss= 2820.748291, Training Accuracy= 0.78125\n",
      "Iter 88320, Minibatch Loss= 1584.898804, Training Accuracy= 0.82812\n",
      "Iter 89600, Minibatch Loss= 2093.144043, Training Accuracy= 0.85156\n",
      "Iter 90880, Minibatch Loss= 3032.479736, Training Accuracy= 0.81250\n",
      "Iter 92160, Minibatch Loss= 3705.991699, Training Accuracy= 0.75781\n",
      "Iter 93440, Minibatch Loss= 5291.400391, Training Accuracy= 0.74219\n",
      "Iter 94720, Minibatch Loss= 3132.286377, Training Accuracy= 0.83594\n",
      "Iter 96000, Minibatch Loss= 1595.596191, Training Accuracy= 0.85938\n",
      "Iter 97280, Minibatch Loss= 3431.613525, Training Accuracy= 0.78125\n",
      "Iter 98560, Minibatch Loss= 1978.394653, Training Accuracy= 0.83594\n",
      "Iter 99840, Minibatch Loss= 1347.087891, Training Accuracy= 0.84375\n",
      "Iter 101120, Minibatch Loss= 1594.096436, Training Accuracy= 0.85938\n",
      "Iter 102400, Minibatch Loss= 2554.735107, Training Accuracy= 0.77344\n",
      "Iter 103680, Minibatch Loss= 3041.286865, Training Accuracy= 0.73438\n",
      "Iter 104960, Minibatch Loss= 2337.223145, Training Accuracy= 0.83594\n",
      "Iter 106240, Minibatch Loss= 1959.466064, Training Accuracy= 0.83594\n",
      "Iter 107520, Minibatch Loss= 2089.540283, Training Accuracy= 0.85156\n",
      "Iter 108800, Minibatch Loss= 1150.819580, Training Accuracy= 0.86719\n",
      "Iter 110080, Minibatch Loss= 1577.053345, Training Accuracy= 0.82031\n",
      "Iter 111360, Minibatch Loss= 2646.807617, Training Accuracy= 0.82812\n",
      "Iter 112640, Minibatch Loss= 3104.959961, Training Accuracy= 0.75000\n",
      "Iter 113920, Minibatch Loss= 1402.829468, Training Accuracy= 0.89062\n",
      "Iter 115200, Minibatch Loss= 978.385925, Training Accuracy= 0.89844\n",
      "Iter 116480, Minibatch Loss= 2034.345215, Training Accuracy= 0.80469\n",
      "Iter 117760, Minibatch Loss= 2885.123047, Training Accuracy= 0.79688\n",
      "Iter 119040, Minibatch Loss= 1316.211914, Training Accuracy= 0.85938\n",
      "Iter 120320, Minibatch Loss= 1793.564453, Training Accuracy= 0.88281\n",
      "Iter 121600, Minibatch Loss= 2054.473877, Training Accuracy= 0.80469\n",
      "Iter 122880, Minibatch Loss= 3006.597168, Training Accuracy= 0.75000\n",
      "Iter 124160, Minibatch Loss= 2203.635254, Training Accuracy= 0.81250\n",
      "Iter 125440, Minibatch Loss= 2593.634766, Training Accuracy= 0.76562\n",
      "Iter 126720, Minibatch Loss= 2010.406982, Training Accuracy= 0.82812\n",
      "Iter 128000, Minibatch Loss= 2225.289062, Training Accuracy= 0.82812\n",
      "Iter 129280, Minibatch Loss= 2963.034180, Training Accuracy= 0.80469\n",
      "Iter 130560, Minibatch Loss= 1633.569336, Training Accuracy= 0.82031\n",
      "Iter 131840, Minibatch Loss= 1669.807373, Training Accuracy= 0.80469\n",
      "Iter 133120, Minibatch Loss= 1121.816528, Training Accuracy= 0.87500\n",
      "Iter 134400, Minibatch Loss= 1984.562622, Training Accuracy= 0.82812\n",
      "Iter 135680, Minibatch Loss= 2442.747070, Training Accuracy= 0.82812\n",
      "Iter 136960, Minibatch Loss= 1961.065186, Training Accuracy= 0.79688\n",
      "Iter 138240, Minibatch Loss= 2164.058594, Training Accuracy= 0.82812\n",
      "Iter 139520, Minibatch Loss= 1776.983765, Training Accuracy= 0.82812\n",
      "Iter 140800, Minibatch Loss= 1386.886475, Training Accuracy= 0.83594\n",
      "Iter 142080, Minibatch Loss= 2618.878662, Training Accuracy= 0.80469\n",
      "Iter 143360, Minibatch Loss= 1601.192139, Training Accuracy= 0.80469\n",
      "Iter 144640, Minibatch Loss= 1908.253540, Training Accuracy= 0.81250\n",
      "Iter 145920, Minibatch Loss= 2753.727539, Training Accuracy= 0.77344\n",
      "Iter 147200, Minibatch Loss= 1609.261230, Training Accuracy= 0.85156\n",
      "Iter 148480, Minibatch Loss= 2011.445435, Training Accuracy= 0.80469\n",
      "Iter 149760, Minibatch Loss= 1675.868896, Training Accuracy= 0.85156\n",
      "Iter 151040, Minibatch Loss= 1675.673340, Training Accuracy= 0.79688\n",
      "Iter 152320, Minibatch Loss= 1175.329346, Training Accuracy= 0.87500\n",
      "Iter 153600, Minibatch Loss= 1899.806885, Training Accuracy= 0.85938\n",
      "Iter 154880, Minibatch Loss= 1453.725952, Training Accuracy= 0.82812\n",
      "Iter 156160, Minibatch Loss= 1125.842407, Training Accuracy= 0.80469\n",
      "Iter 157440, Minibatch Loss= 2797.127686, Training Accuracy= 0.74219\n",
      "Iter 158720, Minibatch Loss= 1806.216431, Training Accuracy= 0.82031\n",
      "Iter 160000, Minibatch Loss= 989.411804, Training Accuracy= 0.89062\n",
      "Iter 161280, Minibatch Loss= 1558.456665, Training Accuracy= 0.83594\n",
      "Iter 162560, Minibatch Loss= 2167.416992, Training Accuracy= 0.81250\n",
      "Iter 163840, Minibatch Loss= 1380.369995, Training Accuracy= 0.88281\n",
      "Iter 165120, Minibatch Loss= 1993.288086, Training Accuracy= 0.78125\n",
      "Iter 166400, Minibatch Loss= 1159.055786, Training Accuracy= 0.82812\n",
      "Iter 167680, Minibatch Loss= 1753.339478, Training Accuracy= 0.81250\n",
      "Iter 168960, Minibatch Loss= 1410.502197, Training Accuracy= 0.82812\n",
      "Iter 170240, Minibatch Loss= 1139.687744, Training Accuracy= 0.86719\n",
      "Iter 171520, Minibatch Loss= 1870.528564, Training Accuracy= 0.80469\n",
      "Iter 172800, Minibatch Loss= 697.512878, Training Accuracy= 0.89062\n",
      "Iter 174080, Minibatch Loss= 1559.518066, Training Accuracy= 0.85156\n",
      "Iter 175360, Minibatch Loss= 1491.494385, Training Accuracy= 0.84375\n",
      "Iter 176640, Minibatch Loss= 1651.786377, Training Accuracy= 0.84375\n",
      "Iter 177920, Minibatch Loss= 807.212097, Training Accuracy= 0.87500\n",
      "Iter 179200, Minibatch Loss= 2277.700684, Training Accuracy= 0.83594\n",
      "Iter 180480, Minibatch Loss= 1736.802979, Training Accuracy= 0.80469\n",
      "Iter 181760, Minibatch Loss= 1607.853027, Training Accuracy= 0.87500\n",
      "Iter 183040, Minibatch Loss= 1562.956787, Training Accuracy= 0.87500\n",
      "Iter 184320, Minibatch Loss= 1527.308960, Training Accuracy= 0.82812\n",
      "Iter 185600, Minibatch Loss= 837.757324, Training Accuracy= 0.88281\n",
      "Iter 186880, Minibatch Loss= 1127.805542, Training Accuracy= 0.89062\n",
      "Iter 188160, Minibatch Loss= 1508.778809, Training Accuracy= 0.82812\n",
      "Iter 189440, Minibatch Loss= 1783.610596, Training Accuracy= 0.78906\n",
      "Iter 190720, Minibatch Loss= 1184.317139, Training Accuracy= 0.85938\n",
      "Iter 192000, Minibatch Loss= 2039.092041, Training Accuracy= 0.81250\n",
      "Iter 193280, Minibatch Loss= 1482.966064, Training Accuracy= 0.84375\n",
      "Iter 194560, Minibatch Loss= 1302.019653, Training Accuracy= 0.84375\n",
      "Iter 195840, Minibatch Loss= 1827.861938, Training Accuracy= 0.79688\n",
      "Iter 197120, Minibatch Loss= 1660.632812, Training Accuracy= 0.78906\n",
      "Iter 198400, Minibatch Loss= 1846.879761, Training Accuracy= 0.82031\n",
      "Iter 199680, Minibatch Loss= 640.921387, Training Accuracy= 0.89062\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# def train():\n",
    "training_iters = 200000\n",
    "batchSize = 128\n",
    "display_step = 10\n",
    "def train(graphDict):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        epoch = 1\n",
    "        while epoch * batchSize < training_iters:\n",
    "            batchData = trainData_[(epoch-1)*batchSize : epoch*batchSize]\n",
    "            batchLabels = trainLabels_[(epoch-1)*batchSize : epoch*batchSize]\n",
    "\n",
    "            _, loss, acc = sess.run([graphDict['optimizer'],\n",
    "                                      graphDict['lossCE'], \n",
    "                                      graphDict['accuracy']], \n",
    "                                      feed_dict = {graphDict['trainData']: batchData,\n",
    "                                                   graphDict['trainLabels']: batchLabels\n",
    "                                                  }\n",
    "                                    )\n",
    "\n",
    "            if epoch % display_step == 0:\n",
    "                print (\"Iter \" + str(epoch*batchSize) + \", Minibatch Loss= \" + \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \"{:.5f}\".format(acc))\n",
    "            epoch += 1\n",
    "        print (\"Optimization Finished!\")\n",
    "        \n",
    "reset_graph()\n",
    "graphDict = BuildConvNet().conv_net()\n",
    "train(graphDict)\n",
    "\n",
    "\n",
    "\n",
    "#     Calculate accuracy for 256 mnist test images\n",
    "#     print (\"Testing Accuracy:\", \\\n",
    "#         sess.run(accuracy, feed_dict={x: mnist.test.images[:256],\n",
    "#                                       y: mnist.test.labels[:256],\n",
    "#                                       keep_prob: 1.}))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
