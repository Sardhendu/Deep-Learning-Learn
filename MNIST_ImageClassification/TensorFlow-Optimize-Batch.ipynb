{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports are doe Here:\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (200000, 28, 28) (200000,)\n",
      "Cross Validation: (9810, 28, 28) (9810,)\n",
      "Testing: (7709, 28, 28) (7709,)\n"
     ]
    }
   ],
   "source": [
    "# Now as always we get the data we stored in the disk.\n",
    "cleaned_dataset_path = '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/MNIST_ImageClassification/DataPreparation/dataset_cleaned.p'\n",
    "\n",
    "with open(cleaned_dataset_path, 'rb') as f:\n",
    "    fnl_dataset = pickle.load(f)\n",
    "    training_dataset = (fnl_dataset['training_dataset'])\n",
    "    training_labels = (fnl_dataset['training_labels'])\n",
    "    test_dataset = (fnl_dataset['test_dataset'])\n",
    "    test_labels = (fnl_dataset['test_labels'])\n",
    "    crossvalid_dataset = (fnl_dataset['crossvalid_dataset'])\n",
    "    crossvalid_labels = (fnl_dataset['crossvalid_labels'])\n",
    "    \n",
    "print('Training:', training_dataset.shape, training_labels.shape)\n",
    "print('Cross Validation:', crossvalid_dataset.shape, crossvalid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Cross Validation set (9810, 784) (9810, 10)\n",
      "Test set (7709, 784) (7709, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "no_of_labels = 10\n",
    "no_input_units = image_size * image_size\n",
    "\n",
    "# tensor flow takes the labels input as binary code, where Alphabet A whose binary value is 0 will turn to a array\n",
    "# with elements [1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0] and B becomes [0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "def reshape_data(dataset, labels, sample_size=None):\n",
    "    if sample_size:\n",
    "        dataset = dataset[:sample_size].reshape(sample_size, no_input_units) # To reshape the  \n",
    "        # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "        labels = (np.arange(no_of_labels) == labels[:,None]).astype(np.float32)\n",
    "    else:\n",
    "        dataset = dataset.reshape(len(dataset), no_input_units) # To reshape the  \n",
    "        # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "        labels = (np.arange(no_of_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "# We just reshape the image so that 1 image defines a row\n",
    "\n",
    "# training_dataset[:].reshapeshape\n",
    "training_dataset_, training_labels_ = reshape_data(training_dataset, training_labels)\n",
    "crossvalid_dataset_, crossvalid_labels_ = reshape_data(crossvalid_dataset, crossvalid_labels)\n",
    "test_dataset_, test_labels_ = reshape_data(test_dataset, test_labels)\n",
    "print('Training set', training_dataset_.shape, training_labels_.shape)\n",
    "print('Cross Validation set', crossvalid_dataset_.shape, crossvalid_labels_.shape)\n",
    "print('Test set', test_dataset_.shape, test_labels_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the tensor flow graph\n",
    "learning_rate = 0.5\n",
    "momentum = 0.9\n",
    "graph = tf.Graph()\n",
    "batch_size = 128\n",
    "\n",
    "with graph.as_default():\n",
    "    # We load all the training data, test data and crossvalid data into the contants\n",
    "    tf_training_dataset = tf.placeholder(tf.float32,shape=(batch_size, no_input_units))\n",
    "    tf_training_labels = tf.placeholder(tf.float32,shape=(batch_size, no_of_labels))\n",
    "    # The Training data are put in a placholder because we update the training for every batch\n",
    "    tf_crossvalid_dataset = tf.constant(crossvalid_dataset_)\n",
    "    tf_crossvalid_labels = tf.constant(crossvalid_labels_)\n",
    "    tf_test_dataset = tf.constant(test_dataset_)\n",
    "    tf_test_labels = tf.constant(test_labels_)\n",
    "    \n",
    "    \n",
    "    # Weight Initialization: In weight Initialization the weights are randomly initialized from a normal distribution\n",
    "    # One weight for each pixel and for each output label plus one 1 bais term.\n",
    "    weight_matrix = tf.Variable(tf.truncated_normal([no_input_units, no_of_labels]))\n",
    "    \n",
    "    biases = tf.Variable(tf.zeros([no_of_labels]))\n",
    "\n",
    "    # We have now obtained our random weights and x inputs, now lets train our model \n",
    "    # We multiply our weight to X's and add the baises term.\n",
    "    logits = tf.matmul(tf_training_dataset, weight_matrix) + biases\n",
    "    \n",
    "    # The next step after the logit function is to compute the softmax and then the perform the cross-entropy. \n",
    "    # In Tensor flow both the steps are achieved with a single function.\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_training_labels))\n",
    "    \n",
    "    # Now we build the optimization function using Gradient Descet to find the mnimum point\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, \n",
    "                                            momentum, \n",
    "                                            use_locking=False, \n",
    "                                            name='Momentum', \n",
    "                                            use_nesterov=False).minimize(loss)\n",
    "     \n",
    "    # We have built the logit function and used the optimize to find the minimum point.\n",
    "    # Now we make the prediction and compare the accurary, \n",
    "    training_prediction = tf.nn.softmax(logits)\n",
    "    crossvalid_prediction = tf.nn.softmax(tf.matmul(tf_crossvalid_dataset, weight_matrix) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weight_matrix) + biases)\n",
    "    \n",
    "#     print ([w for no, w in enumerate(start_session().run(weight_matrix)) if no<=2])\n",
    "#     print ([w for w in start_session().run(biases)])\n",
    "#     print ([w for no, w in enumerate(start_session().run(loss)) if no<=2])\n",
    "#     print ('')\n",
    "#     print ([w for n, w in enumerate(start_session().run(train_prediction)) if n<=10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All variable Initialized successfully\n",
      "Minibatch loss at epoch 0: 16.359072\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 12.3%\n",
      "Minibatch loss at epoch 500: 1.298825\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 55.3%\n",
      "Minibatch loss at epoch 1000: 2.221663\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 56.9%\n",
      "Minibatch loss at epoch 1500: 1.521302\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 54.3%\n",
      "Minibatch loss at epoch 2000: 0.984752\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 56.9%\n",
      "Minibatch loss at epoch 2500: 1.171123\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 56.8%\n",
      "Minibatch loss at epoch 3000: 1.612632\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 58.7%\n",
      "Test accuracy: 82.0%\n"
     ]
    }
   ],
   "source": [
    "# Here We start the session for the graph build in the above section. We use mini-batch stochastic method\n",
    "# for calculating the gradient.\n",
    "epochs = 3001\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"All variable Initialized successfully\")\n",
    "    for epoch in range(epochs):\n",
    "#         print ('The Epoch is: ', epoch)\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "        offset = (epoch * batch_size) % (training_labels.shape[0] - batch_size)  # 128 as the batch_size\n",
    "        # Generate a minibatch.\n",
    "        batch_data = training_dataset_[offset:(offset + batch_size), :]\n",
    "        batch_labels = training_labels_[offset:(offset + batch_size), :]\n",
    "#         print ('offset is ; ', offset)\n",
    "        #Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_training_dataset : batch_data, tf_training_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, training_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "        # just to get that one numpy array. Note that it recomputes all its graph\n",
    "        # dependencies.\n",
    "        if (epoch % 500 == 0):\n",
    "            print(\"Minibatch loss at epoch %d: %f\" % (epoch, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(crossvalid_prediction.eval(), crossvalid_labels_))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Notes Because of the large learning_rate (0.5), the accuracy depletes after sometime. This could mean that the \n",
    "# descent jumps the minimum position and may Oscillate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
