{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports are doe Here:\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (200000, 28, 28) (200000,)\n",
      "Cross Validation: (9810, 28, 28) (9810,)\n",
      "Testing: (7709, 28, 28) (7709,)\n"
     ]
    }
   ],
   "source": [
    "# Now as always we get the data we stored in the disk.\n",
    "cleaned_dataset_path = '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/MNIST_ImageClassification/DataPreparation/dataset_cleaned.p'\n",
    "\n",
    "with open(cleaned_dataset_path, 'rb') as f:\n",
    "    fnl_dataset = pickle.load(f)\n",
    "    training_dataset = (fnl_dataset['training_dataset'])\n",
    "    training_labels = (fnl_dataset['training_labels'])\n",
    "    test_dataset = (fnl_dataset['test_dataset'])\n",
    "    test_labels = (fnl_dataset['test_labels'])\n",
    "    crossvalid_dataset = (fnl_dataset['crossvalid_dataset'])\n",
    "    crossvalid_labels = (fnl_dataset['crossvalid_labels'])\n",
    "    \n",
    "print('Training:', training_dataset.shape, training_labels.shape)\n",
    "print('Cross Validation:', crossvalid_dataset.shape, crossvalid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Cross Validation set (9810, 784) (9810, 10)\n",
      "Test set (7709, 784) (7709, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "no_of_labels = 10\n",
    "no_of_input_units = image_size * image_size\n",
    "\n",
    "# tensor flow takes the labels input as binary code, where Alphabet A whose binary value is 0 will turn to a array\n",
    "# with elements [1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0] and B becomes [0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "def reshape_data(dataset, labels, sample_size=None):\n",
    "    if sample_size:\n",
    "        dataset = dataset[:sample_size].reshape(sample_size, no_of_input_units) # To reshape the  \n",
    "        # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "        labels = (np.arange(no_of_labels) == labels[:,None]).astype(np.float32)\n",
    "    else:\n",
    "        dataset = dataset.reshape(len(dataset), no_of_input_units) # To reshape the  \n",
    "        # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "        labels = (np.arange(no_of_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "# We just reshape the image so that 1 image defines a row\n",
    "\n",
    "# training_dataset[:].reshapeshape\n",
    "training_dataset_, training_labels_ = reshape_data(training_dataset, training_labels)\n",
    "crossvalid_dataset_, crossvalid_labels_ = reshape_data(crossvalid_dataset, crossvalid_labels)\n",
    "test_dataset_, test_labels_ = reshape_data(test_dataset, test_labels)\n",
    "print('Training set', training_dataset_.shape, training_labels_.shape)\n",
    "print('Cross Validation set', crossvalid_dataset_.shape, crossvalid_labels_.shape)\n",
    "print('Test set', test_dataset_.shape, test_labels_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# In this section, we build the complete graph. We do this only for the forward pass.\n",
    "# When we compute the matrix multiplication and indicate the Tensor API of the forward pass structure,\n",
    "# The Tensor flow API automatically calculated the Backpropogation pass and updates the weights automatcially,\n",
    "# All we have to do is select the optimization technique and the hyperparameters.\n",
    "sample_size = 10000\n",
    "\n",
    "   # Loading interactive Tensor flow session to print tesorflow objects\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "def start_session():\n",
    "    init = tf.initialize_all_variables()\n",
    "    session = tf.Session()\n",
    "    session.run(init)\n",
    "    return session\n",
    "    \n",
    "with graph.as_default():\n",
    "    # We load all the training data, test data and crossvalid data into the contants\n",
    "    tf_training_dataset = tf.constant(training_dataset_[:sample_size, :])\n",
    "    tf_training_labels = tf.constant(training_labels_[:sample_size, :])\n",
    "    tf_crossvalid_dataset = tf.constant(crossvalid_dataset_)\n",
    "    tf_crossvalid_labels = tf.constant(crossvalid_labels_)\n",
    "    tf_test_dataset = tf.constant(test_dataset_)\n",
    "    tf_test_labels = tf.constant(test_labels_)\n",
    "    \n",
    "    \n",
    "    # Weight Initialization: In weight Initialization the weights are randomly initialized from a normal distribution\n",
    "    # One weight for each pixel and for each output label plus one 1 bais term.\n",
    "    weight_matrix = tf.Variable(tf.truncated_normal([no_of_input_units, no_of_labels]))\n",
    "    \n",
    "    biases = tf.Variable(tf.zeros([no_of_labels]))\n",
    "\n",
    "    # We have now obtained our random weights and x inputs, now lets train our model \n",
    "    # We multiply our weight to X's and add the baises term.\n",
    "    logits = tf.matmul(tf_training_dataset, weight_matrix) + biases\n",
    "    \n",
    "    # The next step after the logit function is to compute the softmax and then the perform the cross-entropy. \n",
    "    # In Tensor flow both the steps are achieved with a single function.\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_training_labels))\n",
    "    \n",
    "    # Now we build the optimization function using Gradient Descet to find the mnimum point\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "     \n",
    "    # We have built the logit function and used the optimize to find the minimum point.\n",
    "    # Now we make the prediction and compare the accurary, \n",
    "    training_prediction = tf.nn.softmax(logits)\n",
    "    crossvalid_prediction = tf.nn.softmax(tf.matmul(tf_crossvalid_dataset, weight_matrix) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weight_matrix) + biases)\n",
    "    \n",
    "#     print ([w for no, w in enumerate(start_session().run(weight_matrix)) if no<=2])\n",
    "#     print ([w for w in start_session().run(biases)])\n",
    "#     print ([w for no, w in enumerate(start_session().run(loss)) if no<=2])\n",
    "#     print ('')\n",
    "#     print ([w for n, w in enumerate(start_session().run(train_prediction)) if n<=10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable Initialized\n",
      "Loss at step 0: 18.836712\n",
      "Training accuracy: 7.4%\n",
      "Validation accuracy: 9.8%\n",
      "Loss at step 100: 2.157829\n",
      "Training accuracy: 73.7%\n",
      "Validation accuracy: 50.0%\n",
      "Loss at step 200: 1.736687\n",
      "Training accuracy: 76.7%\n",
      "Validation accuracy: 53.0%\n",
      "Loss at step 300: 1.505381\n",
      "Training accuracy: 78.2%\n",
      "Validation accuracy: 53.7%\n",
      "Loss at step 400: 1.350352\n",
      "Training accuracy: 79.0%\n",
      "Validation accuracy: 54.0%\n",
      "Loss at step 500: 1.235651\n",
      "Training accuracy: 79.8%\n",
      "Validation accuracy: 54.2%\n",
      "Loss at step 600: 1.145626\n",
      "Training accuracy: 80.4%\n",
      "Validation accuracy: 54.4%\n",
      "Loss at step 700: 1.072251\n",
      "Training accuracy: 81.0%\n",
      "Validation accuracy: 54.5%\n",
      "Loss at step 800: 1.010984\n",
      "Training accuracy: 81.3%\n",
      "Validation accuracy: 54.5%\n",
      "Test accuracy: 79.8%\n"
     ]
    }
   ],
   "source": [
    "# In this section we run the session for tensorflow. The gradient is computed using the complete dataset.\n",
    "\n",
    "epochs = 801\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # This is a one-time operation which ensures the parameters get initialized as\n",
    "    # we described in the graph: random weights for the matrix, zeros for the biases. \n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Variable Initialized')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "    # and get the loss value and the training predictions returned as numpy\n",
    "    # arrays.\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "        \n",
    "        if (epoch % 100 == 0):\n",
    "            print('Loss at step %d: %f' % (epoch, l))\n",
    "            print('Training accuracy: %.1f%%' % accuracy(predictions, training_labels_[:sample_size, :]))\n",
    "            # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "            # just to get that one numpy array. Note that it recomputes all its graph dependencies.\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(crossvalid_prediction.eval(), crossvalid_labels_))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
