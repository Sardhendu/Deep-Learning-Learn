{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    About me: this piece of Code checks if the code given in the url \"http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html\"\n",
    "    performs well with the news data (training_with_sentences).\n",
    "    \n",
    "    The Problem that we encounter here is that the loss we receive after every batch is : \"nan\"\n",
    "    \n",
    "    BEFORE RUNNING THIS CODE YOU SHOULD FIRST RUN THE \"DATA_BUILDER.PY\" TO FIRST EXTRACT, CLEAN AND LOAD THE DATA\n",
    "    INTO PICKLE FILES AND THEN THIS CODE WILL PART COME IN HANDY.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import urllib.request\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.models.rnn.ptb import reader\n",
    "from gensim import corpora\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_graph():  # Reset the graph\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_multilayer_lstm_graph_with_dynamic_rnn(\n",
    "    state_size = 100,\n",
    "    num_classes = 10,\n",
    "    batch_size = 128,\n",
    "#     num_layers = 3,\n",
    "    learning_rate = 1e-4):\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    print ('The num of hidden unit is: ', state_size)\n",
    "    print ('The Vocab size is: ', num_classes)\n",
    "    print ('The batch_size is: ', batch_size)\n",
    "    print ('The learning_rate is: ', learning_rate)\n",
    "    \n",
    "    x = tf.placeholder(tf.int32, [batch_size, None], name='input_placeholder')\n",
    "    y = tf.placeholder(tf.int32, [batch_size, None], name='labels_placeholder')\n",
    "\n",
    "    embeddings = tf.get_variable('embedding_matrix', [num_classes, state_size])\n",
    "\n",
    "    # Note that our inputs are no longer a list, but a tensor of dims batch_size x num_steps x state_size\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n",
    "#     cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes])\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    #reshape rnn_outputs and y so we can get the logits in a single matmul\n",
    "    rnn_outputs = tf.reshape(rnn_outputs, [-1, state_size])\n",
    "    y_reshaped = tf.reshape(y, [-1])\n",
    "\n",
    "    logits = tf.matmul(rnn_outputs, W) + b\n",
    "    output_state = tf.nn.softmax(logits, name=None)\n",
    "    \n",
    "    total_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y_reshaped))\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n",
    "\n",
    "    return dict(\n",
    "        x = x,\n",
    "        y = y,\n",
    "        init_state = init_state,\n",
    "        final_state = final_state,\n",
    "        total_loss = total_loss,\n",
    "        train_step = train_step,\n",
    "        training_prediction = output_state\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# class Train():\n",
    "#     def __init__(self):\n",
    "train_batch_dir = '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Word-Search-NNets/Word-Nets/training_batch/'\n",
    "dictionary_dir = '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Word-Search-NNets/Word-Nets/dictionary.txt'\n",
    "\n",
    "def accuracy(predictions, labels, labels_one_hot = None):\n",
    "    # The input labels are a One-Hot Vector\n",
    "    if labels_one_hot:\n",
    "        return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "              / predictions.shape[0])\n",
    "    else:\n",
    "        return (100.0 * np.sum(np.argmax(predictions, 1) == np.reshape(labels, [-1]))\n",
    "              / predictions.shape[0])\n",
    "        \n",
    "\n",
    "def train_network(g, num_batches, epochs=1, verbose=None ):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        for epoch in np.arange(epochs):\n",
    "            training_state = None\n",
    "            training_loss = 0\n",
    "\n",
    "            for no in np.arange(num_batches):\n",
    "                with open(train_batch_dir+'batch'+str(no)+'.pickle', 'rb') as f:\n",
    "                    dataset = pickle.load(f)\n",
    "\n",
    "                    batch_train_dataset = dataset['batch_train_dataset']\n",
    "                    batch_train_labels = dataset['batch_train_labels']\n",
    "\n",
    "                    feed_dict={g['x']: batch_train_dataset, g['y']: batch_train_labels}\n",
    "                    if training_state is not None:\n",
    "                        feed_dict[g['init_state']] = training_state\n",
    "                    training_loss_, training_state, _, tp = sess.run([g['total_loss'],\n",
    "                                                          g['final_state'],\n",
    "                                                          g['train_step'],\n",
    "                                                          g['training_prediction']],\n",
    "                                                                 feed_dict)\n",
    "                    training_loss += training_loss_\n",
    "                    acc = accuracy(tp, batch_train_labels)\n",
    "\n",
    "                    print ('accuracy of the batch %d is: '%no, acc)\n",
    "                    print ('')\n",
    "                    print ('Average Loss for the batch %d is: '%no, training_loss_)\n",
    "                    print ('')\n",
    "\n",
    "            print ('All %d Batches Done..'%num_batches)\n",
    "            print ('')\n",
    "\n",
    "            if verbose:\n",
    "                print(\"Average training loss for Epoch\", epoch, \":\", training_loss/num_batches)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The num of hidden unit is:  100\n",
      "The Vocab size is:  10\n",
      "The batch_size is:  128\n",
      "The learning_rate is:  0.0001\n",
      "It took 0.3268280029296875 seconds to build the graph.\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "build_multilayer_lstm_graph_with_dynamic_rnn()\n",
    "print(\"It took\", time.time() - t, \"seconds to build the graph.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The num of hidden unit is:  100\n",
      "The Vocab size is:  17155\n",
      "The batch_size is:  128\n",
      "The learning_rate is:  0.0001\n",
      "accuracy of the batch 0 is:  0.0\n",
      "\n",
      "Average Loss for the batch 0 is:  nan\n",
      "\n",
      "accuracy of the batch 1 is:  55.5921052632\n",
      "\n",
      "Average Loss for the batch 1 is:  nan\n",
      "\n",
      "accuracy of the batch 2 is:  61.8489583333\n",
      "\n",
      "Average Loss for the batch 2 is:  nan\n",
      "\n",
      "accuracy of the batch 3 is:  59.6514423077\n",
      "\n",
      "Average Loss for the batch 3 is:  nan\n",
      "\n",
      "accuracy of the batch 4 is:  54.5915570175\n",
      "\n",
      "Average Loss for the batch 4 is:  nan\n",
      "\n",
      "All 5 Batches Done..\n",
      "\n",
      "Average training loss for Epoch 0 : nan\n",
      "It took 36.00739288330078 seconds to train for 3 epochs.\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(corpora.Dictionary.load_from_text(dictionary_dir))\n",
    "graph_dict =  build_multilayer_lstm_graph_with_dynamic_rnn(state_size = 100, \n",
    "                                                           num_classes = vocab_size,\n",
    "                                                           batch_size = 128)\n",
    "t = time.time()\n",
    "train_network(graph_dict, num_batches = 5, epochs=1, verbose=True)\n",
    "print(\"It took\", time.time() - t, \"seconds to train for 3 epochs.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
