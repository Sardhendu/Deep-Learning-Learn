{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    About me: this piece of Code checks if the shakespeare data performs good with my code.\n",
    "    Problem: The problem we had in the previous (learn-4 file is thay the loss we encountered was \"nan\")\n",
    "    Approach: Here we take the Shakespeare data and check if the shakespeare data performs good with my code.\n",
    "                if the output here is good we conclude that the code is okay and the data is not proper.\n",
    "    \n",
    "    Needed: Here to run this code we need the shakespeare data in the local directory. IF Not there then refer learn-3\n",
    "    Output: we see from the output that the loss in this case is not \"nan\", which means that the code is correct.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import urllib.request\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.models.rnn.ptb import reader\n",
    "from gensim import corpora\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length: 1115394\n"
     ]
    }
   ],
   "source": [
    "file_name = '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Word-Search-NNets/Practise/tinyshakespeare.txt'\n",
    "with open(file_name,'r') as f:\n",
    "    raw_data = f.read()\n",
    "    print(\"Data length:\", len(raw_data))\n",
    "    \n",
    "vocab = set(raw_data)\n",
    "vocab_size = len(vocab)\n",
    "idx_to_vocab = dict(enumerate(vocab))\n",
    "vocab_to_idx = dict(zip(idx_to_vocab.values(), idx_to_vocab.keys()))\n",
    "\n",
    "data = [vocab_to_idx[c] for c in raw_data]\n",
    "del raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_graph():  # Reset the graph\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dynamic_RNN_model(\n",
    "    num_hid_units = 100,\n",
    "    momentum = 0.9,\n",
    "    num_classes = 10,\n",
    "    learning_rate = 0.01\n",
    "    ):\n",
    "    vocab_size = num_classes\n",
    "    print ('The num of hidden unit is: ', num_hid_units)\n",
    "    print ('The Vocab size is: ', vocab_size)\n",
    "    print ('The momentum is: ', momentum)\n",
    "    print ('The learning_rate is: ', learning_rate)\n",
    "    \n",
    "    \n",
    "    reset_graph()\n",
    "    \n",
    "    x = tf.placeholder(tf.int32, shape = [None, None], name='input_placeholder')\n",
    "    y = tf.placeholder(tf.int32, shape = [None, None], name='output_placeholdr')\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    \n",
    "\n",
    "    # ENBEDDING(INPUT) LAYER OPERATION\n",
    "    # Creating an Embedding matrix with a random weight for all vacab to hidden_matrix\n",
    "    embed_to_hid_wghts = tf.get_variable('embedding_matrix', [vocab_size, num_hid_units])\n",
    "    embed_to_hid_layer = tf.nn.embedding_lookup(embed_to_hid_wghts, x)\n",
    "    print ('The shape of embed_to_hid_wghts is: ', embed_to_hid_wghts.get_shape())\n",
    "    print ('The shape of embed_to_hid_layer is: ', embed_to_hid_layer.get_shape())\n",
    "\n",
    "\n",
    "    # HIDDEN LAYER OPERATION\n",
    "    rnn_cell = tf.nn.rnn_cell.LSTMCell(num_hid_units, state_is_tuple=True)\n",
    "    init_state = rnn_cell.zero_state(batch_size, tf.float32)  # Each sequence will hava a state that it passes to its next sequence\n",
    "    rnn_outputs, new_state = tf.nn.dynamic_rnn(cell=rnn_cell,\n",
    "                                            # sequence_length=X_lengths,\n",
    "                                            initial_state=init_state,\n",
    "                                            inputs=embed_to_hid_layer)\n",
    "    \n",
    "\n",
    "    # OUTPUT LAYER OPERATION\n",
    "    # Initialize the weight and biases for the output layer. We use variable scope because we would like to share the weights \n",
    "    with tf.variable_scope('output_layer'):\n",
    "        hid_to_output_wght = tf.get_variable('hid_to_output_wght', [num_hid_units, num_classes])\n",
    "        output_bias = tf.get_variable('output_bias',\n",
    "                                      [num_classes],\n",
    "                                      initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "    rnn_outputs = tf.reshape(rnn_outputs, [-1, num_hid_units])  \n",
    "    hid_to_ouptut_layer = tf.matmul(rnn_outputs, hid_to_output_wght) +  output_bias\n",
    "    output_state = tf.nn.softmax(hid_to_ouptut_layer, name=None)\n",
    " \n",
    "    \n",
    "    \n",
    "    # CALCULATING LOSS, OPTIMIZING THE COST FUNCTION, MEASURING ACCURACY\n",
    "    y_reshaped = tf.reshape(y, [-1])\n",
    "    loss_CE = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(hid_to_ouptut_layer, y_reshaped))\n",
    "    \n",
    "    # The sparse_softmax uses dtype as int32 or int64\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, \n",
    "                                            momentum, \n",
    "                                            use_locking=False, \n",
    "                                            name='Momentum', \n",
    "                                            use_nesterov=True).minimize(loss_CE)\n",
    "    # y_ = tf.reshape(y, [-1])\n",
    "    # correct_prediction = tf.equal(tf.arg_max(output_state,1), tf.arg_max(y_ ,1))\n",
    "    # accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "    \n",
    "\n",
    "    # Returns graph objects\n",
    "    return dict(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        batch_size = batch_size,\n",
    "        init_state = init_state,\n",
    "        new_state = new_state,\n",
    "        loss_CE = loss_CE,\n",
    "        optimizer = optimizer,\n",
    "        training_prediction = output_state\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_epochs(n, num_steps, batch_size):\n",
    "    for i in range(n):\n",
    "        yield reader.ptb_iterator(data, batch_size, num_steps)\n",
    "\n",
    "def train_network(g, num_epochs, num_steps = 200, batch_size = 32, verbose = True, save=False):\n",
    "    tf.set_random_seed(2345)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        training_losses = []\n",
    "        for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps, batch_size)):\n",
    "            training_loss = 0\n",
    "            steps = 0\n",
    "            new_hid_layer_state = None\n",
    "            \n",
    "            for X, Y in epoch:\n",
    "                if not new_hid_layer_state:\n",
    "                    print ('Using the zero init RNN State')\n",
    "                    feed_dict= {graph_dict['x']: X, \n",
    "                                graph_dict['y']: Y}\n",
    "                else:\n",
    "                    print ('Using the new RNN State')\n",
    "                    feed_dict= {graph_dict['x']: X, \n",
    "                                graph_dict['y']: Y,\n",
    "                                graph_dict['init_state'] : new_hid_layer_state}\n",
    "\n",
    "                bs, nwst, loss, opt, tp = sess.run([graph_dict['batch_size'],\n",
    "                                                graph_dict['new_state'],\n",
    "                                                graph_dict['loss_CE'],\n",
    "                                                graph_dict['optimizer'],\n",
    "                                                graph_dict['training_prediction']], \n",
    "                                                feed_dict=feed_dict)\n",
    "                new_hid_layer_state = nwst\n",
    "                training_loss += loss\n",
    "#                 acc = self.accuracy(tp, batch_train_labels)\n",
    "\n",
    "#                 print ('accuracy of the batch %d is: '%no, acc)\n",
    "#                 print ('')\n",
    "                print ('Average Loss for the batch is:', loss)\n",
    "                print ('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The num of hidden unit is:  100\n",
      "The Vocab size is:  10\n",
      "The momentum is:  0.9\n",
      "The learning_rate is:  0.01\n",
      "The shape of embed_to_hid_wghts is:  (10, 100)\n",
      "The shape of embed_to_hid_layer is:  (?, ?, 100)\n",
      "It took 0.2942180633544922 seconds to build the graph.\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "dynamic_RNN_model()\n",
    "print(\"It took\", time.time() - t, \"seconds to build the graph.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size is:  65\n",
      "The num of hidden unit is:  100\n",
      "The Vocab size is:  65\n",
      "The momentum is:  0.9\n",
      "The learning_rate is:  0.01\n",
      "The shape of embed_to_hid_wghts is:  (65, 100)\n",
      "The shape of embed_to_hid_layer is:  (?, ?, 100)\n",
      "Using the zero init RNN State\n",
      "Average Loss for the batch is: 4.17213\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 4.16948\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 4.16546\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 4.16034\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 4.15652\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 4.1506\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 4.1449\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 4.13654\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 4.12921\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 4.12009\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 4.11362\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 4.10106\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 4.09432\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 4.08345\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 4.07211\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 4.06216\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 4.05027\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 4.03565\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 4.02018\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 4.01327\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.99928\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.98198\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.9673\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.95371\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.93781\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.91197\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.89451\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.8753\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.85755\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.83675\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.81184\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.79467\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.7758\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.75613\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.71849\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.73199\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.69572\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.68155\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.67105\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.64061\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.65128\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.63631\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.59911\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.57246\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.57586\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.55762\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.55518\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.55177\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.55182\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.52753\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.53105\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.52248\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.53172\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.52808\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.4774\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.48306\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.46874\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.46522\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.46271\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.46639\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.43609\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.42853\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.44979\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.41392\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.43657\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.40973\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.39056\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.39977\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.3764\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.42476\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.43811\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.41818\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.40767\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.39926\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.40517\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.39853\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.37657\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.36753\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.34385\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.39374\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.35048\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.36132\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.37229\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.38327\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.37625\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.37394\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.34842\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.36665\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.40365\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.34781\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.35345\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.35651\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.38942\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.34124\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.3486\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.33619\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.34475\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.34658\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.36533\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.34833\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.32944\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.3672\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.36306\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.34031\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.34716\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.32904\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.35088\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.33338\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.36569\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.36427\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.36287\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.36799\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.33192\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.32212\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.35423\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.35713\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.30288\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.32071\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.34447\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.34626\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.37125\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.32557\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.3077\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.30693\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.35251\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.33399\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.34319\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.33263\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.32965\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.33731\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.35455\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.37213\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.38045\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.35136\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.36865\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.36959\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.34357\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.36711\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.36309\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.3689\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.31077\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.29284\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.32912\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.34084\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.32809\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.3373\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.33472\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.34618\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.31519\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.34547\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.31525\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.30391\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.29961\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.3023\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.29039\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.37715\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.37689\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.31587\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.30466\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.32307\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.28884\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.28214\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.32677\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.33483\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.27631\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.30579\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.27983\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.34265\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.29153\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.24386\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.31724\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.28312\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.28169\n",
      "\n",
      "Using the new RNN State\n",
      "Average Loss for the batch is: 3.28164\n",
      "\n",
      "It took 60.53509211540222 seconds to train for 3 epochs.\n"
     ]
    }
   ],
   "source": [
    "print ('vocab_size is: ', vocab_size)\n",
    "graph_dict =  dynamic_RNN_model(num_hid_units = 100, num_classes = vocab_size)\n",
    "t = time.time()\n",
    "train_network(graph_dict, num_epochs=1, num_steps = 200, batch_size = 32)\n",
    "print(\"It took\", time.time() - t, \"seconds to train for 1 epochs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
