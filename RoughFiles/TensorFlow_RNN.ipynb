{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Now as always we get the data we stored in the disk.\n",
    "# cleaned_dataset_path = '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/MNIST_ImageClassification/DataPreparation/dataset_cleaned.p'\n",
    "\n",
    "# with open(cleaned_dataset_path, 'rb') as f:\n",
    "#     fnl_dataset = pickle.load(f)\n",
    "#     training_dataset = (fnl_dataset['training_dataset'])\n",
    "#     training_labels = (fnl_dataset['training_labels'])\n",
    "#     test_dataset = (fnl_dataset['test_dataset'])\n",
    "#     test_labels = (fnl_dataset['test_labels'])\n",
    "#     crossvalid_dataset = (fnl_dataset['crossvalid_dataset'])\n",
    "#     crossvalid_labels = (fnl_dataset['crossvalid_labels'])\n",
    "    \n",
    "# print('Training:', training_dataset.shape, training_labels.shape)\n",
    "# print('Cross Validation:', crossvalid_dataset.shape, crossvalid_labels.shape)\n",
    "# print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Mini-Batch Creation\n",
    "# no_of_batches = 200\n",
    "# batch_indices = []\n",
    "# def batch_indice_creator(training_labels, no_of_batches):\n",
    "#     \"\"\"Create batch with random samples and return appropriate format\"\"\"\n",
    "#     label_rand_arr = []\n",
    "#     no_of_labels = len(np.unique(training_labels))\n",
    "    \n",
    "#     if len(training_labels)/no_of_batches < no_of_labels:\n",
    "#         raise Exception (\"Every batch should contain atleast 1 example from all the differnt class labels\")\n",
    "#     else:\n",
    "#         for unq_labels in np.unique(training_labels):\n",
    "#             arr = np.array(np.where(training_labels==unq_labels)[0])\n",
    "#             print ('Total number of label %d is :'%unq_labels, len(arr))\n",
    "#             np.random.shuffle(arr)\n",
    "#     #         print ('the random array is: ', np.reshape(arr, (len(arr),1)))\n",
    "#             if np.any(label_rand_arr):\n",
    "#                 label_rand_arr = np.hstack((label_rand_arr, np.reshape(arr, (len(arr),1))))\n",
    "#             else:\n",
    "#                 label_rand_arr = np.reshape(arr, (len(arr),1))\n",
    "        \n",
    "#         no_of_batches_new = no_of_batches\n",
    "#         if label_rand_arr.shape[0]%no_of_batches!=0:\n",
    "#             no_of_batches_new = no_of_batches + label_rand_arr.shape[0]%no_of_batches\n",
    "#             print ('Provided no_of_batches doesnt distribute the labels equally, ne no_of_batches is: ', no_of_batches_new) \n",
    "#         batch_indices = np.array_split(label_rand_arr.flatten(), no_of_batches_new)\n",
    "#         batch_size = len(batch_indices[0])\n",
    "#     return batch_indices, batch_size\n",
    "\n",
    "\n",
    "# training_batch_dir = '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/MNIST_ImageClassification/DataPreparation/'        \n",
    "# batch_training_dict = {}\n",
    "# def batch_store(training_dataset, training_labels, batch_indices):\n",
    "#     for no, indices in enumerate(batch_indices):\n",
    "#         #print (training_labels[indices])\n",
    "#         try:\n",
    "#             f = open(training_batch_dir+'batch'+str(no)+'.pickle', 'wb')\n",
    "#             batch = {\n",
    "#                 'batch_train_dataset': training_dataset[indices],\n",
    "#                 'batch_train_labels': training_labels[indices],\n",
    "#             }\n",
    "#             pickle.dump(batch, f, pickle.HIGHEST_PROTOCOL)\n",
    "#             f.close()\n",
    "#         except Exception as e:\n",
    "#             print('Unable to save data to', complete_dataset_path, ':', e)\n",
    "#             raise\n",
    "            \n",
    "# batch_indices, batch_size = batch_indice_creator(training_labels, no_of_batches)  #print ([arr.shape for arr in batch_indices])\n",
    "# #batch_store(training_dataset, training_labels, batch_indices)\n",
    "\n",
    "# print ('the batch size is:',batch_size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# image_size = 28\n",
    "# no_of_labels = 10\n",
    "# no_inp_unit = image_size * image_size\n",
    "\n",
    "# # tensor flow takes the labels input as binary code, where Alphabet A whose binary value is 0 will turn to a array\n",
    "# # with elements [1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0] and B becomes [0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "# def reshape_data(dataset, labels, sample_size=None):\n",
    "#     if sample_size:\n",
    "#         dataset = dataset[:sample_size].reshape(sample_size, no_inp_unit) # To reshape the  \n",
    "#         # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "#         labels = (np.arange(no_of_labels) == labels[:,None]).astype(np.float32)\n",
    "#     else:\n",
    "#         dataset = dataset.reshape(len(dataset), no_inp_unit) # To reshape the  \n",
    "#         # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "#         labels = (np.arange(no_of_labels) == labels[:,None]).astype(np.float32)\n",
    "#     return dataset, labels\n",
    "\n",
    "# # We just reshape the image so that 1 image defines a row\n",
    "\n",
    "# crossvalid_dataset_, crossvalid_labels_ = reshape_data(crossvalid_dataset, crossvalid_labels)\n",
    "# test_dataset_, test_labels_ = reshape_data(test_dataset, test_labels)\n",
    "# print('Cross Validation set', crossvalid_dataset_.shape, crossvalid_labels_.shape)\n",
    "# print('Test set', test_dataset_.shape, test_labels_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "# mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot = True)\n",
    "\n",
    "# n_nodes_hl1 = 500\n",
    "# n_nodes_hl2 = 500\n",
    "# n_nodes_hl3 = 500\n",
    "\n",
    "# n_classes = 10\n",
    "# batch_size = 100\n",
    "\n",
    "# x = tf.placeholder('float', [None, 784])\n",
    "# y = tf.placeholder('float')\n",
    "\n",
    "# def neural_network_model(data):\n",
    "#     hidden_1_layer = {'weights':tf.Variable(tf.random_normal([784, n_nodes_hl1])),\n",
    "#                       'biases':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "\n",
    "#     hidden_2_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "#                       'biases':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "\n",
    "#     hidden_3_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),\n",
    "#                       'biases':tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "\n",
    "#     output_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),\n",
    "#                     'biases':tf.Variable(tf.random_normal([n_classes])),}\n",
    "\n",
    "\n",
    "#     l1 = tf.add(tf.matmul(data,hidden_1_layer['weights']), hidden_1_layer['biases'])\n",
    "#     l1 = tf.nn.relu(l1)\n",
    "\n",
    "#     l2 = tf.add(tf.matmul(l1,hidden_2_layer['weights']), hidden_2_layer['biases'])\n",
    "#     l2 = tf.nn.relu(l2)\n",
    "\n",
    "#     l3 = tf.add(tf.matmul(l2,hidden_3_layer['weights']), hidden_3_layer['biases'])\n",
    "#     l3 = tf.nn.relu(l3)\n",
    "\n",
    "#     output = tf.matmul(l3,output_layer['weights']) + output_layer['biases']\n",
    "\n",
    "#     return output\n",
    "\n",
    "# def train_neural_network(x):\n",
    "#     prediction = neural_network_model(x)\n",
    "#     cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(prediction,y) )\n",
    "#     optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "#     hm_epochs = 10\n",
    "#     with tf.Session() as sess:\n",
    "#         sess.run(tf.initialize_all_variables())\n",
    "\n",
    "#         for epoch in range(hm_epochs):\n",
    "#             epoch_loss = 0\n",
    "#             for _ in range(int(mnist.train.num_examples/batch_size)):\n",
    "#                 epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n",
    "#                 _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})\n",
    "#                 epoch_loss += c\n",
    "\n",
    "#             print('Epoch', epoch, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
    "\n",
    "#         correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "\n",
    "#         accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "#         print('Accuracy:',accuracy.eval({x:mnist.test.images, y:mnist.test.labels}))\n",
    "\n",
    "# train_neural_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot = True)\n",
    "\n",
    "hm_epochs = 3\n",
    "n_classes = 10\n",
    "batch_size = 128\n",
    "chunk_size = 28\n",
    "n_chunks = 28\n",
    "rnn_size = 128\n",
    "\n",
    "\n",
    "x = tf.placeholder('float', [None, n_chunks,chunk_size])\n",
    "y = tf.placeholder('float')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print (x.get_shape())\n",
    "#print (y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot = True)\n",
    "\n",
    "hm_epochs = 3\n",
    "n_classes = 10\n",
    "batch_size = 128\n",
    "chunk_size = 28  # 28 pickles would comprise of 1 chunck\n",
    "n_chunks = 28    # for one image of 28*28, we decide 28 sequences\n",
    "no_of_hid_units = 128\n",
    "\n",
    "\n",
    "x = tf.placeholder('float', [None, n_chunks,chunk_size])\n",
    "y = tf.placeholder('float')\n",
    "\n",
    "def recurrent_neural_network(x):\n",
    "    layer = {'weights':tf.Variable(tf.random_normal([rnn_size,n_classes])),\n",
    "             'biases':tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "    x = tf.transpose(x, [1,0,2])\n",
    "    x = tf.reshape(x, [-1, chunk_size])\n",
    "    x = tf.split(0, n_chunks, x)\n",
    "\n",
    "    lstm_cell = rnn_cell.BasicLSTMCell(no_of_hid_units,state_is_tuple=True)\n",
    "    outputs, states = rnn.rnn(lstm_cell, x, dtype=tf.float32)\n",
    "#     print ('The shape of the output is given as: ', outputs.get_shape())\n",
    "#     print ('The second shape of the output is given as ', outputs[-1].get_shape())\n",
    "    output = tf.matmul(outputs[-1],layer['weights']) + layer['biases']\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(128, 784)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(x):\n",
    "#     prediction = recurrent_neural_network(x)\n",
    "#     cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(prediction,y) )\n",
    "#     optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        for epoch in range(hm_epochs):\n",
    "            epoch_loss = 0\n",
    "            for i in range(int(mnist.train.num_examples/batch_size)):\n",
    "                print (i)\n",
    "                epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n",
    "                print (epoch_x.shape)\n",
    "                print (epoch_x\n",
    "                \n",
    "                break\n",
    "            break\n",
    "#                 epoch_x = epoch_x.reshape((batch_size,n_chunks,chunk_size))\n",
    "\n",
    "#                 _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})\n",
    "#                 epoch_loss += c\n",
    "\n",
    "#             print('Epoch', epoch, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
    "\n",
    "#         correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "\n",
    "#         accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "#         print('Accuracy:',accuracy.eval({x:mnist.test.images.reshape((-1, n_chunks, chunk_size)), y:mnist.test.labels}))\n",
    "\n",
    "train_neural_network(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1.  2.  3.]\n",
      "  [ 4.  5.  6.]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 2.],\n",
       "       [ 3.],\n",
       "       [ 4.],\n",
       "       [ 5.],\n",
       "       [ 6.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# a = np.array([1,2,3,4,5,6])\n",
    "# np.transpose(a, (1,0,2))\n",
    "\n",
    "a = np.array([[[ 1.,  2.,  3.], [ 4.,  5.,  6.]]])\n",
    "#a = np.ones((1,2,3))\n",
    "print (a)\n",
    "np.transpose(a, (1,0,2))\n",
    "np.reshape(a, [-1, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
