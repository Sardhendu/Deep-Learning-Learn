{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Files\n",
    "#%pylab inline\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from six.moves import cPickle as pickle\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Tensor-Flow-Learn/DataFolder/Training/a', '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Tensor-Flow-Learn/DataFolder/Training/b', '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Tensor-Flow-Learn/DataFolder/Training/c']\n",
      "\n",
      "/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Tensor-Flow-Learn/DataFolder/Training/a\n",
      "Running for folder a\n",
      "Yes+Yes--The pickle file exists\n",
      "\n",
      "/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Tensor-Flow-Learn/DataFolder/Training/b\n",
      "Running for folder b\n",
      "Yes+Yes--The pickle file exists\n",
      "\n",
      "/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Tensor-Flow-Learn/DataFolder/Training/c\n",
      "Running for folder c\n",
      "Yes+Yes--The pickle file exists\n"
     ]
    }
   ],
   "source": [
    "# Training Dataset Builder: Get the pixels, prepross them store the dataset using pickle files into the disk\n",
    "\n",
    "image_size = 28\n",
    "\n",
    "def image_pixel_standarize(image_pixel_val):\n",
    "    return(image_pixel_val - 255.0/2)/255.0\n",
    "    \n",
    "\n",
    "def bld_pixels_arr(folder_name, min_no_of_image, max_no_of_image=None):\n",
    "    # Build the pickle file for one Alphabet Image\n",
    "    if max_no_of_image:\n",
    "        image_filenames = os.listdir(folder_name)[0:max_no_of_image]\n",
    "    else:\n",
    "        image_filenames = os.listdir(folder_name)\n",
    "\n",
    "    # Assign a numpy multudimensional array to store the image pixels, the image is 28*28 pixels\n",
    "    dataset = np.ndarray(shape=(len(image_filenames), \n",
    "                                image_size, \n",
    "                                image_size\n",
    "                               ),\n",
    "                         dtype=np.float32)\n",
    "    print (dataset.shape)\n",
    "    for num_images, image in enumerate(image_filenames):\n",
    "#         print (num_images)\n",
    "        image_file_dir = os.path.join(folder_name, image)\n",
    "        try:\n",
    "            image_pixels = ndimage.imread(image_file_dir).astype(float)\n",
    "            image_standarized = image_pixel_standarize(image_pixels)\n",
    "\n",
    "            if image_standarized.shape != (image_size, image_size):\n",
    "                raise Exception('Unexpected image shape: %s' % str(image_standarized.shape))\n",
    "            dataset[num_images, :, :] = image_standarized\n",
    "        except IOError as e:\n",
    "            print('Could not read:', image, ':', e, '- hence skipping.')\n",
    "        \n",
    "    dataset = dataset[0:num_images+1, :, :]  # Will combine all the image pixels for 1 type Example All the 'a'\n",
    "#     print (dataset.shape)\n",
    "#     print (dataset)\n",
    "    if num_images < min_no_of_image:\n",
    "        raise Exception('Many fewer images than expected: %d < %d' %(num_images, min_no_of_image))\n",
    "\n",
    "    print('Complete Training/Crossvalidation dataset :', dataset.shape)\n",
    "    print('Mean:', np.mean(dataset))\n",
    "    print('Standard deviation:', np.std(dataset))    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "        \n",
    "def crt_dataset(data_folders, dataset_type, min_no_of_image, max_no_of_image=None, force=None):    \n",
    "    dataset_dir_n_names = []\n",
    "    # One folder would comtain all the images pertaining to one particular alphabet.\n",
    "    for folder in data_folders:  # [0:2]  \n",
    "        print ('')\n",
    "        print (folder)\n",
    "        print ('Running for folder',os.path.basename(folder))\n",
    "        set_filename = dataset_type + os.path.basename(folder) + '.pickle'\n",
    "#         print (set_filename)\n",
    "        dataset_dir_n_names.append(set_filename)\n",
    "#         Ckech if the pickle file already exists, if not then we create one for each\n",
    "        if os.path.exists(set_filename) and not force:\n",
    "            print ('Yes+Yes--The pickle file exists')\n",
    "        else:\n",
    "            dataset = bld_pixels_arr(folder, min_no_of_image=min_no_of_image, max_no_of_image=max_no_of_image)\n",
    "            try:\n",
    "                with open(set_filename, 'wb') as f:\n",
    "                    pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "            except Exception as e:\n",
    "                print('Unable to save data to', set_filename, ':', e)\n",
    "    return dataset_dir_n_names\n",
    "                \n",
    "\n",
    "DataPath_train = \"/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Tensor-Flow-Learn/DataFolder/Training/\"\n",
    "data_folders_train = [os.path.join(DataPath_train,folders) for folders in os.listdir(DataPath_train) if folders!='.DS_Store']    \n",
    "print (data_folders_train)\n",
    "\n",
    "training_dataset_dir = '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Tensor-Flow-Learn/DataPrepared/Training/'\n",
    "# test_dataset_dir = \"/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Udacity/Datapreparation/test_dataset/\"\n",
    "\n",
    "training_data_names_n_dir = crt_dataset(data_folders=data_folders_train, \n",
    "                            dataset_type=training_dataset_dir, \n",
    "                            min_no_of_image=4, \n",
    "                            max_no_of_image=None, \n",
    "                            force=None)\n",
    "# test_data_names_n_dir = crt_dataset(data_folders=data_folders_test, \n",
    "#                             dataset_type=test_dataset_dir, \n",
    "#                             min_no_of_image=1800, \n",
    "#                             max_no_of_image=None, \n",
    "#                             force=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Tensor-Flow-Learn/DataPrepared/Training/a.pickle', '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Tensor-Flow-Learn/DataPrepared/Training/b.pickle', '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Tensor-Flow-Learn/DataPrepared/Training/c.pickle']\n",
      "training/teting size per class =  9\n",
      "crossvalid size per class =  3\n"
     ]
    }
   ],
   "source": [
    "# Create training and crossvalidation dataset \n",
    "def initialize_arrays(no_of_images, img_size):\n",
    "    dataset = np.ndarray((no_of_images, img_size, img_size), dtype=np.float32)\n",
    "    labels = np.ndarray(no_of_images, dtype=np.int32)\n",
    "    return dataset, labels\n",
    "\n",
    "def train_valid_test_builder(training_data_names_n_dir, training_size, crossvalid_size=0):\n",
    "    no_of_labels = len(training_data_names_n_dir)\n",
    "    training_dataset, training_labels = initialize_arrays(training_size, image_size)\n",
    "    if crossvalid_size!=0:\n",
    "        crossvalid_dataset, crossvalid_labels = initialize_arrays(crossvalid_size, image_size)\n",
    "    else:\n",
    "        crossvalid_dataset, crossvalid_labels = None, None\n",
    "\n",
    "    training_size_per_class = training_size // no_of_labels\n",
    "    crossvalid_size_per_class = crossvalid_size // no_of_labels\n",
    "    print ('training/teting size per class = ',training_size_per_class)\n",
    "    print ('crossvalid size per class = ', crossvalid_size_per_class)\n",
    "    \n",
    "    start_v, start_t = 0, 0\n",
    "    end_v, end_t = crossvalid_size_per_class, training_size_per_class\n",
    "    end_vt = crossvalid_size_per_class+training_size_per_class\n",
    "\n",
    "    for label, pickle_files in enumerate(training_data_names_n_dir):      \n",
    "#         print ('Running for alphabet: ', os.path.basename(pickle_files))\n",
    "        try:\n",
    "            with open(pickle_files, 'rb') as f:\n",
    "                letter_data = pickle.load(f)\n",
    "                np.random.shuffle(letter_data)          # This will suffle the data which will give the top elements as random\n",
    "                if crossvalid_dataset is not None:\n",
    "                    crossvalid_for_letter = letter_data[0:crossvalid_size_per_class, :, :]\n",
    "                    crossvalid_dataset[start_v:end_v, :, :] = crossvalid_for_letter\n",
    "                    crossvalid_labels[start_v:end_v] = label\n",
    "                    start_v += crossvalid_size_per_class\n",
    "                    end_v += crossvalid_size_per_class\n",
    "\n",
    "                training_for_letter = letter_data[crossvalid_size_per_class:end_vt, :, :]\n",
    "                training_dataset[start_t:end_t, :, :] = training_for_letter\n",
    "                training_labels[start_t:end_t] = label\n",
    "                start_t += training_size_per_class\n",
    "                end_t += training_size_per_class\n",
    "        except Exception as e:\n",
    "            print('Unable to process data from', pickle_files, ':', e)\n",
    "            raise\n",
    "    return crossvalid_dataset, crossvalid_labels, training_dataset, training_labels\n",
    "\n",
    "\n",
    "\n",
    "# Load the pixel Data Stored\n",
    "training_size = 27      # Total Train size including all different labels\n",
    "crossvalid_size = 9     # Total Crossvalid size including all different labels\n",
    "batch_size = 12\n",
    "\n",
    "print (training_data_names_n_dir)\n",
    "crossvalid_dataset, crossvalid_labels, training_dataset, training_labels = \\\n",
    "train_valid_test_builder(training_data_names_n_dir, training_size, crossvalid_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed pickle size: 113426\n"
     ]
    }
   ],
   "source": [
    "complete_dataset_path = '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Tensor-Flow-Learn/DataPrepared/dataset_complete.p'\n",
    "\n",
    "try:\n",
    "    f = open(complete_dataset_path, 'wb')\n",
    "    dataset_complete = {\n",
    "        'training_dataset': training_dataset,\n",
    "        'training_labels': training_labels,\n",
    "        'crossvalid_dataset': crossvalid_dataset,\n",
    "        'crossvalid_labels': crossvalid_labels,\n",
    "#         'test_dataset': test_dataset,\n",
    "#         'test_labels': test_labels,\n",
    "    }\n",
    "    pickle.dump(dataset_complete, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', complete_dataset_path, ':', e)\n",
    "    raise\n",
    "\n",
    "    \n",
    "statinfo = os.stat(complete_dataset_path)\n",
    "print('Compressed pickle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 0 1 2 0 1 2]\n",
      "[0 1 2 0 1 2 0 1 2]\n",
      "[0 1 2 0 1 2 0 1 2]\n"
     ]
    }
   ],
   "source": [
    "# Loading The picke file and building mini-batches from the training dataset\n",
    "complete_dataset_path = '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Tensor-Flow-Learn/DataPrepared/dataset_complete.p'\n",
    "with open(complete_dataset_path, 'rb') as f:\n",
    "    fnl_dataset = pickle.load(f)\n",
    "    training_dataset = (fnl_dataset['training_dataset'])\n",
    "    training_labels = (fnl_dataset['training_labels'])\n",
    "#     test_dataset = (fnl_dataset['test_dataset'])\n",
    "#     test_labels = (fnl_dataset['test_labels'])\n",
    "    crossvalid_dataset = (fnl_dataset['crossvalid_dataset'])\n",
    "    crossvalid_labels = (fnl_dataset['crossvalid_labels'])\n",
    "  \n",
    "\n",
    "\n",
    "#Mini-Batch Creation\n",
    "batch_indices = []\n",
    "def batch_indice_creator(training_labels, batch_size_per_class):\n",
    "    \"\"\"Create batch with random samples and return appropriate format\"\"\"\n",
    "    label_rand_arr = []\n",
    "    for unq_labels in np.unique(training_labels):\n",
    "        arr = np.array(np.where(training_labels==unq_labels)[0])\n",
    "        np.random.shuffle(arr)\n",
    "#         print ('the random array is: ', np.reshape(arr, (len(arr),1)))\n",
    "        if np.any(label_rand_arr):\n",
    "            label_rand_arr = np.hstack((label_rand_arr, np.reshape(arr, (len(arr),1))))\n",
    "        else:\n",
    "            label_rand_arr = np.reshape(arr, (len(arr),1))\n",
    "    batch_indices = np.array_split(label_rand_arr.flatten(), batch_size_per_class)\n",
    "    return batch_indices\n",
    "\n",
    "\n",
    "training_batch_dir = '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Tensor-Flow-Learn/DataPrepared/'        \n",
    "batch_training_dict = {}\n",
    "def batch_store(training_dataset, training_labels, batch_indices):\n",
    "    for no, indices in enumerate(batch_indices):\n",
    "        print (training_labels[indices])\n",
    "        try:\n",
    "            f = open(training_batch_dir+'batch'+str(no)+'.pickle', 'wb')\n",
    "            batch = {\n",
    "                'batch_train_dataset': training_dataset[indices],\n",
    "                'batch_train_labels': training_labels[indices],\n",
    "            }\n",
    "            pickle.dump(batch, f, pickle.HIGHEST_PROTOCOL)\n",
    "            f.close()\n",
    "        except Exception as e:\n",
    "            print('Unable to save data to', complete_dataset_path, ':', e)\n",
    "            raise\n",
    "            \n",
    "batch_indices = batch_indice_creator(training_labels, 3)\n",
    "batch_store(training_dataset, training_labels, batch_indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 3)\n",
      "(9, 784)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the matrix and final matrix builder \n",
    "# tensor flow takes the labels input as binary code, where Alphabet A whose binary value is 0 will turn to a array\n",
    "# with elements [1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0] and B becomes [0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "def reshape_data(dataset, labels, sample_size=None):\n",
    "    no_of_labels = len(np.unique(labels))\n",
    "    if sample_size:\n",
    "        dataset = dataset[:sample_size].reshape(sample_size, image_size * image_size) # To reshape the  \n",
    "        # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "        labels = (np.arange(no_of_labels) == labels[:,None]).astype(np.float32)\n",
    "    else:\n",
    "        dataset = dataset.reshape(len(dataset), image_size * image_size) # To reshape the  \n",
    "        # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "        labels = (np.arange(no_of_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "# Retrieve the data of first batch:\n",
    "training_batch_dir = '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Tensor-Flow-Learn/DataPrepared/batch0.pickle'        \n",
    "with open(training_batch_dir, 'rb') as f:\n",
    "    batch_dataset = pickle.load(f)\n",
    "    training_dataset_ = (batch_dataset['batch_train_dataset'])\n",
    "    training_labels_ = (batch_dataset['batch_train_labels'])\n",
    "\n",
    "training_dataset_, training_labels_ = reshape_data(training_dataset_, training_labels_)\n",
    "\n",
    "print (training_labels_.shape)\n",
    "print (training_dataset_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_196:0\", shape=(2, 2), dtype=float32)\n",
      "Tensor(\"Const_197:0\", shape=(2, 2), dtype=float32)\n",
      "{'hid_to_output_wghts_delta': <tf.Tensor 'add_13:0' shape=(3, 2) dtype=float32>, 'input_to_hid_wghts_delta': <tf.Tensor 'add_12:0' shape=(2, 3) dtype=float32>, 'input_to_hid_wghts': <tf.Tensor 'sub_29:0' shape=(2, 3) dtype=float32>, 'hid_to_output_wghts': <tf.Tensor 'sub_30:0' shape=(3, 2) dtype=float32>}\n",
      "{'hid_bias_delta': <tf.Tensor 'add_14:0' shape=(3,) dtype=float32>, 'output_bias': <tf.Tensor 'sub_32:0' shape=(2,) dtype=float32>, 'output_bias_delta': <tf.Tensor 'add_15:0' shape=(2,) dtype=float32>, 'hid_bias': <tf.Tensor 'sub_31:0' shape=(3,) dtype=float32>}\n",
      "Tensor(\"Add_6:0\", shape=(2, 3), dtype=float32)\n",
      "Tensor(\"Sigmoid_61:0\", shape=(2, 3), dtype=float32)\n",
      "Tensor(\"Add_7:0\", shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# # Declare and initialize the variables, In short Construct the whole Graph:\n",
    "# no_inp_unit = 28*28\n",
    "# no_hid_unit = 500\n",
    "# no_output_unit = 3\n",
    "# learning_rate = 0.01\n",
    "\n",
    "# # Define Placeholders inputs and labels\n",
    "# tf_training_dataset = tf.constant(training_dataset_)\n",
    "# tf_training_labels = tf.constant(training_labels_)\n",
    "\n",
    "###########  Practise  #############\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "no_inp_unit = 2\n",
    "no_hid_unit = 3\n",
    "no_output_unit = 2\n",
    "no_of_labels = 2\n",
    "momentum = 0.9\n",
    "learning_rate = 0.1\n",
    "batch_size = 2\n",
    "\n",
    "training_dataset_ = np.array([[2,3],[4,5]], dtype='float32')\n",
    "training_labels_ = (np.arange(no_of_labels) == np.array([0,1], dtype=float)[:,None]).astype(np.float32)\n",
    "tf_training_dataset = tf.constant(training_dataset_)\n",
    "tf_training_labels = tf.constant(training_labels_)\n",
    "###########  Practise  #############\n",
    "\n",
    "seed = 128\n",
    "rng = np.random.RandomState(seed)\n",
    "\n",
    "weights = {\n",
    "    'input_to_hid_wghts': tf.Variable(tf.random_normal([no_inp_unit, no_hid_unit], seed=seed)),\n",
    "    'hid_to_output_wghts': tf.Variable(tf.random_normal([no_hid_unit, no_output_unit], seed=seed)),\n",
    "    'input_to_hid_wghts_delta': tf.Variable(tf.zeros([no_inp_unit, no_hid_unit])),\n",
    "    'hid_to_output_wghts_delta': tf.Variable(tf.zeros([no_hid_unit, no_output_unit])),\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'hid_bias' : tf.Variable(tf.zeros([no_hid_unit])),\n",
    "    'output_bias' : tf.Variable(tf.zeros([no_output_unit])),\n",
    "    'hid_bias_delta' : tf.Variable(tf.zeros([no_hid_unit])),\n",
    "    'output_bias_delta' : tf.Variable(tf.zeros([no_output_unit]))\n",
    "}\n",
    "\n",
    "\n",
    "###### Forward Propagate ######\n",
    "# Hidden Layer\n",
    "input_to_hid_layer = tf.add(tf.matmul(tf_training_dataset, weights['input_to_hid_wghts']), biases['hid_bias'])\n",
    "hid_layer_state = tf.sigmoid(inputs_to_hid, name=None)\n",
    "\n",
    "# Output Layer\n",
    "hid_to_output_layer = tf.add(tf.matmul(hid_layer_state, weights['hid_to_output_wghts']), biases['output_bias'])\n",
    "output_layer_state = tf.nn.softmax(hid_to_output_layer, name=None)\n",
    "error_derivative = tf.sub(output_layer_state, tf_training_labels, name=None)  # -(y - y_hat) = (y_hat - y)\n",
    "# The above three lines of code can also be combined into one line as below.\n",
    "loss_CE = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(hid_to_output_layer, tf_training_labels))\n",
    "\n",
    "\n",
    "####### Back Propagate #######\n",
    "# Output_Layer\n",
    "hid_to_output_wghts_gradient =  tf.matmul(tf.transpose(hid_layer_state), error_derivative)\n",
    "output_bias_gradient =  tf.reduce_sum(error_derivative, 0)  # sum all the row_value for each column \n",
    "\n",
    "# Hidden_Layer\n",
    "back_propagate_deriv = tf.mul(tf.transpose(tf.matmul(weights['hid_to_output_wghts'], tf.transpose(error_derivative))),\n",
    "                              tf.mul(hid_layer_state, 1.0-hid_layer_state)\n",
    "                             )\n",
    "input_to_hid_wghts_gradient = tf.matmul(tf.transpose(tf_training_dataset), back_propagate_deriv)\n",
    "hid_bias_gradient = tf.reduce_sum(back_propagate_deriv, 0)\n",
    "\n",
    "####### Weight Update  #######\n",
    "# Update Weights\n",
    "weights['input_to_hid_wghts_delta'] = tf.mul(momentum, weights['input_to_hid_wghts_delta']) \\\n",
    "                            + tf.div(input_to_hid_wghts_gradient, batch_size)\n",
    "weights['input_to_hid_wghts'] = weights['input_to_hid_wghts'] - tf.mul(learning_rate,weights['input_to_hid_wghts_delta'])\n",
    "\n",
    "weights['hid_to_output_wghts_delta'] = tf.mul(momentum, weights['hid_to_output_wghts_delta']) \\\n",
    "                            + tf.div(hid_to_output_wghts_gradient, batch_size)\n",
    "weights['hid_to_output_wghts'] = weights['hid_to_output_wghts'] - tf.mul(learning_rate,weights['hid_to_output_wghts_delta'])\n",
    "\n",
    "\n",
    "####### Bias Update  #######\n",
    "biases['hid_bias_delta'] = tf.mul(momentum, biases['hid_bias_delta']) + tf.div(hid_bias_gradient, batch_size)\n",
    "biases['hid_bias'] = biases['hid_bias'] - tf.mul(learning_rate,biases['hid_bias_delta'])\n",
    "\n",
    "biases['output_bias_delta'] = tf.mul(momentum, biases['output_bias_delta']) + tf.div(output_bias_gradient, batch_size)\n",
    "biases['output_bias'] = biases['output_bias'] - tf.mul(learning_rate,biases['output_bias_delta'])\n",
    "\n",
    "\n",
    "\n",
    "#embed_to_hid_weights = embed_to_hid_weights - learning_rate * embed_to_hid_weights_delta\n",
    "\n",
    "# Optimizer: Lets just use Gradient Descend\n",
    "# learning_rate = 0.5\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss_CE)\n",
    "\n",
    "print (tf_training_dataset)\n",
    "print (tf_training_labels)\n",
    "print (weights)\n",
    "print (biases)\n",
    "print (inputs_to_hid)\n",
    "print (hid_layer_state)\n",
    "print (output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Pass\n",
      "The input_to_hid_layer is:  [array([ 0.02459085,  2.61364675, -3.66876554], dtype=float32), array([-0.46376991,  5.2827177 , -7.14768505], dtype=float32)]\n",
      "\n",
      "The hid_layer_state is:  [array([ 0.50614744,  0.93173474,  0.02487347], dtype=float32), array([  3.86091858e-01,   9.94947135e-01,   7.86064717e-04], dtype=float32)]\n",
      "\n",
      "The hid_to_output_layer is:  [array([-1.82842767,  1.17673755], dtype=float32), array([-1.83414948,  1.04686284], dtype=float32)]\n",
      "\n",
      "The output_layer_state is:  [array([ 0.04719308,  0.95280689], dtype=float32), array([ 0.05310021,  0.94689977], dtype=float32)]\n",
      "\n",
      "The error_derivative is:  [array([-0.95280695,  0.95280689], dtype=float32), array([ 0.05310021, -0.05310023], dtype=float32)]\n",
      "\n",
      "The Loss value:  1.55404\n",
      "\n",
      "\n",
      "Back Propagate\n",
      "The hid_to_output_wghts_gradient is:  [array([-0.46175924,  0.46175921], dtype=float32), array([-0.83493143,  0.83493131], dtype=float32), array([-0.02365787,  0.02365787], dtype=float32)]\n",
      "\n",
      "The hid_to_output_bias_gradient is:  [-0.89970672, 0.89970666]\n",
      "\n",
      "The back_propagate_deriv is:  [array([ 0.51136369,  0.12499845, -0.00310651], dtype=float32), array([ -2.70234644e-02,  -5.50606812e-04,   5.60639455e-06], dtype=float32)]\n",
      "\n",
      "The input_to_hid_wghts_gradient is:  [array([ 0.91463351,  0.24779448, -0.00619059], dtype=float32), array([ 1.3989737 ,  0.37224233, -0.0092915 ], dtype=float32)]\n",
      "\n",
      "\n",
      "Weight Update\n",
      "The input_to_hid_wghts_delta is:  [array([ 0.45731676,  0.12389724, -0.0030953 ], dtype=float32), array([ 0.69948685,  0.18612117, -0.00464575], dtype=float32)]\n",
      "\n",
      "The new input_to_hid_wghts is:  [array([-0.8028639 ,  1.37757003, -1.54930377], dtype=float32), array([ 0.44300312, -0.07403639, -0.18938172], dtype=float32)]\n",
      "\n",
      "The hid_to_output_wghts_delta is:  [array([-0.23087962,  0.2308796 ], dtype=float32), array([-0.41746572,  0.41746566], dtype=float32), array([-0.01182894,  0.01182894], dtype=float32)]\n",
      "\n",
      "The new hid_to_output_wghts is:  [array([-0.73404425,  1.36687183], dtype=float32), array([-1.50786674,  0.47120523], dtype=float32), array([-0.05424138, -0.19102919], dtype=float32)]\n",
      "\n",
      "\n",
      "Bias Update\n",
      "The hid_bias_delta is:  [0.24217011, 0.062223922, -0.0015504513]\n",
      "\n",
      "The new hid_bias is:  [-0.024217011, -0.0062223924, 0.00015504513]\n",
      "\n",
      "The output_bias_delta is:  [-0.44985336, 0.44985333]\n",
      "\n",
      "The new output_bias is:  [0.044985335, -0.044985335]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Forward Pass\n",
      "The input_to_hid_layer is:  [array([ 0.02459085,  2.61364675, -3.66876554], dtype=float32), array([-0.46376991,  5.2827177 , -7.14768505], dtype=float32)]\n",
      "\n",
      "The hid_layer_state is:  [array([ 0.50614744,  0.93173474,  0.02487347], dtype=float32), array([  3.86091858e-01,   9.94947135e-01,   7.86064717e-04], dtype=float32)]\n",
      "\n",
      "The hid_to_output_layer is:  [array([-1.82842767,  1.17673755], dtype=float32), array([-1.83414948,  1.04686284], dtype=float32)]\n",
      "\n",
      "The output_layer_state is:  [array([ 0.04719308,  0.95280689], dtype=float32), array([ 0.05310021,  0.94689977], dtype=float32)]\n",
      "\n",
      "The error_derivative is:  [array([-0.95280695,  0.95280689], dtype=float32), array([ 0.05310021, -0.05310023], dtype=float32)]\n",
      "\n",
      "The Loss value:  1.55404\n",
      "\n",
      "\n",
      "Back Propagate\n",
      "The hid_to_output_wghts_gradient is:  [array([-0.46175924,  0.46175921], dtype=float32), array([-0.83493143,  0.83493131], dtype=float32), array([-0.02365787,  0.02365787], dtype=float32)]\n",
      "\n",
      "The hid_to_output_bias_gradient is:  [-0.89970672, 0.89970666]\n",
      "\n",
      "The back_propagate_deriv is:  [array([ 0.51136369,  0.12499845, -0.00310651], dtype=float32), array([ -2.70234644e-02,  -5.50606812e-04,   5.60639455e-06], dtype=float32)]\n",
      "\n",
      "The input_to_hid_wghts_gradient is:  [array([ 0.91463351,  0.24779448, -0.00619059], dtype=float32), array([ 1.3989737 ,  0.37224233, -0.0092915 ], dtype=float32)]\n",
      "\n",
      "\n",
      "Weight Update\n",
      "The input_to_hid_wghts_delta is:  [array([ 0.45731676,  0.12389724, -0.0030953 ], dtype=float32), array([ 0.69948685,  0.18612117, -0.00464575], dtype=float32)]\n",
      "\n",
      "The new input_to_hid_wghts is:  [array([-0.8028639 ,  1.37757003, -1.54930377], dtype=float32), array([ 0.44300312, -0.07403639, -0.18938172], dtype=float32)]\n",
      "\n",
      "The hid_to_output_wghts_delta is:  [array([-0.23087962,  0.2308796 ], dtype=float32), array([-0.41746572,  0.41746566], dtype=float32), array([-0.01182894,  0.01182894], dtype=float32)]\n",
      "\n",
      "The new hid_to_output_wghts is:  [array([-0.73404425,  1.36687183], dtype=float32), array([-1.50786674,  0.47120523], dtype=float32), array([-0.05424138, -0.19102919], dtype=float32)]\n",
      "\n",
      "\n",
      "Bias Update\n",
      "The hid_bias_delta is:  [0.24217011, 0.062223922, -0.0015504513]\n",
      "\n",
      "The new hid_bias is:  [-0.024217011, -0.0062223924, 0.00015504513]\n",
      "\n",
      "The output_bias_delta is:  [-0.44985336, 0.44985333]\n",
      "\n",
      "The new output_bias is:  [0.044985335, -0.044985335]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now we create a session and run the session\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "#print (init)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(2):\n",
    "        print ('Forward Pass')\n",
    "        print ('The input_to_hid_layer is: ', [w for no, w in enumerate(sess.run(input_to_hid_layer))])\n",
    "        print ('')\n",
    "        print ('The hid_layer_state is: ', [w for no, w in enumerate(sess.run(hid_layer_state))])\n",
    "        print ('')\n",
    "        print ('The hid_to_output_layer is: ', [w for no, w in enumerate(sess.run(hid_to_output_layer))])\n",
    "        print ('')\n",
    "        print ('The output_layer_state is: ', [w for no, w in enumerate(sess.run(output_layer_state))])\n",
    "        print ('')\n",
    "        print ('The error_derivative is: ', [w for no, w in enumerate(sess.run(error_derivative))])\n",
    "        print ('')\n",
    "        print ('The Loss value: ', sess.run(loss_CE))\n",
    "        print ('')\n",
    "        print ('')\n",
    "\n",
    "        print ('Back Propagate')\n",
    "        print ('The hid_to_output_wghts_gradient is: ', [w for no, w in enumerate(sess.run(hid_to_output_wghts_gradient))])\n",
    "        print ('')\n",
    "        print ('The hid_to_output_bias_gradient is: ', [w for no, w in enumerate(sess.run(hid_to_output_bias_gradient))])\n",
    "        print ('')\n",
    "        print ('The back_propagate_deriv is: ', [w for no, w in enumerate(sess.run(back_propagate_deriv))])\n",
    "        print ('')\n",
    "        print ('The input_to_hid_wghts_gradient is: ', [w for no, w in enumerate(sess.run(input_to_hid_wghts_gradient))])\n",
    "        print ('')\n",
    "        print ('')\n",
    "\n",
    "        print ('Weight Update')\n",
    "        print ('The input_to_hid_wghts_delta is: ', [w for no, w in enumerate(sess.run(weights['input_to_hid_wghts_delta']))])\n",
    "        print ('')\n",
    "        print ('The new input_to_hid_wghts is: ', [w for no, w in enumerate(sess.run(weights['input_to_hid_wghts']))])\n",
    "        print ('')\n",
    "        print ('The hid_to_output_wghts_delta is: ', [w for no, w in enumerate(sess.run(weights['hid_to_output_wghts_delta']))])\n",
    "        print ('')\n",
    "        print ('The new hid_to_output_wghts is: ', [w for no, w in enumerate(sess.run(weights['hid_to_output_wghts']))])\n",
    "        print ('')\n",
    "        print ('')\n",
    "\n",
    "        print ('Bias Update')\n",
    "        print ('The hid_bias_delta is: ', [w for no, w in enumerate(sess.run(biases['hid_bias_delta']))])\n",
    "        print ('')\n",
    "        print ('The new hid_bias is: ', [w for no, w in enumerate(sess.run(biases['hid_bias']))])\n",
    "        print ('')\n",
    "        print ('The output_bias_delta is: ', [w for no, w in enumerate(sess.run(biases['output_bias_delta']))])\n",
    "        print ('')\n",
    "        print ('The new output_bias is: ', [w for no, w in enumerate(sess.run(biases['output_bias']))])\n",
    "        print ('')\n",
    "        print ('')\n",
    "        print ('')\n",
    "        print ('')\n",
    "        print ('')\n",
    "        print ('')\n",
    "    \n",
    "    ### for each epoch, do:\n",
    "    ###   for each batch, do:\n",
    "    ###     create pre-processed batch\n",
    "    ###     run optimizer by feeding batch\n",
    "    ###     find cost and reiterate to minimize\n",
    "    \n",
    "#     for epoch in range(epochs):\n",
    "#         avg_cost = 0\n",
    "#         total_batch = int(train.shape[0]/batch_size)\n",
    "#         for i in range(total_batch):\n",
    "#             batch_x, batch_y = batch_creator(batch_size, train_x.shape[0], 'train')\n",
    "#             _, c = sess.run([optimizer, cost], feed_dict = {x: batch_x, y: batch_y})\n",
    "            \n",
    "#             avg_cost += c / total_batch\n",
    "            \n",
    "#         print \"Epoch:\", (epoch+1), \"cost =\", \"{:.5f}\".format(avg_cost)\n",
    "    \n",
    "#     print \"\\nTraining complete!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
