{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from six.moves import xrange\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Create the Conv Net computation graph:\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 28, 28, 3] [None, 2]\n"
     ]
    }
   ],
   "source": [
    "imageSize = 28\n",
    "numChannels = 3\n",
    "numLabels = 2\n",
    "batchSize = 20\n",
    "numEpochs = 3\n",
    "trainSize = 100\n",
    "\n",
    "\n",
    "# declare graph operations\n",
    "reset_graph()\n",
    "# Inplementing Convolution layer\n",
    "# Declare Adaptive learning rate\n",
    "starter_learning_rate = 0.01\n",
    "globalStep = tf.Variable(0, dtype=tf.float32)\n",
    "learningRate = tf.train.exponential_decay(learning_rate = starter_learning_rate, \n",
    "                                          global_step = globalStep*batchSize, \n",
    "                                          decay_steps = trainSize, \n",
    "                                          decay_rate=0.95, \n",
    "                                          staircase=True)\n",
    "\n",
    "# Let us add the learning Rate to the Tensorflow summary\n",
    "tf.summary.scalar('Learning_Rate',learningRate)\n",
    "\n",
    "def convActivation(inputData, inpOutShape=[5,5,3,12], name=\"convLayer\"):\n",
    "    kernelY, kernelX, numImp, numOut = inpOutShape\n",
    "    with tf.name_scope(name):\n",
    "        with tf.variable_scope(name):\n",
    "            w = tf.get_variable(dtype=tf.float32, \n",
    "                                shape=inpOutShape, \n",
    "                                initializer=tf.random_normal_initializer(\n",
    "                                    mean=0, stddev=0.1, seed = 9743),\n",
    "                                name=\"convWghts\")\n",
    "            b = tf.get_variable(dtype=tf.float32,\n",
    "                               shape=[numOut],\n",
    "                               initializer=tf.constant_initializer(1.0),\n",
    "                               name=\"convBias\")\n",
    "\n",
    "            conv = tf.nn.conv2d(inputData, w, strides=[1,1,1,1], padding=\"SAME\")\n",
    "            act = tf.nn.relu(conv + b)\n",
    "            pool = tf.nn.max_pool(act, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
    "            \n",
    "            # Load summaries into Tensorboard\n",
    "            tf.summary.histogram(\"convWeights\", w)\n",
    "            tf.summary.histogram(\"convBiases\", b)\n",
    "            tf.summary.histogram('reluActivation', act)\n",
    "            return pool\n",
    "    \n",
    "# Implement fully connected layer\n",
    "def fcLayer(inputData, inpOutShape, name=\"fcLayer\"):\n",
    "    numInp, numOut = inpOutShape\n",
    "    with tf.name_scope(name):\n",
    "        with tf.variable_scope(name):\n",
    "            w = tf.get_variable(dtype=tf.float32,\n",
    "                               shape=inpOutShape,\n",
    "                               initializer=tf.random_normal_initializer(\n",
    "                                   mean=0, stddev=0.1, seed=2349),\n",
    "                               name=\"fcWghts\")\n",
    "            b = tf.get_variable(dtype=tf.float32,\n",
    "                               shape=[numOut],\n",
    "                               initializer=tf.constant_initializer(1.0),\n",
    "                               name=\"fcBias\")\n",
    "            convToFc1 = tf.matmul(inputData, w) + b\n",
    "\n",
    "            tf.summary.histogram('fcWeights', w)\n",
    "            tf.summary.histogram('fcBiases', b)\n",
    "            return tf.nn.relu(convToFc1)\n",
    "    \n",
    "# Build the forward feed network\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, imageSize, imageSize, numChannels], name='xInp')\n",
    "y = tf.placeholder(tf.float32, shape=[None, numLabels], name=\"xLabels\")\n",
    "print (x.get_shape().as_list(), y.get_shape().as_list())\n",
    "conv1 = convActivation(inputData=x,\n",
    "                      inpOutShape=[5,5,3,12],\n",
    "                      name = \"conv1\")\n",
    "conv2 = convActivation(inputData=conv1,\n",
    "                      inpOutShape=[5,5,12,12],\n",
    "                      name = \"conv2\")\n",
    "\n",
    "shapeY, shapeX, depth = conv2.get_shape().as_list()[1:4]\n",
    "flattenedShape = shapeY * shapeX * depth\n",
    "conv2Flattened = tf.reshape(conv2, [-1, flattenedShape])\n",
    "fcOut1 = fcLayer(inputData=conv2Flattened,\n",
    "                inpOutShape=[flattenedShape, 28],\n",
    "                name=\"fcLayer1\")\n",
    "fcOut2 = fcLayer(inputData=fcOut1,\n",
    "                inpOutShape=[28,numLabels],\n",
    "                name=\"fcLayer2\")\n",
    "\n",
    "# Now we send our input to the softmax function to calculate the cross entropy loss\n",
    "with tf.name_scope(\"trainLoss\"):\n",
    "    loss = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(logits=fcOut2, labels=y)\n",
    "    )\n",
    "    # Add loss to the TensorBoard summary\n",
    "    tf.summary.scalar(\"Loss\",loss)\n",
    "\n",
    "# Implement the optimizer\n",
    "with tf.name_scope(\"trainOpt\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learningRate).minimize(loss, global_step=globalStep)\n",
    "\n",
    "# finally we calculate the accuracy\n",
    "with tf.name_scope(\"Accuracy\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(fcOut2,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # Add the accuracy to the summary\n",
    "    tf.summary.scalar(\"Accuracy\",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# [op for op in tf.get_default_graph().get_operations()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Session and Train:\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "summaryOutput = \"/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/TensorFlowOPS/\"\n",
    "\n",
    "def summaryBuilder(sess, outFilePath):\n",
    "    mergedSummary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(outFilePath)\n",
    "    writer.add_graph(sess.graph)\n",
    "    return mergedSummary, writer\n",
    "    \n",
    "# Here we build fake mnist data\n",
    "def fake_data(trainSize, numFeatures):\n",
    "    \"\"\"Generate a fake dataset that matches the dimensions of MNIST.\"\"\"\n",
    "    data = np.random.rand(\n",
    "        trainSize, imageSize, imageSize, 3)\n",
    "    labels = np.zeros(shape=(trainSize,), dtype=np.int64)\n",
    "    for image in xrange(trainSize):\n",
    "        label = image % 2\n",
    "#         data[image, 0] = label - 0.5\n",
    "        labels[image] = label\n",
    "    return data, labels\n",
    "\n",
    "trainData, trainLabels = fake_data(trainSize=trainSize, numFeatures=3)\n",
    "\n",
    "trainLabels = np.eye(numLabels)[trainLabels]\n",
    "numSteps = int((numEpochs * trainSize)) // batchSize\n",
    "# Capture the remainder if in case the trainSize is not ezactly divisible by batchSize.  If such is the case then we may miss some training set. \n",
    "# For example, if trainingSize = 11 and batchSize=2 them since 10%2 = 0, but 11%2 = 1. So when we itereate over a batch of 2 then for every epoch \n",
    "# we miss one trainig example\n",
    "remainder = trainSize % batchSize  \n",
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "#     print ('Total number of Steps are: ', numSteps)\n",
    "    # Now that the computation graph is initialized, we would want to merge all the summaties\n",
    "    # to display in the tensor board.\n",
    "    mergedSummary, writer = summaryBuilder(sess, summaryOutput+\"/newFile\")\n",
    "    for step in np.arange(numSteps):\n",
    "        offset = (step * batchSize) % (trainSize-remainder)\n",
    "#         print ('(Step * batchSize) modulus (trainSize - remainder) = offset (%s * %s) modulus (%s - %s) = %s : '%(str(step), str(batchSize), str(trainSize), str(remainder), str(offset)))\n",
    "        if offset == (trainSize - remainder - batchSize):\n",
    "            batchData = trainData[offset:(offset+batchSize+remainder),:]\n",
    "            batchLabels = trainLabels[offset:(offset+batchSize+remainder),:]\n",
    "        else:\n",
    "            batchData = trainData[offset:(offset+batchSize),:]\n",
    "            batchLabels = trainLabels[offset:(offset+batchSize),:]\n",
    "            \n",
    "        feedDict = {x:batchData, y:batchLabels}\n",
    "        \n",
    "        # Before we run the batch we would like to capture and store summaries into disk\n",
    "        # But we dont do it for all the batches because we dont want to overload Tensorboard with the \n",
    "        # statistics from all the steps.\n",
    "        if (step%2) == 0:\n",
    "            smry = sess.run(mergedSummary, feed_dict = feedDict)\n",
    "            writer.add_summary(smry, step)\n",
    "#         print (batchData.shape)\n",
    "        \n",
    "        # Now we run the optimizer \n",
    "        fc_out, lessCE, acc, gs, lr, _ = sess.run([fcOut2, loss, accuracy,\n",
    "                                                        globalStep, learningRate, \n",
    "                                                        optimizer],feed_dict=feedDict)\n",
    "#         print ('Accuracy : ', acc)\n",
    "#         print (\"globalStep : \", gs)\n",
    "#         print ('learningRate : ', lr)\n",
    "        \n",
    "    # We have many summaries (histogram, scaler and etc.), we would want to merge them all into one graph\n",
    "#     merged_summary = tf.summary.merge_all()\n",
    "    # Here we store the session into the Disk\n",
    "    writer = tf.summary.FileWriter(summaryOutput, sess.graph)\n",
    "#     writer = tf.summary.FileWriter(summaryOutput)\n",
    "#     writer.add_graph(sess.graph)\n",
    "    writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data = np.ndarray(\n",
    "        shape=(20, 28, 28, 3),\n",
    "        dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 28, 28, 3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
