{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNote : Before running the code below ensure you have all the data in TFRecord and appertaining format.\\n        Else : Follow the code inside TensorFlowIO module to create such files. THis module assumes that \\n        the data are in their proper directories\\n'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "from tensorflow.models import slim\n",
    "from tensorflow.models.slim.preprocessing import inception_preprocessing\n",
    "\n",
    "from tensorflow.contrib.slim.python.slim.data import tfexample_decoder, dataset, dataset_data_provider, data_provider\n",
    "import os, time\n",
    "\n",
    "\"\"\"\n",
    "Note : Before running the code below ensure you have all the data in TFRecord and appertaining format.\n",
    "        Else : Follow the code inside TensorFlowIO module to create such files. THis module assumes that \n",
    "        the data are in their proper directories\n",
    "\"\"\"\n",
    "\n",
    "# The below code is copied from the code source mentioned below. \n",
    "# Code Source : https://kwotsin.github.io/tech/2017/02/11/transfer-learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Directory Listings\n",
    "\n",
    "# Log file creation\n",
    "logDir = './log'\n",
    "\n",
    "# State the image size we are resizing\n",
    "imageSize = 299 # the default size for inception\n",
    "\n",
    "# State the path where the label file is stored\n",
    "labelFile = \"/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/TensorFlowOPS/flowers/labels.txt\"\n",
    "labels = open(labelFile, 'r')\n",
    "\n",
    "# Dictionary to refer each label to its string name\n",
    "labelsToNameDict = {}\n",
    "for record in labels:\n",
    "    label, stringName = record.split(':')\n",
    "    stringName = stringName[:-1]\n",
    "    labelsToNameDict[int(label)] = stringName\n",
    "    \n",
    "# Record the file pattern for the TFRecord files\n",
    "filePattern = \"flowers_%s_*.tfRecord\"\n",
    "\n",
    "# A dictionary describing the dataset, to be required by tensorflow dataset Class\n",
    "itemToDescription = {\n",
    "    'image': \"A 3-channel RGB coloured five different types of flowers, tulips, sunflower, roses, dandelion, daisy\",\n",
    "    'label': \"A label value defined to each flower 1: dandelion, 2:roses, 3:sunflower, 4:tulips\"\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function details:\n",
    "\n",
    "(1) **getSplits**: It allows us to obtain a specific split form the disk. We store two different types of files in the disk \"trai file and validation file\". These files were split into two and get_split helps us to ge the required split.\n",
    "\n",
    "In the process we also collect some information such as number_of samples in the split, and etc.\n",
    "\n",
    "(2) **loadBatch**  The most important function in this is the DatasetDataProvider\n",
    "\n",
    "**DatasetDataProvider:** Our Dataset is provided in tfrecord format, the motive we have here is that we need tensors from our dataset to feed it as batches to the algorithm. DatasetDataProvider helps us doing that. It has two things 1) ParallelReader and 2) Decoder. The ParallelReader reads the input tfRecords and the decoder converts the input tfRecords into Tensor.\n",
    "\n",
    "* **Parallel Reader**: As the name suggests it will the input file with multiple reader and queue these records into tf.RandomShuffleQueue(**The RandomShuffleQueue already shuffles your dataset, so you dont have to bother doing it later again**). Then these records are dequeued and passed on to the decoder. \n",
    "\n",
    "* **Decoder**: The decoder does most of the heavy lifting. IT takes in the two dictionary that we described above\n",
    "  * keysToFeature: This dictionary provides the ItemHandler object information about the data such as what format is the picture image, the label type (Fixed lenght or variable lenght)\n",
    "  * itemsToHandler: It specifies the name of the file to which the tensor are converted. For example by default it looks for keys such as image/encoded, image/format.\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/TensorFlowOPS/flowers/flowers_train_*.tfrecord\n",
      "flowers_train_*.tfrecord\n",
      "tfRecordsFile :  ['/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/TensorFlowOPS/flowers/flowers_train_00000-of-00002.tfrecord', '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/TensorFlowOPS/flowers/flowers_train_00001-of-00002.tfrecord']\n",
      "Total records to to be trained are:  2569\n",
      "{'image/encoded': FixedLenFeature(shape=(), dtype=tf.string, default_value=''), 'image/format': FixedLenFeature(shape=(), dtype=tf.string, default_value='jpg'), 'image/class/label': FixedLenFeature(shape=[], dtype=tf.int64, default_value=<tf.Tensor 'zeros_8:0' shape=() dtype=int64>)}\n",
      "<tensorflow.contrib.slim.python.slim.data.tfexample_decoder.TFExampleDecoder object at 0x11ed77940>\n"
     ]
    }
   ],
   "source": [
    "def getSplits(splitName, datasetDir, filePattern):\n",
    "    if splitName not in ['train','validation']:\n",
    "        raise ValueError('The spllit name %s is not recognized, Make sure they are either train, validation or test'%splitName)\n",
    "\n",
    "    # create a path for data IO: The pattern should be the split name\n",
    "    filePatternPath = os.path.join(datasetDir, filePattern%(splitName))\n",
    "    print (filePatternPath)\n",
    "\n",
    "    # Count the total number of training samples:\n",
    "    filePattern = filePattern%(splitName)\n",
    "    print (filePattern)\n",
    "    tfRecordsFilesPath = [os.path.join(datasetDir, file) \n",
    "                          for file in os.listdir(datasetDir) \n",
    "                          if file.startswith('flowers_' + split_name)]\n",
    "    print ('tfRecordsFile : ',tfRecordsFilesPath)\n",
    "\n",
    "    # We now count all the records inside the tfRecord files that are separated into multiple shards\n",
    "    numSamples = 0\n",
    "    for tfRecord in tfRecordsFilesPath:\n",
    "        for record in tf.python_io.tf_record_iterator(tfRecord):\n",
    "            numSamples += 1\n",
    "\n",
    "    print ('Total records to to be trained are: ', numSamples)\n",
    "\n",
    "    # Define what type of reader\n",
    "    reader = tf.TFRecordReader\n",
    "    \n",
    "    # Create a keys to feature dictionary\n",
    "    keysToFeature = {\n",
    "        'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),\n",
    "        'image/format': tf.FixedLenFeature((), tf.string, default_value='jpg'),\n",
    "        'image/class/label': tf.FixedLenFeature(\n",
    "        [], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n",
    "    }\n",
    "    print (keysToFeature)\n",
    "\n",
    "    # Create the dictionary for the decoder\n",
    "    itemsToHandlers = {\n",
    "        'image': tfexample_decoder.Image(),   # takes by default : (image_key=\"image/encoded\", format_key=\"image_format\")  \n",
    "        'label': tfexample_decoder.Tensor('image/class/label')\n",
    "    }\n",
    "    # tfexample_decoder is the module and Image in the class so basically the key imaage int the dictionary contains the \n",
    "    # object of class image as the value.\n",
    "    # Similarly the key Label also holds the obect of class Tensor in the value\n",
    "\n",
    "    decoder = tfexample_decoder.TFExampleDecoder(keysToFeature, itemsToHandlers)\n",
    "    print (decoder)\n",
    "\n",
    "    labelsTONameDict = labelsToNameDict\n",
    "\n",
    "    dataetObj = dataset.Dataset(\n",
    "            data_sources = filePatternPath,\n",
    "            decoder = decoder,\n",
    "            reader = reader,\n",
    "            num_readers = 4,                  # Specify how many reader you would want to read and queue the tfRecords\n",
    "            num_samples = numSamples,\n",
    "            num_classes = 5,\n",
    "            labels_to_name = labelsTONameDict,\n",
    "            items_to_descriptions = itemToDescription)\n",
    "    \n",
    "    return dataetObj\n",
    "\n",
    "datasetDir = \"/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/TensorFlowOPS/flowers\"\n",
    "split_name = \"train\"\n",
    "filePattern = \"flowers_%s_*.tfrecord\"\n",
    "\n",
    "datasetOBJ = getSplits(splitName=split_name, datasetDir=datasetDir, filePattern=filePattern)\n",
    "# 'image': slim.tfexample_decoder.Image(),\n",
    "#     'label': slim.tfexample_decocer.tensor('image/class/label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadBatch(datasetOBJ, batchSize, height, width, isTraining=True):\n",
    "    \"\"\"\n",
    "    dataset: the return value from getSplit Function\n",
    "    batchSize: How many records in a batch.\n",
    "    height: the height of the image\n",
    "    width: The weigth of the image\n",
    "    isTraining: to determine weather to perform training of validation\n",
    "    \n",
    "    Output:\n",
    "    1: Tensor of shape [batchSize, height, width, numChannels]\n",
    "    2. Labels of shape [batchSize,]\n",
    "    \"\"\"\n",
    "    \n",
    "    # First create a dataprovider object, so that we can easily extract the image and its label from the object\n",
    "    dataProviderOBJ = dataset_data_provider.DatasetDataProvider(\n",
    "        dataset = datasetOBJ, \n",
    "        common_queue_capacity = 24+3*batchSize,   # The normal queue capacity\n",
    "        common_queue_min = 24)                    # atleast how many records to queue\n",
    "    \n",
    "    # The below operation gets the image in Tensor format, and we can preprocess them using Inception Preprocessing\n",
    "    rawImage, label = dataProviderOBJ.get(['image', 'label'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of raw image is :  [None, None, 3]\n",
      "The shape of the label data is :  []\n"
     ]
    }
   ],
   "source": [
    "# As discussed in the above description let us create a DatasetProvider\n",
    "batchSize = 64\n",
    "dataProvider = dataset_data_provider.DatasetDataProvider(\n",
    "    dataset = datasetOBJ,\n",
    "    common_queue_capacity = 24 + 3 * batchSize,\n",
    "    common_queue_min = 24)\n",
    "\n",
    "raw_image, label = dataProvider.get(('image', 'label'))\n",
    "\n",
    "print ('The shape of raw image is : ', raw_image.get_shape().as_list())\n",
    "print ('The shape of the label data is : ', label.get_shape().as_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Inception:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'height' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-c430ff80173d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Preprocesing :\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minception_preprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'height' is not defined"
     ]
    }
   ],
   "source": [
    "# Preprocesing :\n",
    "height = \n",
    "image = inception_preprocessing.preprocess_image(raw_image, height, width, is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
