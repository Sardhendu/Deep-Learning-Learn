{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to train and Cross validate without creating a seperate crossvalidation graph:\n",
    "\n",
    "The objective of this section is to just check if the following approach has the same weight for the 1 iteration of training and the crossvalidation stage. If that is the case then we know that the weights are not changed or updted because of the crossvalidation data.\n",
    "\n",
    "We also proceed the above state by optimizing with the function with cross validation data. If the weith changes than we know that infact the first stage was correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training Begin training Begin training Begin training Begin training \n",
      "\n",
      "############### Check the weights after the training batch (compare the results to previous example): \n",
      "\n",
      "The loss after one iteration is:  2.11358\n",
      "\n",
      "The variable list is:  ['Layer1/w1:0', 'Layer1/b1:0', 'Layer2/w2:0', 'Layer2/b2:0']\n",
      "\n",
      "The layer 1 weight is: \n",
      " [array([[-2.11875343,  1.11074066, -1.60225117, -1.04866743],\n",
      "       [ 1.07435846, -0.37837502, -0.39874262,  1.21538198],\n",
      "       [ 0.70911151, -0.10536171, -1.40221608, -0.44625875]], dtype=float32)]\n",
      "\n",
      "The layer 1 bias is: \n",
      " [array([  1.15548055e-07,   7.86918690e-07,   5.52410668e-08,\n",
      "         5.78365530e-07], dtype=float32)]\n",
      "\n",
      "The layer 2 weight is: \n",
      " [array([[-2.41577721, -0.75244588,  2.13699627,  0.52897501],\n",
      "       [-0.88196021, -1.1298064 , -0.13271846,  1.08850992],\n",
      "       [-1.20912242, -0.32934937,  0.7239632 , -0.67482013],\n",
      "       [ 0.30861691, -1.04130256,  0.30698499,  0.04391584]], dtype=float32)]\n",
      "\n",
      "The layer 2 bias is: \n",
      " [array([  6.90829935e-08,  -1.14726254e-06,  -1.90584810e-08,\n",
      "         1.09820508e-06], dtype=float32)]\n",
      "\n",
      "\n",
      "############### Check if the weights are the same after the validation batch (compare the results to weights after training example): \n",
      "\n",
      "The loss after validation iteration is:  2.11358\n",
      "\n",
      "The variable list is:  ['Layer1/w1:0', 'Layer1/b1:0', 'Layer2/w2:0', 'Layer2/b2:0']\n",
      "\n",
      "The layer 1 weight is: \n",
      " [array([[-2.11875343,  1.11074066, -1.60225117, -1.04866743],\n",
      "       [ 1.07435846, -0.37837502, -0.39874262,  1.21538198],\n",
      "       [ 0.70911151, -0.10536171, -1.40221608, -0.44625875]], dtype=float32)]\n",
      "\n",
      "The layer 1 bias is: \n",
      " [array([  1.15548055e-07,   7.86918690e-07,   5.52410668e-08,\n",
      "         5.78365530e-07], dtype=float32)]\n",
      "\n",
      "The layer 2 weight is: \n",
      " [array([[-2.41577721, -0.75244588,  2.13699627,  0.52897501],\n",
      "       [-0.88196021, -1.1298064 , -0.13271846,  1.08850992],\n",
      "       [-1.20912242, -0.32934937,  0.7239632 , -0.67482013],\n",
      "       [ 0.30861691, -1.04130256,  0.30698499,  0.04391584]], dtype=float32)]\n",
      "\n",
      "The layer 2 bias is: \n",
      " [array([  6.90829935e-08,  -1.14726254e-06,  -1.90584810e-08,\n",
      "         1.09820508e-06], dtype=float32)]\n",
      "############### Check if the weights change after using the optimizer with validation dataset : \n",
      "\n",
      "The loss after validation iteration is:  2.26603\n",
      "\n",
      "The variable list is:  ['Layer1/w1:0', 'Layer1/b1:0', 'Layer2/w2:0', 'Layer2/b2:0']\n",
      "\n",
      "The layer 1 weight is: \n",
      " [array([[-2.11875296,  1.11074114, -1.6022507 , -1.04866672],\n",
      "       [ 1.07435822, -0.37837639, -0.39874244,  1.21538115],\n",
      "       [ 0.70911211, -0.10536141, -1.40221667, -0.44625884]], dtype=float32)]\n",
      "\n",
      "The layer 1 bias is: \n",
      " [array([ -4.02032185e-07,  -2.14392799e-06,   4.44028501e-07,\n",
      "        -1.36022834e-06], dtype=float32)]\n",
      "\n",
      "The layer 2 weight is: \n",
      " [array([[-2.41577649, -0.75244486,  2.13699627,  0.52897495],\n",
      "       [-0.88196051, -1.12980747, -0.13271837,  1.08850992],\n",
      "       [-1.20912278, -0.32934901,  0.72396302, -0.67481983],\n",
      "       [ 0.30861658, -1.04130054,  0.30698469,  0.04391532]], dtype=float32)]\n",
      "\n",
      "The layer 2 bias is: \n",
      " [array([ -6.69981205e-07,   1.44030105e-06,  -4.06810130e-07,\n",
      "        -2.74756985e-07], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "dim = 3\n",
    "hidden_units1 = 4\n",
    "hidden_units2 = 4\n",
    "\n",
    "def linearActivation1(scope, x):\n",
    "    with tf.variable_scope(scope):\n",
    "        w = tf.get_variable(dtype=tf.float32, initializer=tf.random_normal_initializer(seed=432), shape=[dim, hidden_units1], name=\"w1\")\n",
    "        b = tf.get_variable(dtype = tf.float32, initializer=tf.zeros(dtype=tf.float32, shape=[hidden_units1]), name=\"b1\" )\n",
    "        \n",
    "        return tf.nn.tanh(tf.matmul(x, w) + b)\n",
    "    \n",
    "def linearActivation2(scope, x):\n",
    "    with tf.variable_scope(scope):\n",
    "        w = tf.get_variable(dtype=tf.float32, initializer=tf.random_normal_initializer(seed=989), shape=[hidden_units1, hidden_units2], name=\"w2\")\n",
    "        b = tf.get_variable(dtype = tf.float32, initializer=tf.zeros(dtype=tf.float32, shape=[hidden_units2]), name=\"b2\" )\n",
    "        \n",
    "        return tf.nn.tanh(tf.matmul(x, w) + b)\n",
    "    \n",
    "    \n",
    "ops.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "    x = tf.placeholder(dtype=tf.float32, shape=[None, dim], name=\"x\")\n",
    "    y = tf.placeholder(dtype=tf.int32, shape=[None], name=\"y\")\n",
    "    \n",
    "    logits1 = linearActivation1(\"Layer1\", x)\n",
    "#     print (logits1.get_shape().as_list())\n",
    "    logits2 = linearActivation2(\"Layer2\", logits1)\n",
    "    \n",
    "    \n",
    "    #################\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits2, labels=y, name=\"xentropy\")\n",
    "    lossCE = tf.reduce_mean(cross_entropy) \n",
    "    optimizer = tf.train.GradientDescentOptimizer(1e-5).minimize(lossCE)\n",
    "        \n",
    "    #################\n",
    "    \n",
    "    print ('Begin training Begin training Begin training Begin training Begin training ')\n",
    "    print ('')\n",
    "    \n",
    "    np.random.seed(398)\n",
    "    trainData = np.random.randn(batch_size, dim)\n",
    "    trainLabels = np.random.randint(0, hidden_units2, size=batch_size)\n",
    "\n",
    "    \n",
    "    # RUN SESSION:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # When Training\n",
    "    _, loss, cent = sess.run([optimizer, lossCE, cross_entropy], feed_dict={x:trainData, y:trainLabels})\n",
    "    \n",
    "\n",
    "    print ('############### Check the weights after the training batch (compare the results to previous example): \\n')\n",
    "    print ('The loss after one iteration is: ', loss)\n",
    "    print ('')\n",
    "    variableList = [v.name for v in tf.trainable_variables()]\n",
    "    print ('The variable list is: ', variableList)\n",
    "    print ('')\n",
    "    print ('The layer 1 weight is: \\n', sess.run([v for v in tf.trainable_variables() if v.name == 'Layer1/w1:0']))\n",
    "    print ('')\n",
    "    print ('The layer 1 bias is: \\n', sess.run([v for v in tf.trainable_variables() if v.name == 'Layer1/b1:0']))\n",
    "    print ('')\n",
    "    print ('The layer 2 weight is: \\n', sess.run([v for v in tf.trainable_variables() if v.name == 'Layer2/w2:0']))\n",
    "    print ('')\n",
    "    print ('The layer 2 bias is: \\n', sess.run([v for v in tf.trainable_variables() if v.name == 'Layer2/b2:0']))\n",
    "    print ('')\n",
    "    print ('')\n",
    "    \n",
    "    # When Cross Validation (Since we dont initiate the computation graph for optimization, the weights dont change)\n",
    "    np.random.seed(778)\n",
    "    validData = np.random.randn(batch_size, dim)\n",
    "    validLabel = np.random.randint(0, hidden_units2, size=batch_size)\n",
    "    \n",
    "    cent = sess.run([ cross_entropy], feed_dict={x:validData, y:validLabel})\n",
    "    \n",
    "\n",
    "    print ('############### Check if the weights are the same after the validation batch (compare the results to weights after training example): \\n')\n",
    "    print ('The loss after validation iteration is: ', loss)\n",
    "    print ('')\n",
    "    variableList = [v.name for v in tf.trainable_variables()]\n",
    "    print ('The variable list is: ', variableList)\n",
    "    print ('')\n",
    "    print ('The layer 1 weight is: \\n', sess.run([v for v in tf.trainable_variables() if v.name == 'Layer1/w1:0']))\n",
    "    print ('')\n",
    "    print ('The layer 1 bias is: \\n', sess.run([v for v in tf.trainable_variables() if v.name == 'Layer1/b1:0']))\n",
    "    print ('')\n",
    "    print ('The layer 2 weight is: \\n', sess.run([v for v in tf.trainable_variables() if v.name == 'Layer2/w2:0']))\n",
    "    print ('')\n",
    "    print ('The layer 2 bias is: \\n', sess.run([v for v in tf.trainable_variables() if v.name == 'Layer2/b2:0']))\n",
    "    \n",
    "    # When Cross Validation with optimization and loss (This should change the weights)\n",
    "\n",
    "    _, loss, cent = sess.run([optimizer, lossCE, cross_entropy], feed_dict={x:validData, y:validLabel})\n",
    "    \n",
    "    print ('############### Check if the weights change after using the optimizer with validation dataset : \\n')\n",
    "    print ('The loss after validation iteration is: ', loss)\n",
    "    print ('')\n",
    "    variableList = [v.name for v in tf.trainable_variables()]\n",
    "    print ('The variable list is: ', variableList)\n",
    "    print ('')\n",
    "    print ('The layer 1 weight is: \\n', sess.run([v for v in tf.trainable_variables() if v.name == 'Layer1/w1:0']))\n",
    "    print ('')\n",
    "    print ('The layer 1 bias is: \\n', sess.run([v for v in tf.trainable_variables() if v.name == 'Layer1/b1:0']))\n",
    "    print ('')\n",
    "    print ('The layer 2 weight is: \\n', sess.run([v for v in tf.trainable_variables() if v.name == 'Layer2/w2:0']))\n",
    "    print ('')\n",
    "    print ('The layer 2 bias is: \\n', sess.run([v for v in tf.trainable_variables() if v.name == 'Layer2/b2:0']))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
