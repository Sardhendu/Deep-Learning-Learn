{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Weights after one update: Using TensorFlow weight update\n",
    "----------\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(1e-5).minimize(lossCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial weights initilized are: \n",
      "\n",
      "[[-2.11875319  1.11074173 -1.60225117 -1.04866672]\n",
      " [ 1.0743587  -0.37837428 -0.39874253  1.21538258]\n",
      " [ 0.70911169 -0.10536155 -1.4022162  -0.44625875]]\n",
      "\n",
      "The initial bais initilized are: \n",
      "\n",
      "[0 0 0 0]\n",
      "\n",
      "Begin training Begin training Begin training Begin training Begin training \n",
      "\n",
      "[[-2.11875176  1.1107409  -1.60225093 -1.04867029]\n",
      " [ 1.07435918 -0.37837315 -0.39874318  1.21537995]\n",
      " [ 0.70911378 -0.10536345 -1.40221572 -0.44625914]]\n",
      "\n",
      "[  6.51314224e-07  -1.33663309e-06   4.03212113e-07   2.65411586e-06]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 4\n",
    "dim = 3\n",
    "hidden_units = 4\n",
    "\n",
    "ops.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "with sess.as_default():\n",
    "    x = tf.placeholder(dtype=tf.float32, shape=[None, dim], name=\"x\")\n",
    "    y = tf.placeholder(dtype=tf.int32, shape=[None], name=\"y\")\n",
    "    w = tf.Variable(initial_value=tf.random_normal(shape=[dim, hidden_units], seed=432), name=\"w\")\n",
    "    b = tf.Variable(initial_value=tf.zeros(shape=[hidden_units]), name=\"b\")\n",
    "    \n",
    "    print ('The initial weights initilized are: \\n')\n",
    "    print (tf.cast(tf.random_normal(shape=[dim, hidden_units], seed=432), dtype=tf.float32).eval())\n",
    "    print ('')\n",
    "    print ('The initial bais initilized are: \\n')\n",
    "    print (tf.cast(tf.zeros(shape=[hidden_units]), dtype=tf.int32).eval())\n",
    "    print ('')\n",
    "    \n",
    "    logits = tf.nn.tanh(tf.matmul(x, w) + b)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y, name=\"xentropy\")\n",
    "    lossCE = tf.reduce_mean(cross_entropy) \n",
    "    optimizer = tf.train.GradientDescentOptimizer(1e-5).minimize(lossCE)\n",
    "        \n",
    "    print ('Begin training Begin training Begin training Begin training Begin training ')\n",
    "    print ('')\n",
    "    \n",
    "    np.random.seed(398)\n",
    "    data = np.random.randn(batch_size, dim)\n",
    "    labels = np.random.randint(0, hidden_units, size=batch_size)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    newW, newB, _ = sess.run([w, b, optimizer], feed_dict={x:data, y:labels})\n",
    "\n",
    "    print (newW)\n",
    "    print ('')\n",
    "    print (newB)\n",
    "    \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Weights after one update: Manual Update:\n",
    "* gradients = tf.gradients(lossCE, tf.trainable_variables())\n",
    "* gradients_and_vars = list(zip(gradients, tf.trainable_variables()))\n",
    "\n",
    "* for g, v in gradients_and_vars:\n",
    "   * if g is not None:\n",
    "    *    newVar = v - 1e-5*(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial weights initilized are: \n",
      "\n",
      "[[-2.11875319  1.11074173 -1.60225117 -1.04866672]\n",
      " [ 1.0743587  -0.37837428 -0.39874253  1.21538258]\n",
      " [ 0.70911169 -0.10536155 -1.4022162  -0.44625875]]\n",
      "\n",
      "The initial bais initilized are: \n",
      "\n",
      "[0 0 0 0]\n",
      "\n",
      "Begin training Begin training Begin training Begin training Begin training \n",
      "\n",
      "['w:0', 'b:0']\n",
      "Grads_and_Vars when using optimizer: \n",
      " [(array([[-0.58818352,  0.34526709, -0.08834551,  1.42918241],\n",
      "       [-0.18376471, -0.45474505,  0.26138756,  1.02784896],\n",
      "       [-0.82264823,  0.7613858 , -0.178047  ,  0.16059123]], dtype=float32), array([[-2.11875319,  1.11074173, -1.60225117, -1.04866672],\n",
      "       [ 1.0743587 , -0.37837428, -0.39874253,  1.21538258],\n",
      "       [ 0.70911169, -0.10536155, -1.4022162 , -0.44625875]], dtype=float32)), (array([-0.2605257 ,  0.53465325, -0.16128485, -1.06164634], dtype=float32), array([ 0.,  0.,  0.,  0.], dtype=float32))]\n",
      "\n",
      "Gradients_and_Vars when NOT using optimizer: \n",
      " [(array([[-0.14704588,  0.08631677, -0.02208638,  0.3572956 ],\n",
      "       [-0.04594118, -0.11368626,  0.06534689,  0.25696224],\n",
      "       [-0.20566206,  0.19034645, -0.04451175,  0.04014781]], dtype=float32), array([[-2.11875319,  1.11074173, -1.60225117, -1.04866672],\n",
      "       [ 1.0743587 , -0.37837428, -0.39874253,  1.21538258],\n",
      "       [ 0.70911169, -0.10536155, -1.4022162 , -0.44625875]], dtype=float32)), (array([-0.06513143,  0.13366331, -0.04032121, -0.26541159], dtype=float32), array([ 0.,  0.,  0.,  0.], dtype=float32))]\n",
      "\n",
      "Gradients when NOT using optimizer: \n",
      " [array([[-0.14704588,  0.08631677, -0.02208638,  0.3572956 ],\n",
      "       [-0.04594118, -0.11368626,  0.06534689,  0.25696224],\n",
      "       [-0.20566206,  0.19034645, -0.04451175,  0.04014781]], dtype=float32), array([-0.06513143,  0.13366331, -0.04032121, -0.26541159], dtype=float32)]\n",
      "\n",
      "\n",
      "Running for Variable Num  w:0\n",
      "**************** this is variable *************\n",
      "variable's shape:\n",
      "[[-2.11875319  1.11074173 -1.60225117 -1.04866672]\n",
      " [ 1.0743587  -0.37837428 -0.39874253  1.21538258]\n",
      " [ 0.70911169 -0.10536155 -1.4022162  -0.44625875]]\n",
      "**************** this is gradient *************\n",
      "gradient's shape:\n",
      "[[-0.14704588  0.08631677 -0.02208638  0.3572956 ]\n",
      " [-0.04594118 -0.11368626  0.06534689  0.25696224]\n",
      " [-0.20566206  0.19034645 -0.04451175  0.04014781]]\n",
      "\n",
      "The new Variable looks like\n",
      "[[-2.11875176  1.1107409  -1.60225093 -1.04867029]\n",
      " [ 1.07435918 -0.37837315 -0.39874318  1.21537995]\n",
      " [ 0.70911378 -0.10536345 -1.40221572 -0.44625914]]\n",
      "\n",
      "\n",
      "Running for Variable Num  b:0\n",
      "**************** this is variable *************\n",
      "variable's shape:\n",
      "[ 0.  0.  0.  0.]\n",
      "**************** this is gradient *************\n",
      "gradient's shape:\n",
      "[-0.06513143  0.13366331 -0.04032121 -0.26541159]\n",
      "\n",
      "The new Variable looks like\n",
      "[  6.51314224e-07  -1.33663309e-06   4.03212113e-07   2.65411586e-06]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 4\n",
    "dim = 3\n",
    "hidden_units = 4\n",
    "\n",
    "ops.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "with sess.as_default():\n",
    "    x = tf.placeholder(dtype=tf.float32, shape=[None, dim], name=\"x\")\n",
    "    y = tf.placeholder(dtype=tf.int32, shape=[None], name=\"y\")\n",
    "    w = tf.Variable(initial_value=tf.random_normal(shape=[dim, hidden_units], seed=432), name=\"w\")\n",
    "    b = tf.Variable(initial_value=tf.zeros(shape=[hidden_units]), name=\"b\")\n",
    "    \n",
    "    print ('The initial weights initilized are: \\n')\n",
    "    print (tf.cast(tf.random_normal(shape=[dim, hidden_units], seed=432), dtype=tf.float32).eval())\n",
    "    print ('')\n",
    "    print ('The initial bais initilized are: \\n')\n",
    "    print (tf.cast(tf.zeros(shape=[hidden_units]), dtype=tf.int32).eval())\n",
    "    print ('')\n",
    "    \n",
    "    \n",
    "    logits = tf.nn.tanh(tf.matmul(x, w) + b)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y, name=\"xentropy\")\n",
    "    lossCE = tf.reduce_mean(cross_entropy)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1e-5)\n",
    "    \n",
    "    print ('Begin training Begin training Begin training Begin training Begin training ')\n",
    "    print ('')\n",
    "    \n",
    "    grads_and_vars = optimizer.compute_gradients(cross_entropy, tf.trainable_variables())\n",
    "    gradients = tf.gradients(lossCE, tf.trainable_variables())\n",
    "    gradients_and_vars = list(zip(gradients, tf.trainable_variables()))\n",
    "    print ([v.name for v in tf.trainable_variables()])\n",
    "    # generate data\n",
    "    \n",
    "    np.random.seed(398)\n",
    "    data = np.random.randn(batch_size, dim)\n",
    "    labels = np.random.randint(0, hidden_units, size=batch_size)\n",
    "    \n",
    "    # RUN SESSION:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    Grads_and_Vars, Gradients_and_Vars, Gradients = sess.run([grads_and_vars, gradients_and_vars, gradients], feed_dict={x:data, y:labels})\n",
    "    print (\"Grads_and_Vars when using optimizer: \\n\",Grads_and_Vars)\n",
    "    print ('')\n",
    "    print (\"Gradients_and_Vars when NOT using optimizer: \\n\", Gradients_and_Vars)\n",
    "    print ('')\n",
    "    print (\"Gradients when NOT using optimizer: \\n\", Gradients)\n",
    "    print ('')\n",
    "    print ('')\n",
    "\n",
    "    for varNum, (g, v) in enumerate(Gradients_and_Vars):\n",
    "#         print (varNum)\n",
    "        print ('Running for Variable Num ', variableNames[varNum])\n",
    "        if g is not None:\n",
    "            print (\"**************** this is variable *************\")\n",
    "            print (\"variable's shape:\")\n",
    "            print (v)\n",
    "            print (\"**************** this is gradient *************\")\n",
    "            print (\"gradient's shape:\",)\n",
    "            print (g)\n",
    "            \n",
    "            print ('')\n",
    "            print ('The new Variable looks like')\n",
    "            newVar = v - 1e-5*(g)\n",
    "            print (newVar)\n",
    "            print('')\n",
    "            print ('')\n",
    "\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting the above situation with 2 layers:\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Weights after one update: Using TensorFlow weight update\n",
    "----------\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(1e-5).minimize(lossCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial weights (w1) initilized are: \n",
      "\n",
      "[[-2.11875319  1.11074173 -1.60225117 -1.04866672]\n",
      " [ 1.0743587  -0.37837428 -0.39874253  1.21538258]\n",
      " [ 0.70911169 -0.10536155 -1.4022162  -0.44625875]]\n",
      "\n",
      "The initial bais (b1) initilized are: \n",
      "\n",
      "[0 0 0 0]\n",
      "\n",
      "The initial weights (w2) initilized are: \n",
      "\n",
      "[[-2.11875319  1.11074173 -1.60225117 -1.04866672]\n",
      " [ 1.0743587  -0.37837428 -0.39874253  1.21538258]\n",
      " [ 0.70911169 -0.10536155 -1.4022162  -0.44625875]]\n",
      "\n",
      "The initial bais (b1) initilized are: \n",
      "\n",
      "[0 0 0 0]\n",
      "\n",
      "Begin training Begin training Begin training Begin training Begin training \n",
      "\n",
      "[[-2.11875319  1.11074126 -1.60225117 -1.0486691 ]\n",
      " [ 1.07435882 -0.37837431 -0.39874256  1.21538115]\n",
      " [ 0.70911139 -0.10536185 -1.4022162  -0.44625914]]\n",
      "\n",
      "[ -3.27776860e-07   1.07476318e-07   2.79387002e-08   1.60490333e-06]\n",
      "\n",
      "[[-2.11875343  1.11074054 -1.60225117 -1.04866552]\n",
      " [ 1.07435882 -0.37837321 -0.39874253  1.2153815 ]\n",
      " [ 0.70911151 -0.10536274 -1.4022162  -0.44625753]\n",
      " [-0.02040439 -1.24266374 -0.77951401  1.5199616 ]]\n",
      "\n",
      "[  9.54884740e-08  -2.44732610e-06  -5.94951999e-09  -7.12481807e-08]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 4\n",
    "dim = 3\n",
    "hidden_units1 = 4\n",
    "hidden_units2 = 4\n",
    "\n",
    "ops.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "with sess.as_default():\n",
    "    x = tf.placeholder(dtype=tf.float32, shape=[None, dim], name=\"x\")\n",
    "    y = tf.placeholder(dtype=tf.int32, shape=[None], name=\"y\")\n",
    "    w1 = tf.Variable(initial_value=tf.random_normal(shape=[dim, hidden_units1], seed=432), name=\"w1\")\n",
    "    b1 = tf.Variable(initial_value=tf.zeros(shape=[hidden_units1]), name=\"b1\")\n",
    "    w2 = tf.Variable(initial_value=tf.random_normal(shape=[hidden_units1, hidden_units2], seed=989), name=\"w2\")\n",
    "    b2 = tf.Variable(initial_value=tf.zeros(shape=[hidden_units2]), name=\"b2\") \n",
    "    \n",
    "    print ('The initial weights (w1) initilized are: \\n')\n",
    "    print (tf.cast(tf.random_normal(shape=[dim, hidden_units], seed=432), dtype=tf.float32).eval())\n",
    "    print ('')\n",
    "    print ('The initial bais (b1) initilized are: \\n')\n",
    "    print (tf.cast(tf.zeros(shape=[hidden_units]), dtype=tf.int32).eval())\n",
    "    print ('')\n",
    "    print ('The initial weights (w2) initilized are: \\n')\n",
    "    print (tf.cast(tf.random_normal(shape=[dim, hidden_units], seed=989), dtype=tf.float32).eval())\n",
    "    print ('')\n",
    "    print ('The initial bais (b1) initilized are: \\n')\n",
    "    print (tf.cast(tf.zeros(shape=[hidden_units]), dtype=tf.int32).eval())\n",
    "    print ('')\n",
    "    \n",
    "    \n",
    "    logits1 = tf.nn.tanh(tf.matmul(x, w1) + b1)\n",
    "    logits2 = tf.nn.tanh(tf.matmul(logits1, w2) + b2)\n",
    "    \n",
    "    ################\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits2, labels=y, name=\"xentropy\")\n",
    "    lossCE = tf.reduce_mean(cross_entropy) \n",
    "    optimizer = tf.train.GradientDescentOptimizer(1e-5).minimize(lossCE)\n",
    "    ################\n",
    "    \n",
    "    print ('Begin training Begin training Begin training Begin training Begin training ')\n",
    "    print ('')\n",
    "    \n",
    "    np.random.seed(398)\n",
    "    data = np.random.randn(batch_size, dim)\n",
    "    labels = np.random.randint(0, hidden_units2, size=batch_size)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    newW1, newB1, newW2, newB2, _ = sess.run([w1, b1, w2, b2, optimizer], feed_dict={x:data, y:labels})\n",
    "\n",
    "    print (newW1)\n",
    "    print ('')\n",
    "    print (newB1)\n",
    "    print ('')\n",
    "    print (newW2)\n",
    "    print ('')\n",
    "    print (newB2)\n",
    "    \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial weights (w1) initilized are: \n",
      "\n",
      "[[-2.11875319  1.11074173 -1.60225117 -1.04866672]\n",
      " [ 1.0743587  -0.37837428 -0.39874253  1.21538258]\n",
      " [ 0.70911169 -0.10536155 -1.4022162  -0.44625875]]\n",
      "\n",
      "The initial bais (b1) initilized are: \n",
      "\n",
      "[0 0 0 0]\n",
      "\n",
      "The initial weights (w2) initilized are: \n",
      "\n",
      "[[-2.41577721 -0.75244576  2.13699627  0.52897286]\n",
      " [-0.88196027 -1.1298064  -0.1327185   1.08851182]\n",
      " [-1.2091223  -0.32934931  0.72396326 -0.67482239]]\n",
      "\n",
      "The initial bais (b1) initilized are: \n",
      "\n",
      "[0 0 0 0]\n",
      "\n",
      "Begin training Begin training Begin training Begin training Begin training \n",
      "\n",
      "Grads_and_Vars when using optimizer: \n",
      " [(array([[ 0.12204197,  0.44748574, -0.02048684,  0.27312103],\n",
      "       [ 0.08219481,  0.30057651,  0.03871645,  0.2215021 ],\n",
      "       [ 0.06438579,  0.06360757, -0.02989684,  0.00324626]], dtype=float32), array([[-2.11875319,  1.11074173, -1.60225117, -1.04866672],\n",
      "       [ 1.0743587 , -0.37837428, -0.39874253,  1.21538258],\n",
      "       [ 0.70911169, -0.10536155, -1.4022162 , -0.44625875]], dtype=float32)), (array([-0.04621922, -0.31476748, -0.02209643, -0.23134622], dtype=float32), array([ 0.,  0.,  0.,  0.], dtype=float32)), (array([[ 0.02600339,  0.03594229,  0.02065514, -0.85354167],\n",
      "       [-0.03080492, -0.01701269, -0.01701956,  0.75395048],\n",
      "       [ 0.03670534,  0.02967389,  0.02104382, -0.90754771],\n",
      "       [ 0.03265493, -0.15582368,  0.01121951, -0.51058811]], dtype=float32), array([[-2.41577721, -0.75244576,  2.13699627,  0.52897286],\n",
      "       [-0.88196027, -1.1298064 , -0.1327185 ,  1.08851182],\n",
      "       [-1.2091223 , -0.32934931,  0.72396326, -0.67482239],\n",
      "       [ 0.308617  , -1.04130292,  0.30698502,  0.04391456]], dtype=float32)), (array([-0.0276332 ,  0.45890504,  0.00762339, -0.43928206], dtype=float32), array([ 0.,  0.,  0.,  0.], dtype=float32))]\n",
      "\n",
      "Gradients_and_Vars when NOT using optimizer: \n",
      " [(array([[ 0.03051049,  0.11187144, -0.00512171,  0.06828026],\n",
      "       [ 0.0205487 ,  0.07514413,  0.00967911,  0.05537552],\n",
      "       [ 0.01609645,  0.01590189, -0.00747421,  0.00081157]], dtype=float32), array([[-2.11875319,  1.11074173, -1.60225117, -1.04866672],\n",
      "       [ 1.0743587 , -0.37837428, -0.39874253,  1.21538258],\n",
      "       [ 0.70911169, -0.10536155, -1.4022162 , -0.44625875]], dtype=float32)), (array([-0.01155481, -0.07869187, -0.00552411, -0.05783655], dtype=float32), array([ 0.,  0.,  0.,  0.], dtype=float32)), (array([[ 0.00650085,  0.00898557,  0.00516379, -0.21338542],\n",
      "       [-0.00770123, -0.00425317, -0.00425489,  0.18848762],\n",
      "       [ 0.00917633,  0.00741847,  0.00526096, -0.22688693],\n",
      "       [ 0.00816373, -0.03895592,  0.00280488, -0.12764703]], dtype=float32), array([[-2.41577721, -0.75244576,  2.13699627,  0.52897286],\n",
      "       [-0.88196027, -1.1298064 , -0.1327185 ,  1.08851182],\n",
      "       [-1.2091223 , -0.32934931,  0.72396326, -0.67482239],\n",
      "       [ 0.308617  , -1.04130292,  0.30698502,  0.04391456]], dtype=float32)), (array([-0.0069083 ,  0.11472626,  0.00190585, -0.10982051], dtype=float32), array([ 0.,  0.,  0.,  0.], dtype=float32))]\n",
      "\n",
      "Gradients when NOT using optimizer: \n",
      " [array([[ 0.03051049,  0.11187144, -0.00512171,  0.06828026],\n",
      "       [ 0.0205487 ,  0.07514413,  0.00967911,  0.05537552],\n",
      "       [ 0.01609645,  0.01590189, -0.00747421,  0.00081157]], dtype=float32), array([-0.01155481, -0.07869187, -0.00552411, -0.05783655], dtype=float32), array([[ 0.00650085,  0.00898557,  0.00516379, -0.21338542],\n",
      "       [-0.00770123, -0.00425317, -0.00425489,  0.18848762],\n",
      "       [ 0.00917633,  0.00741847,  0.00526096, -0.22688693],\n",
      "       [ 0.00816373, -0.03895592,  0.00280488, -0.12764703]], dtype=float32), array([-0.0069083 ,  0.11472626,  0.00190585, -0.10982051], dtype=float32)]\n",
      "\n",
      "\n",
      "Running for Variable Num  w1:0\n",
      "\n",
      "The new Variable looks like\n",
      "[[-2.11875343  1.11074066 -1.60225117 -1.04866743]\n",
      " [ 1.07435846 -0.37837502 -0.39874262  1.21538198]\n",
      " [ 0.70911151 -0.10536171 -1.40221608 -0.44625875]]\n",
      "\n",
      "\n",
      "Running for Variable Num  b1:0\n",
      "\n",
      "The new Variable looks like\n",
      "[  1.15548055e-07   7.86918690e-07   5.52410668e-08   5.78365530e-07]\n",
      "\n",
      "\n",
      "Running for Variable Num  w2:0\n",
      "\n",
      "The new Variable looks like\n",
      "[[-2.41577721 -0.75244588  2.13699627  0.52897501]\n",
      " [-0.88196021 -1.1298064  -0.13271846  1.08850992]\n",
      " [-1.20912242 -0.32934937  0.7239632  -0.67482013]\n",
      " [ 0.30861691 -1.04130256  0.30698499  0.04391584]]\n",
      "\n",
      "\n",
      "Running for Variable Num  b2:0\n",
      "\n",
      "The new Variable looks like\n",
      "[  6.90829935e-08  -1.14726254e-06  -1.90584810e-08   1.09820508e-06]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 4\n",
    "dim = 3\n",
    "hidden_units1 = 4\n",
    "hidden_units2 = 4\n",
    "\n",
    "ops.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "with sess.as_default():\n",
    "    x = tf.placeholder(dtype=tf.float32, shape=[None, dim], name=\"x\")\n",
    "    y = tf.placeholder(dtype=tf.int32, shape=[None], name=\"y\")\n",
    "    w1 = tf.Variable(initial_value=tf.random_normal(shape=[dim, hidden_units1], seed=432), name=\"w1\")\n",
    "    b1 = tf.Variable(initial_value=tf.zeros(shape=[hidden_units1]), name=\"b1\")\n",
    "    w2 = tf.Variable(initial_value=tf.random_normal(shape=[hidden_units1, hidden_units2], seed=989), name=\"w2\")\n",
    "    b2 = tf.Variable(initial_value=tf.zeros(shape=[hidden_units2]), name=\"b2\") \n",
    "    \n",
    "    print ('The initial weights (w1) initilized are: \\n')\n",
    "    print (tf.cast(tf.random_normal(shape=[dim, hidden_units], seed=432), dtype=tf.float32).eval())\n",
    "    print ('')\n",
    "    print ('The initial bais (b1) initilized are: \\n')\n",
    "    print (tf.cast(tf.zeros(shape=[hidden_units]), dtype=tf.int32).eval())\n",
    "    print ('')\n",
    "    print ('The initial weights (w2) initilized are: \\n')\n",
    "    print (tf.cast(tf.random_normal(shape=[dim, hidden_units], seed=989), dtype=tf.float32).eval())\n",
    "    print ('')\n",
    "    print ('The initial bais (b1) initilized are: \\n')\n",
    "    print (tf.cast(tf.zeros(shape=[hidden_units]), dtype=tf.int32).eval())\n",
    "    print ('')\n",
    "    \n",
    "    \n",
    "    logits1 = tf.nn.tanh(tf.matmul(x, w1) + b1)\n",
    "    logits2 = tf.nn.tanh(tf.matmul(logits1, w2) + b2)\n",
    "    \n",
    "    \n",
    "    #################\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits2, labels=y, name=\"xentropy\")\n",
    "    lossCE = tf.reduce_mean(cross_entropy)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1e-5)\n",
    "    #################\n",
    "    \n",
    "    print ('Begin training Begin training Begin training Begin training Begin training ')\n",
    "    print ('')\n",
    "    \n",
    "    # Fetching variables to manually caluculate the new weights\n",
    "    grads_and_vars = optimizer.compute_gradients(cross_entropy, tf.trainable_variables())\n",
    "    gradients = tf.gradients(lossCE, tf.trainable_variables())\n",
    "    gradients_and_vars = list(zip(gradients, tf.trainable_variables()))\n",
    "    variableNames = [v.name for v in tf.trainable_variables()]\n",
    "    # generate data\n",
    "    \n",
    "    np.random.seed(398)\n",
    "    data = np.random.randn(batch_size, dim)\n",
    "    labels = np.random.randint(0, hidden_units2, size=batch_size)\n",
    "    \n",
    "    # RUN SESSION:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    Grads_and_Vars, Gradients_and_Vars, Gradients = sess.run([grads_and_vars, gradients_and_vars, gradients], feed_dict={x:data, y:labels})\n",
    "    print (\"Grads_and_Vars when using optimizer: \\n\",Grads_and_Vars)\n",
    "    print ('')\n",
    "    print (\"Gradients_and_Vars when NOT using optimizer: \\n\", Gradients_and_Vars)\n",
    "    print ('')\n",
    "    print (\"Gradients when NOT using optimizer: \\n\", Gradients)\n",
    "    print ('')\n",
    "    print ('')\n",
    "\n",
    "    for varNum, (g, v) in enumerate(Gradients_and_Vars):\n",
    "#         print (varNum)\n",
    "        print ('Running for Variable Num ', variableNames[varNum])\n",
    "        if g is not None:\n",
    "#             print (\"**************** this is variable *************\")\n",
    "#             print (\"variable's shape:\")\n",
    "#             print (v)\n",
    "#             print (\"**************** this is gradient *************\")\n",
    "#             print (\"gradient's shape:\",)\n",
    "#             print (g)\n",
    "            \n",
    "            print ('')\n",
    "            print ('The new Variable looks like')\n",
    "            newVar = v - 1e-5*(g)\n",
    "            print (newVar)\n",
    "            print('')\n",
    "            print ('')\n",
    "\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
