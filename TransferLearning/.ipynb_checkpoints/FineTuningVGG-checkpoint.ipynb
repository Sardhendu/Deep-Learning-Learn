{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Based on the Git hub page from omoindrot {https://gist.github.com/omoindrot/dedc857cdc0e680dfb1be99762990c9c}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import os, sys\n",
    "\n",
    "checkpoints_dir = '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/TransferLearning/VGG-CheckPoint'\n",
    "\n",
    "directory_to_slim = \"/Users/sam/App-Setup/anaconda/lib/python3.6/site-packages/tensorflow/models/slim\"\n",
    "# set the path so that import is easy\n",
    "sys.path.append(directory_to_slim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import urllib\n",
    "import tensorflow as tf\n",
    "import scipy as spy\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from datasets import imagenet\n",
    "from nets import vgg\n",
    "from datasets import dataset_utils\n",
    "from preprocessing import vgg_preprocessing\n",
    "\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "\n",
    "trainDataFolder = \"/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/coco-animals/train\"\n",
    "testDataFolder = \"/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/coco-animals/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_images(directory):\n",
    "    \"\"\"\n",
    "    Get all the images and labels in directory/label/*.jpg\n",
    "    \"\"\"\n",
    "    labels = [dirs for dirs in os.listdir(directory) if dirs != \".DS_Store\"]\n",
    "    files_and_labels = []\n",
    "    print (labels)\n",
    "    for label in labels:\n",
    "        for f in os.listdir(os.path.join(directory, label)):\n",
    "            files_and_labels.append((os.path.join(directory, label, f), label))\n",
    "\n",
    "    filenames, labels = zip(*files_and_labels)\n",
    "    filenames = list(filenames)\n",
    "    labels = list(labels)\n",
    "    unique_labels = list(set(labels))\n",
    "\n",
    "    label_to_int = {}\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        label_to_int[label] = i\n",
    "\n",
    "    labels = [label_to_int[l] for l in labels]\n",
    "\n",
    "    return filenames, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Fuction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(dataIN, title):\n",
    "    plt.figure()\n",
    "    plt.imshow(dataIN.astype(np.uint8))  # Convering into insigned integer\n",
    "    plt.suptitle(title, fontsize=14, color = 'r')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resize image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# VGG takes in a centered.random cropped input of 224. If we take a center crop from 720x1080 image we would miss a \n",
    "# major part of the actual image. So here we resize it to 256xScaled_height and then apply crop on the resized\n",
    "# image\n",
    "\n",
    "# Reading/Parsing image files using TensorFlow\n",
    "# Resize the image such that the smaller side is 256 pixels long\n",
    "def parseImage(inputImagePath, imageLabel):\n",
    "    imageString = tf.read_file(inputImagePath)\n",
    "    imageDecoded = tf.image.decode_jpeg(imageString, channels=3)\n",
    "    image = tf.cast(imageDecoded, tf.float32)\n",
    "    image2 = image\n",
    "    \n",
    "    print (image.get_shape().as_list())\n",
    "    smallest_side = 256.0\n",
    "    height, width = tf.shape(image)[0], tf.shape(image)[1]\n",
    "    height = tf.to_float(height)\n",
    "    width = tf.to_float(width)\n",
    "\n",
    "    # Basically the smallest side becomes 256 and larger side = (256*100)/orig_larger_side_size\n",
    "    scale = tf.cond(tf.greater(height, width),\n",
    "                    lambda: smallest_side / width,\n",
    "                    lambda: smallest_side / height)\n",
    "    new_height = tf.to_int32(height * scale)\n",
    "    new_width = tf.to_int32(width * scale)\n",
    "\n",
    "#     print(image.get_shape().as_list())\n",
    "    imageResized = tf.image.resize_images(image, [new_height, new_width])  # (2)\n",
    "    return imageResized\n",
    "#     return imageDecoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we coulf either use VGG preoprocessing or write a preprocessing\n",
    "from preprocessing.vgg_preprocessing import _mean_image_subtraction,_R_MEAN, _G_MEAN, _B_MEAN\n",
    "\n",
    "# RGB mean\n",
    "VGG_MEAN = [_R_MEAN, _G_MEAN, _B_MEAN]\n",
    "\n",
    "# print (VGG_MEAN)\n",
    "def preprocessing(imageIN, imageSize, is_training, is_type=\"SELF\"):\n",
    "    if is_type == \"VGG\":\n",
    "        preprocessedImage = vgg_preprocessing.preprocess_image(imageIN, imageSize, \n",
    "                                                               imageSize, is_training=is_training)\n",
    "    elif is_type == \"SELF\" and is_training:\n",
    "        # Crop image\n",
    "        imageCrop = tf.random_crop(imageIN, [imageSize,imageSize,3])\n",
    "        # Augment Image by horizontal Flip\n",
    "        imageFlip = tf.image.random_flip_left_right(imageCrop)\n",
    "        means = tf.reshape(tf.constant(VGG_MEAN), [1, 1, 3])\n",
    "        preprocessedImage = imageFlip - means \n",
    "        \n",
    "    elif is_type == \"SELF\" and not is_training:\n",
    "        # Crop image\n",
    "        crop_image = tf.image.resize_image_with_crop_or_pad(imageIN, imageSize, imageSize)\n",
    "        means = tf.reshape(tf.constant(VGG_MEAN), [1, 1, 3])\n",
    "        preprocessedImage = imageCrop - means\n",
    "        \n",
    "    else:\n",
    "        raise ValueError('Provide a type Preprocessing')\n",
    "        \n",
    "    return imageCrop, imageFlip, means, preprocessedImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bear', 'bird', 'cat', 'dog', 'giraffe', 'horse', 'sheep', 'zebra']\n",
      "[None, None, 3]\n",
      "(224, 224, 3)\n",
      "(224, 224, 3)\n",
      "[[[ 123.68000031  116.77999878  103.94000244]]]\n",
      "(224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "filenames, labels = list_images(trainDataFolder)\n",
    "\n",
    "ops.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "with sess.as_default():\n",
    "    scale, new_height, new_width, imageResized, image = parseImage(filenames[0], imageLabel=labels[0])\n",
    "    imageCrop, imageFlip, means, preprocessedImage = preprocessing(imageIN=imageResized, imageSize=224, \n",
    "                                                                     is_training=True, is_type=\"SELF\")\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "#         sc, nh, nw, rsimg, img = sess.run([scale, new_height, new_width, resized_image, image])\n",
    "        imgcrp, imgflp, imgmeans, imgprs= sess.run([imageCrop, imageFlip, means, preprocessedImage])\n",
    "        print (imgcrp.shape)\n",
    "        print (imgflp.shape)\n",
    "        print (imgmeans)\n",
    "        print (imgprs.shape)\n",
    "#         print(rsimg.shape)\n",
    "#         print(img.shape)\n",
    "#         print (sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "378.3833718244804"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "256/433 * 640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
