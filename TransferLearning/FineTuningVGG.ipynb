{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Based on the Git hub page from omoindrot {https://gist.github.com/omoindrot/dedc857cdc0e680dfb1be99762990c9c}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import os, sys\n",
    "import argparse\n",
    "\n",
    "checkpoints_dir = '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/TransferLearning/VGG-CheckPoint'\n",
    "\n",
    "directory_to_slim = \"/Users/sam/App-Setup/anaconda/lib/python3.6/site-packages/tensorflow/models/slim\"\n",
    "# set the path so that import is easy\n",
    "sys.path.append(directory_to_slim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import urllib\n",
    "import tensorflow as tf\n",
    "import scipy as spy\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from datasets import imagenet\n",
    "from nets import vgg\n",
    "from datasets import dataset_utils\n",
    "from preprocessing import vgg_preprocessing\n",
    "\n",
    "from tensorflow.python.framework import ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directories Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=[], dest='batchSize', nargs=None, const=None, default=32, type=<class 'int'>, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "parentDataDir = \"/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/coco-animals\"\n",
    "parentModelDir = \"/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Models\"\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('trainDir', default=os.path.join(parentDataDir, 'coco-animals/train'))\n",
    "parser.add_argument('validDir', default=os.path.join(parentDataDir, 'coco-animals/val'))\n",
    "parser.add_argument('modelPath', default=os.path.join(parentModelDir, 'VGG/vgg_16.ckpt'), type=str)\n",
    "parser.add_argument('batchSize', default=32, type=int)\n",
    "# parser.add_argument('--num_workers', default=4, type=int)\n",
    "# parser.add_argument('--num_epochs1', default=10, type=int)\n",
    "# parser.add_argument('--num_epochs2', default=10, type=int)\n",
    "# parser.add_argument('learningRate1', default=1e-3, type=float)\n",
    "# parser.add_argument('learningRate2', default=1e-5, type=float)\n",
    "# parser.add_argument('dropoutKeepProb', default=0.5, type=float)\n",
    "# parser.add_argument('weightDecay', default=5e-4, type=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLOTS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(imageArray, rows, columns, figsize=(40, 6)):\n",
    "    fig1, axs = plt.subplots(rows,columns, figsize=figsize, facecolor='y', edgecolor='k')\n",
    "    if columns>1:\n",
    "        axs = axs.ravel()\n",
    "    for no, image in enumerate(imageArray):\n",
    "        axs[no].imshow(image)\n",
    "#         axs[no].suptitle(title, fontsize=14, color = 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch Training and Validation Images from the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_images(directory):\n",
    "    \"\"\"\n",
    "        Get all the images and labels in directory/label/*.jpg\n",
    "    \"\"\"\n",
    "    labels = [dirs for dirs in os.listdir(directory) if dirs != \".DS_Store\"]\n",
    "    files_and_labels = []\n",
    "    print (labels)\n",
    "    for label in labels:\n",
    "        for f in os.listdir(os.path.join(directory, label)):\n",
    "            files_and_labels.append((os.path.join(directory, label, f), label))\n",
    "\n",
    "    filenames, labels = zip(*files_and_labels)\n",
    "    filenames = list(filenames)\n",
    "    labels = list(labels)\n",
    "    unique_labels = list(set(labels))\n",
    "\n",
    "    label_to_int = {}\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        label_to_int[label] = i\n",
    "\n",
    "    labels = [label_to_int[l] for l in labels]\n",
    "\n",
    "    return filenames, labels\n",
    "\n",
    "### Plot Fuction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resize image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# VGG takes in a centered.random cropped input of 224. \n",
    "# If we take a center crop from 720x1080 image we would miss a major part of the actual image. \n",
    "# So here we resize it to 256xScaled_height and then apply crop on the resized image\n",
    "\n",
    "# Reading/Parsing image files using TensorFlow\n",
    "# Resize the image such that the smaller side is 256 pixels long\n",
    "def parseImage(inputImagePath, imageLabel, viz=False):\n",
    "    imageString = tf.read_file(inputImagePath)\n",
    "    imageDecoded = tf.image.decode_jpeg(imageString, channels=3)\n",
    "    imageFloat = tf.cast(imageDecoded, tf.float32)\n",
    "#     image2 = image\n",
    "    \n",
    "#     print (image.get_shape().as_list())\n",
    "    smallest_side = 256.0\n",
    "    height, width = tf.shape(imageFloat)[0], tf.shape(imageFloat)[1]\n",
    "    height = tf.to_float(height)\n",
    "    width = tf.to_float(width)\n",
    "\n",
    "    # Basically the smallest side becomes 256 and larger side = (256*100)/orig_larger_side_size\n",
    "    scale = tf.cond(tf.greater(height, width),\n",
    "                    lambda: smallest_side / width,\n",
    "                    lambda: smallest_side / height)\n",
    "    new_height = tf.to_int32(height * scale)\n",
    "    new_width = tf.to_int32(width * scale)\n",
    "\n",
    "#     print(image.get_shape().as_list())\n",
    "    imageResizedFloat = tf.image.resize_images(imageFloat, [new_height, new_width])  # (2)\n",
    "    \n",
    "    if viz:\n",
    "        imageUnsignedInt = tf.cast(imageDecoded, tf.uint8)\n",
    "        imageResizedUint8 = tf.cast(tf.image.resize_images(imageFloat, [new_height, new_width]), tf.uint8)\n",
    "        return imageUnsignedInt, imageFloat, imageResizedUint8, imageResizedFloat \n",
    "    else:\n",
    "        return imageFloat, imageResizedFloat \n",
    "        \n",
    "#     return imageDecoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we coulf either use VGG preoprocessing or write a preprocessing\n",
    "from preprocessing.vgg_preprocessing import _mean_image_subtraction,_R_MEAN, _G_MEAN, _B_MEAN\n",
    "\n",
    "# RGB mean\n",
    "VGG_MEAN = [_R_MEAN, _G_MEAN, _B_MEAN]\n",
    "\n",
    "# print (VGG_MEAN)\n",
    "def preprocessing(imageIN, imageSize, is_training, is_type=\"SELF\"):\n",
    "    if is_type == \"VGG\":\n",
    "        preprocessedImage = vgg_preprocessing.preprocess_image(imageIN, imageSize, \n",
    "                                                               imageSize, is_training=is_training)\n",
    "    elif is_type == \"SELF\" and is_training:\n",
    "        # Crop image\n",
    "        imageCrop = tf.random_crop(imageIN, [imageSize,imageSize,3])\n",
    "        # Augment Image by horizontal Flip\n",
    "        imageFlip = tf.image.random_flip_left_right(imageCrop)\n",
    "        means = tf.reshape(tf.constant(VGG_MEAN), [1, 1, 3])\n",
    "        preprocessedImage = imageFlip - means \n",
    "        \n",
    "    elif is_type == \"SELF\" and not is_training:\n",
    "        # Crop image\n",
    "        crop_image = tf.image.resize_image_with_crop_or_pad(imageIN, imageSize, imageSize)\n",
    "        means = tf.reshape(tf.constant(VGG_MEAN), [1, 1, 3])\n",
    "        preprocessedImage = imageCrop - means\n",
    "        \n",
    "    else:\n",
    "        raise ValueError('Provide a type Preprocessing')\n",
    "        \n",
    "    return imageCrop, imageFlip, preprocessedImage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do some Vizualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] trainDir validDir modelPath batchSize\n",
      "ipykernel_launcher.py: error: the following arguments are required: validDir, modelPath, batchSize\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sam/App-Setup/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "args = parser.parse_args()\n",
    "# print (args)\n",
    "# filenames, labels = list_images(args.train_dir)\n",
    "\n",
    "# ops.reset_default_graph()\n",
    "# sess = tf.Session()\n",
    "\n",
    "# i=105\n",
    "# with sess.as_default():\n",
    "#     imageAct = mpimg.imread(filenames[i])\n",
    "#     imageUnsignedInt, imageFloat, imageResizedUint8, imageResizedFloat  = parseImage(\n",
    "#         filenames[i], imageLabel=labels[i], viz=True)\n",
    "#     imageCrop, imageFlip, preprocessedImage = preprocessing(imageIN=imageResizedFloat, imageSize=224, \n",
    "#                                                                      is_training=True, is_type=\"SELF\")\n",
    "    \n",
    "#     with tf.Session() as sess:\n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "                                                            \n",
    "# #         imgunint8, imgfloat, imgrszd = sess.run(\n",
    "# #             [imageUnsignedInt, imageFloat, imageResizedUint8, imageResizedFloat])\n",
    "                                                            \n",
    "#         imgunint8, imgfloat, imgrszdunint8, imgrszdfloat, imgcrp, imgflp, imgprs = sess.run(\n",
    "#             [imageUnsignedInt, imageFloat, imageResizedUint8, imageResizedFloat, \n",
    "#              imageCrop, imageFlip, preprocessedImage]\n",
    "#         )\n",
    "        \n",
    "#         imageArray = [imgunint8, imgfloat, imgrszdunint8, imgrszdfloat, imgcrp, imgflp, imgprs]\n",
    "#         plot(imageArray, rows=2, columns=4, figsize=(40, 20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Employ VGG Net: {Training}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames, labels = list_images(trainDataFolder)\n",
    "\n",
    "ops.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "train_filenames, train_labels = list_images(args.train_dir)\n",
    "val_filenames, val_labels = list_images(args.val_dir)\n",
    "for fname, lbl in zip(filenames, labels):\n",
    "    with sess.as_default():\n",
    "        imageAct = mpimg.imread(filenames[i])\n",
    "        imageUnsignedInt, imageFloat, imageResizedUint8, imageResizedFloat  = parseImage(\n",
    "            filenames[i], imageLabel=labels[i], viz=True)\n",
    "        imageCrop, imageFlip, preprocessedImage = preprocessing(imageIN=imageResizedFloat, imageSize=224, \n",
    "                                                                         is_training=True, is_type=\"SELF\")\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    #         imgunint8, imgfloat, imgrszd = sess.run(\n",
    "    #             [imageUnsignedInt, imageFloat, imageResizedUint8, imageResizedFloat])\n",
    "\n",
    "            imgunint8, imgfloat, imgrszdunint8, imgrszdfloat, imgcrp, imgflp, imgprs = sess.run(\n",
    "                [imageUnsignedInt, imageFloat, imageResizedUint8, imageResizedFloat, \n",
    "                 imageCrop, imageFlip, preprocessedImage]\n",
    "            )\n",
    "\n",
    "            imageArray = [imgunint8, imgfloat, imgrszdunint8, imgrszdfloat, imgcrp, imgflp, imgprs]\n",
    "            plot(imageArray, rows=2, columns=4, figsize=(40, 20))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
