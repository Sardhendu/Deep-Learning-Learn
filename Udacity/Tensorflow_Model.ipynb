{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports are doe Here:\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (200000, 28, 28) (200000,)\n",
      "Cross Validation: (9810, 28, 28) (9810,)\n",
      "Testing: (7709, 28, 28) (7709,)\n"
     ]
    }
   ],
   "source": [
    "# Now as always we get the data we stored in the disk.\n",
    "cleaned_dataset_path = '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Udacity/DataPreparation/dataset_cleaned.p'\n",
    "\n",
    "with open(cleaned_dataset_path, 'rb') as f:\n",
    "    fnl_dataset = pickle.load(f)\n",
    "    training_dataset = (fnl_dataset['training_dataset'])\n",
    "    training_labels = (fnl_dataset['training_labels'])\n",
    "    test_dataset = (fnl_dataset['test_dataset'])\n",
    "    test_labels = (fnl_dataset['test_labels'])\n",
    "    crossvalid_dataset = (fnl_dataset['crossvalid_dataset'])\n",
    "    crossvalid_labels = (fnl_dataset['crossvalid_labels'])\n",
    "    \n",
    "print('Training:', training_dataset.shape, training_labels.shape)\n",
    "print('Cross Validation:', crossvalid_dataset.shape, crossvalid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 28, 28)\n",
      "Training set (200000, 784) (200000, 10)\n",
      "Cross Validation set (9810, 784) (9810, 10)\n",
      "Test set (7709, 784) (7709, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "no_of_labels = 10\n",
    "\n",
    "\n",
    "# tensor flow takes the labels input as binary code, where Alphabet A whose binary value is 0 will turn to a array\n",
    "# with elements [1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0] and B becomes [0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "def reshape_data(dataset, labels, sample_size=None):\n",
    "    if sample_size:\n",
    "        dataset = dataset[:sample_size].reshape(sample_size, image_size * image_size) # To reshape the  \n",
    "        # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "        labels = (np.arange(no_of_labels) == labels[:,None]).astype(np.float32)\n",
    "    else:\n",
    "        dataset = dataset.reshape(len(dataset), image_size * image_size) # To reshape the  \n",
    "        # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "        labels = (np.arange(no_of_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "# We just reshape the image so that 1 image defines a row\n",
    "\n",
    "# training_dataset[:].reshapeshape\n",
    "training_dataset_, training_labels_ = reshape_data(training_dataset, training_labels)\n",
    "crossvalid_dataset_, crossvalid_labels_ = reshape_data(crossvalid_dataset, crossvalid_labels)\n",
    "test_dataset_, test_labels_ = reshape_data(test_dataset, test_labels)\n",
    "print('Training set', training_dataset_.shape, training_labels_.shape)\n",
    "print('Cross Validation set', crossvalid_dataset_.shape, crossvalid_labels_.shape)\n",
    "print('Test set', test_dataset_.shape, test_labels_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(10000, 784), dtype=float32)\n",
      "[array([-1.33410001,  0.41242442, -0.97492415,  1.15557241, -0.53051233,\n",
      "        0.73306429, -0.41956896, -0.94588572, -0.76755559,  0.43137017], dtype=float32), array([ 0.45035365,  0.83166057, -0.80095267,  0.18668427,  0.88426548,\n",
      "        0.774257  , -0.64032584, -0.31260365, -1.63985634, -0.8018437 ], dtype=float32), array([-0.44362903,  0.31876448,  0.7484569 , -0.47918907, -0.54529822,\n",
      "       -1.32351243, -1.74207163,  1.79766095, -0.51348251,  0.55717045], dtype=float32)]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "[array([  3.23349270e-19,   7.84768875e-22,   1.48457397e-12,\n",
      "         1.26804922e-16,   1.57270877e-10,   3.69564668e-15,\n",
      "         1.72364848e-11,   9.98498246e-12,   1.00000000e+00,\n",
      "         7.36682659e-10], dtype=float32), array([  2.23358831e-04,   1.70561687e-09,   2.41114833e-07,\n",
      "         8.18779711e-09,   9.99395251e-01,   6.37915631e-09,\n",
      "         4.41121358e-07,   2.17440235e-08,   8.97741666e-06,\n",
      "         3.71756178e-04], dtype=float32), array([  5.19907335e-04,   6.02874218e-16,   4.64471300e-12,\n",
      "         7.48806572e-12,   3.99196398e-10,   5.80694069e-11,\n",
      "         1.04591909e-05,   9.69133019e-01,   1.29641721e-03,\n",
      "         2.90400572e-02], dtype=float32), array([  2.93209214e-05,   2.97998431e-11,   8.34654514e-08,\n",
      "         9.96596277e-01,   1.60600455e-03,   3.60287619e-13,\n",
      "         2.50162338e-05,   6.18058039e-07,   1.39489042e-04,\n",
      "         1.60317507e-03], dtype=float32), array([  3.70794714e-11,   4.37348318e-14,   2.09881754e-13,\n",
      "         7.34744585e-18,   1.22587149e-10,   8.30643627e-12,\n",
      "         5.48488085e-16,   4.02360216e-15,   1.00000000e+00,\n",
      "         1.29998556e-09], dtype=float32), array([  2.33364175e-03,   9.66031492e-01,   3.28089098e-14,\n",
      "         1.35083753e-03,   3.02788876e-02,   6.28558733e-11,\n",
      "         2.25869115e-11,   4.00581769e-14,   6.81262918e-07,\n",
      "         4.54452356e-06], dtype=float32), array([  3.06625419e-15,   8.54042657e-20,   3.57862315e-16,\n",
      "         1.21589863e-24,   6.16917018e-16,   1.38855249e-16,\n",
      "         9.23799702e-20,   3.51713658e-10,   1.00000000e+00,\n",
      "         1.11704122e-15], dtype=float32), array([  6.34957032e-05,   2.26088509e-01,   5.82430326e-09,\n",
      "         3.30987185e-01,   4.42842215e-01,   4.23421170e-06,\n",
      "         2.22447769e-10,   1.27630247e-13,   1.36058656e-11,\n",
      "         1.43071702e-05], dtype=float32), array([  5.71808368e-06,   1.79109747e-07,   2.93823987e-08,\n",
      "         6.86044359e-08,   2.98515381e-08,   9.64549827e-08,\n",
      "         3.10231117e-04,   1.33042849e-04,   1.47721866e-21,\n",
      "         9.99550641e-01], dtype=float32), array([  4.02947020e-21,   5.64102899e-17,   7.67048304e-15,\n",
      "         1.95063963e-14,   1.76412352e-13,   2.15592670e-14,\n",
      "         2.23589249e-16,   1.25634747e-17,   9.99999285e-01,\n",
      "         6.95792153e-07], dtype=float32), array([  4.96362456e-12,   4.52782627e-16,   4.71385778e-15,\n",
      "         4.62325565e-19,   1.95623086e-13,   4.02778548e-12,\n",
      "         2.06937486e-20,   1.60164815e-11,   1.00000000e+00,\n",
      "         1.40249534e-15], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# Now we build Our first Neural Net Using TensorFlow:\n",
    "sample_size = 10000\n",
    "\n",
    "   # Loading interactive Tensor flow session to print tesorflow objects\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "def start_session():\n",
    "    init = tf.initialize_all_variables()\n",
    "    session = tf.Session()\n",
    "    session.run(init)\n",
    "    return session\n",
    "    \n",
    "with graph.as_default():\n",
    "    # We load all the training data, test data and crossvalid data into the contants\n",
    "    tf_training_dataset = tf.constant(training_dataset_[:sample_size, :])\n",
    "    tf_training_labels = tf.constant(training_labels_[:sample_size, :])\n",
    "    tf_crossvalid_dataset = tf.constant(crossvalid_dataset_)\n",
    "    tf_crossvalid_labels = tf.constant(crossvalid_labels_)\n",
    "    tf_test_dataset = tf.constant(test_dataset_)\n",
    "    tf_test_labels = tf.constant(test_labels_)\n",
    "    \n",
    "    print (tf_training_dataset)\n",
    "    # Weight Initialization: In weight Initialization the weights are randomly initialized from a normal distribution\n",
    "    # One weight for each pixel and for each output label plus one 1 bais term.\n",
    "    weight_matrix = tf.Variable(tf.truncated_normal([image_size*image_size, no_of_labels]))\n",
    "    print ([w for no, w in enumerate(start_session().run(weight_matrix)) if no<=2])\n",
    "    biases = tf.Variable(tf.zeros([no_of_labels]))\n",
    "    print ([w for w in start_session().run(biases)])\n",
    "    \n",
    "    \n",
    "    # We have now obtained our random weights and x inputs, now lets train our model \n",
    "    # We multiply our weight to X's and add the baises term.\n",
    "    logits = tf.matmul(tf_training_dataset, weight_matrix) + biases\n",
    "    \n",
    "    # The next step after the logit function is to compute the softmax and then the perform the cross-entropy. \n",
    "    # In Tensor flow both the steps are achieved with a single function.\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_training_labels))\n",
    "#     print ([w for no, w in enumerate(start_session().run(loss)) if no<=2])\n",
    "    \n",
    "    # Now we build the optimization function using Gradient Descet to find the mnimum point\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "     \n",
    "    # We have built the logit function and used the optimize to find the minimum point.\n",
    "    # Now we make the prediction and compare the accurary, \n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    print ('')\n",
    "    print ([w for n, w in enumerate(start_session().run(train_prediction)) if n<=10])\n",
    "    crossvalid_prediction = tf.nn.softmax(tf.matmul(tf_crossvalid_dataset, weights) + biases)\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
