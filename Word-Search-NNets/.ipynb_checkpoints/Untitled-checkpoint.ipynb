{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oh!', 'my god, i dont know why did i buy this product, a waste of momney.', 'this is outrageous, i dont get it.', 'my name is sam helsen and i like trekking.', 'what is your name?', 'are you sam helsen?.', 'i am going to america, the land of free.', 'last night i saw a bad man.']\n",
      "['start oh! end', 'start my god, i dont know why did i buy this product, a waste of momney. end', 'start this is outrageous, i dont get it. end', 'start my name is sam helsen and i like trekking. end', 'start what is your name? end', 'start are you sam helsen?. end', 'start i am going to america, the land of free. end', 'start last night i saw a bad man. end']\n",
      "[['start', 'oh', '!', 'end'], ['start', 'my', 'god', ',', 'i', 'dont', 'know', 'why', 'did', 'i', 'buy', 'this', 'product', ',', 'a', 'waste', 'of', 'momney', '.', 'end'], ['start', 'this', 'is', 'outrageous', ',', 'i', 'dont', 'get', 'it', '.', 'end'], ['start', 'my', 'name', 'is', 'sam', 'helsen', 'and', 'i', 'like', 'trekking', '.', 'end'], ['start', 'what', 'is', 'your', 'name', '?', 'end'], ['start', 'are', 'you', 'sam', 'helsen', '?', '.', 'end'], ['start', 'i', 'am', 'going', 'to', 'america', ',', 'the', 'land', 'of', 'free', '.', 'end'], ['start', 'last', 'night', 'i', 'saw', 'a', 'bad', 'man', '.', 'end']]\n",
      "[('bad', 1), ('outrageous', 1), ('are', 1), ('man', 1), ('this', 2), ('i', 6), ('product', 1), ('and', 1), ('did', 1), ('waste', 1), ('helsen', 2), ('end', 8), ('night', 1), ('saw', 1), ('start', 8), ('you', 1), ('it', 1), ('!', 1), ('name', 2), ('what', 1), (',', 4), ('my', 2), ('know', 1), ('oh', 1), ('a', 2), ('the', 1), ('.', 6), ('why', 1), ('momney', 1), ('like', 1), ('get', 1), ('buy', 1), ('your', 1), ('god', 1), ('free', 1), ('dont', 2), ('last', 1), ('sam', 2), ('going', 1), ('land', 1), ('america', 1), ('is', 3), ('trekking', 1), ('to', 1), ('of', 2), ('?', 2), ('am', 1)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import itertools\n",
    "sentence_start_token = 'start'\n",
    "sentence_end_token = 'end'\n",
    "unknown_token = 'unknown'\n",
    "text = 'Oh! my god, I dont know why did I buy this product, A waste of momney. This is outrageous, I dont get it. \\\n",
    "My name is Sam Helsen and I like trekking. What is your name? Are you Sam Helsen?. I am going to America, the land of free.\\\n",
    " Last night I saw a bad man.'  \n",
    "\n",
    "sentences = nltk.sent_tokenize(text.lower())\n",
    "print (sentences)\n",
    "sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "print (sentences)\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "print (tokenized_sentences)\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print ([i for i in word_freq.items()])\n",
    "#print \"Found %d unique words tokens.\" % len(word_freq.items())\n",
    "\n",
    "\n",
    "# Get the most common words and build index_to_word and word_to_index vectors\n",
    "vocabulary_size  =10\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    "\n",
    "print (word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "x = [[0,1,2,3],[0,1,3,4],[],[]]\n",
    "T = 4\n",
    "s = np.zeros((T + 1, hidden_dim))\n",
    "print (s)\n",
    "s[-1] = np.zeros(hidden_dim)\n",
    "o = np.zeros((T, word_dim))\n",
    "# For each time step...\n",
    "for t in np.arange(T):\n",
    "    print (t)\n",
    "#     # Note that we are indxing U by x[t]. This is the same as multiplying U with a one-hot vector.\n",
    "    s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
    "    o[t] = softmax(self.V.dot(s[t]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.316227766017\n",
      "\n",
      "(4, 10)\n",
      "[[-0.27236171 -0.18703718 -0.18567091  0.27620939  0.28939792 -0.22813283\n",
      "   0.25439654 -0.17029446  0.2839872  -0.13270692]\n",
      " [ 0.3160028   0.10751542  0.25551856  0.29342165 -0.09241058  0.07529338\n",
      "   0.22987368 -0.02720081 -0.10473547 -0.18833905]\n",
      " [ 0.30274776 -0.07751939  0.0664514  -0.2098722   0.26154331 -0.21620149\n",
      "   0.04531291 -0.02117388  0.08927019  0.12508741]\n",
      " [ 0.23707875  0.15097284  0.04387921  0.23701334  0.11795245 -0.16434671\n",
      "   0.05265629 -0.00082858  0.20409558  0.22987345]]\n",
      "(10, 4)\n",
      "[[-0.35765353 -0.18695445 -0.21492783  0.21206408]\n",
      " [ 0.15224721 -0.48947437 -0.09707876  0.26061484]\n",
      " [ 0.40722038 -0.09077849 -0.16145835  0.16016908]\n",
      " [ 0.4139697  -0.35759196 -0.3651696  -0.39993232]\n",
      " [ 0.01846623 -0.00866036  0.40356936 -0.37015085]\n",
      " [-0.40981685  0.14122401  0.2990866   0.15862299]\n",
      " [-0.19565198 -0.11091773 -0.2393248   0.12664907]\n",
      " [ 0.29480836  0.22802878  0.41001215 -0.35944148]\n",
      " [-0.47946292 -0.45633372  0.20869186 -0.05419534]\n",
      " [ 0.09711341  0.44995042  0.28132914 -0.10234039]]\n",
      "(4, 4)\n",
      "[[ 0.03586495  0.46263454 -0.06180025 -0.17202426]\n",
      " [-0.37423768 -0.47782551 -0.11043094 -0.00478881]\n",
      " [-0.41260667  0.33788075  0.48846016 -0.26418101]\n",
      " [-0.1089187   0.0598078   0.43341236 -0.0870724 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "word_dim = 10\n",
    "hidden_dim = 4\n",
    "#bptt_truncate = bptt_truncate\n",
    "# Randomly initialize the network parameters\n",
    "# initialize the weights randomly in the interval from [-1/sqrt(n), -1/sqrt(n)] \n",
    "# where n is the number of incoming connections from the previous layer.\n",
    "U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
    "\n",
    "\n",
    "print (-np.sqrt(1./word_dim))\n",
    "print ('')\n",
    "print (U.shape)\n",
    "print (U)\n",
    "print (V.shape)\n",
    "print (V)\n",
    "print (W.shape)\n",
    "print (W)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
