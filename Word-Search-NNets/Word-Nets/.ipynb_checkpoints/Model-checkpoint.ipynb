{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "    BEFORE RUNNING THIS CODE YOU SHOULD FIRST RUN THE \"DATA_BUILDER.PY\" TO FIRST EXTRACT, CLEAN AND LOAD THE DATA\n",
    "    INTO PICKLE FILES AND THEN THIS CODE WILL PART COME IN HANDY.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Notes:\n",
    "    1. ENBEDDING(INPUT) LAYER OPERATION\n",
    "       --> embed_to_hid_wghts = tf.get_variable('embedding_matrix', [vocab_size, num_hid_units]):\n",
    "           embed_to_hid_layer = tf.nn.embedding_lookup(embed_to_hid_wghts, x)\n",
    "           # Normally we convert the input vector into a one hot matrix and then multiply it to the embedded weights, \n",
    "           When we do so, we get the same embed weight corresponding to 1's in the one-hot vector but in a different \n",
    "           shape. The above operation does all that in a single shot. Basically, embed_to_hid_wghts defines a matrix \n",
    "           with weights going form all vacab to hiddenunits,and embed_to_hid_layer pulls the vectors from embedding_matrix\n",
    "           (embed_to_hid_wghts) corresponding to the idices entries of x for all the batch. \n",
    "           So the matrix embed_to_hid_layer = [batch_size x num_sequences x num_hid_units]\n",
    "\n",
    "    2. HIDDEN LAYER OPERATION\n",
    "       --> The output from dynamic_rnn \"rnn_output\" is a Tensor of shape of [Batch_size x num_sequence x num_hid_units] and,\n",
    "           The hid_to_output_wght is in the shape of [num_hid_units x num_classes]\n",
    "           And We want an output with shape [Batch_size x num_sequence x num_classes]\n",
    "           We horizontlly stack all the batches to form a matrix of [(Batch_size x num_sequence]) x num_classes]\n",
    "       --> In the dynamic_run we provide the \"sequence_length\", this would say the RNN that the batch are padded after\n",
    "           after the given size. Therefore the RNN doesnt consider the padded sequences while calculating the RNN output.\n",
    "           When the actual sequence length is given then the RNN would simply consider the rnn_output as 0 for the padded\n",
    "           sequence\n",
    "\n",
    "    3. OUTPUT LAYER OPERATION\n",
    "       --> sparse_softmax_cross_entropy_with_logits automatically converts the y's into on hot vectors and perform \n",
    "           the softmax operation When using softmax_cross_entropy_with_logits, we have to first convert the y's \n",
    "           into one-hot vector\n",
    "\n",
    "    4. MASK THE LOSES:\n",
    "       --> We can calculate the loss directly as we do for every batch. Normally to calculate the loss we do:\n",
    "           loss_CE = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(hid_to_ouptut_layer, tf.reshape(y, [-1])))\n",
    "           But here we dont do the complete stuff at once because we have zero (0) padded all the sequences in a batch for \n",
    "           equal size . Naively calculating the loss at each time step doesn’t work because that would take into account\n",
    "           the padded positions. So the solution is to create a weight matrix that “masks out” the losses at padded positions.\n",
    "       --> Intuition for why do we do mask. \"softmax_opt\" will give a array of size [(num_sequence*batch_size) * 0],\n",
    "           and reduced_mean just sums up all the element of the softmax and divides it with the array size.\n",
    "           And we know that many of the sequences here are padded with 0's whose softmax output should not be considered\n",
    "           while performing the reduced_mean. Hence masking converts the softmax of those padded 0's to 0 and calculate\n",
    "           the mean of all training example in a batch separetely. And finally computes the reduced mean. \n",
    "       \n",
    "       \n",
    "    5. VERY IMPORTANT NOTE:\n",
    "       --> After we receive the dictionary from the gensim dictionary we need to add 1. Because the gensim dictionary \n",
    "           is build from index 0 to n. But our training or testing batch consist of index from 1 to n+1 (achieved in data_builder.py)\n",
    "           . The addition of 1 is imperative because the embedding_matrix internally builds a corpus of (n*num_hidden_unit)\n",
    "           and it builds from 0 so if we dont add 1 then the last word in the dictionarry will have a nan corresponding to its\n",
    "           output and our loss will be nan too. Just to recap, we add 1 because we do zero (0) padding of the sequence length for a batch. \n",
    "        \n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import urllib.request\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.models.rnn.ptb import reader\n",
    "from gensim import corpora\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_graph():  # Reset the graph\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dynamic_RNN_model(\n",
    "    num_hid_units = 3,\n",
    "    vocab_size = 7,\n",
    "    momentum = 0.9,\n",
    "    learning_rate = 0.1,\n",
    "    output_activation_init = 'RELU' \n",
    "    ):\n",
    "    print ('The num of hidden unit is: ', num_hid_units)\n",
    "    print ('The Vocab size is: ', vocab_size)\n",
    "    print ('The momentum is: ', momentum)\n",
    "    print ('The learning_rate is: ', learning_rate)\n",
    "    \n",
    "    \n",
    "    num_classes = vocab_size\n",
    "\n",
    "    reset_graph()\n",
    "    \n",
    "    x = tf.placeholder(tf.int32, shape = [None, None], name='input_placeholder')\n",
    "    y = tf.placeholder(tf.int32, shape = [None, None], name='output_placeholdr')\n",
    "    x_lenarr = tf.placeholder(tf.float32, shape = [None], name='output_placeholdr')\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    \n",
    "\n",
    "    # ENBEDDING(INPUT) LAYER OPERATION\n",
    "    # Creating an Embedding matrix with a random weight for all vacab to hidden_matrix\n",
    "    embed_to_hid_wghts = tf.get_variable('embedding_matrix', [vocab_size, num_hid_units])\n",
    "    embed_to_hid_layer = tf.nn.embedding_lookup(embed_to_hid_wghts, x)\n",
    "    print ('The shape of embed_to_hid_wghts is: ', embed_to_hid_wghts.get_shape())\n",
    "    print ('The shape of embed_to_hid_layer is: ', embed_to_hid_layer.get_shape())\n",
    "\n",
    "\n",
    "    # HIDDEN LAYER OPERATION\n",
    "    rnn_cell = tf.nn.rnn_cell.LSTMCell(num_hid_units, state_is_tuple=True)\n",
    "    init_state = rnn_cell.zero_state(batch_size, tf.float32)  # Each sequence will hava a state that it passes to its next sequence\n",
    "    rnn_outputs, new_state = tf.nn.dynamic_rnn(cell=rnn_cell,\n",
    "                                               sequence_length=x_lenarr,\n",
    "                                               initial_state=init_state,\n",
    "                                               inputs=embed_to_hid_layer,\n",
    "                                               dtype=tf.float32)\n",
    "    \n",
    "\n",
    "    # OUTPUT LAYER OPERATION\n",
    "    # Initialize the weight and biases for the output layer. We use variable scope because we would like to share the weights \n",
    "    with tf.variable_scope('output_layer'):\n",
    "        hid_to_output_wght = tf.get_variable('hid_to_output_wght', [num_hid_units, num_classes],\n",
    "                                            initializer = tf.random_normal_initializer())\n",
    "        output_bias = tf.get_variable('output_bias', [num_classes],\n",
    "                                      initializer=tf.random_normal_initializer())\n",
    "    \n",
    "    rnn_outputs = tf.reshape(rnn_outputs, [-1, num_hid_units])\n",
    "    hid_to_ouptut_layer = tf.matmul(rnn_outputs, hid_to_output_wght) +  output_bias  \n",
    "    # Also use tf.batch_matmul(rnn_outputs, hid_to_output_wght) +  output_bias  \n",
    "    output_state = tf.nn.softmax(hid_to_ouptut_layer, name=None)\n",
    " \n",
    "    \n",
    "    # SOFTMAX OUTPUT\n",
    "    y_reshaped = tf.reshape(y, [-1])\n",
    "    softmax_opt = tf.nn.sparse_softmax_cross_entropy_with_logits(hid_to_ouptut_layer, y_reshaped)\n",
    "    \n",
    "    # MASK THE LOSES\n",
    "    mask = tf.sign(tf.to_float(y_reshaped))\n",
    "    masked_loss = mask * softmax_opt\n",
    "    masked_loss = tf.reshape(masked_loss,  tf.shape(y))\n",
    "    mean_loss_by_example = tf.reduce_sum(masked_loss, reduction_indices=1) / x_lenarr\n",
    "    mean_loss = tf.reduce_mean(mean_loss_by_example)\n",
    "\n",
    "    \n",
    "    # OPTIMIZING THE LOSS FUNCTION\n",
    "#     optimizer = tf.train.AdamOptimizer(learning_rate).minimize(mean_loss)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, \n",
    "                                            momentum, \n",
    "                                            use_locking=False, \n",
    "                                            name='Momentum', \n",
    "                                            use_nesterov=True).minimize(mean_loss)\n",
    "\n",
    "    # Returns graph objects\n",
    "    return dict(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        x_lenarr=x_lenarr,\n",
    "        batch_size = batch_size,\n",
    "        init_state = init_state,\n",
    "        new_state = new_state,\n",
    "        loss = mean_loss,\n",
    "        optimizer = optimizer,\n",
    "        prediction = output_state\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Train():\n",
    "    def __init__(self, valid_num_batches=10):\n",
    "        self.train_batch_dir = '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Word-Search-NNets/Word-Nets/training_batch/'\n",
    "        self.valid_batch_dir = '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Word-Search-NNets/Word-Nets/crossvalid_batch/'\n",
    "        dictionary_dir = '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Word-Search-NNets/Word-Nets/dictionary.txt'\n",
    "        self.vocab_size = len(corpora.Dictionary.load_from_text(dictionary_dir))\n",
    "        \n",
    "        # Randomly select group of 5 batches from the cross valid dataset to test after every 20 batches\n",
    "        self.cdoc = np.random.choice(np.arange(50), valid_num_batches) \n",
    "\n",
    "    def accuracy(self, predictions, labels, labels_one_hot = None):\n",
    "        # The input labels are a One-Hot Vector\n",
    "        if labels_one_hot:\n",
    "            return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "                  / predictions.shape[0])\n",
    "        else:\n",
    "            return (100.0 * np.sum(np.argmax(predictions, 1) == np.reshape(labels, [-1]))\n",
    "                  / predictions.shape[0])\n",
    "        \n",
    "    def cross_valid(self, sess, graph_dict):\n",
    "        new_valid_state_ = None\n",
    "        for cdoc_no in self.cdoc:\n",
    "            with open(self.valid_batch_dir+'batch'+str(cdoc_no)+'.pickle', 'rb') as f1:\n",
    "                dataset = pickle.load(f1)\n",
    "\n",
    "                batch_valid_dataset = dataset['batch_valid_dataset']\n",
    "                batch_valid_labels = dataset['batch_valid_labels']\n",
    "                batch_valid_lenarr = dataset['batch_valid_lenarr']\n",
    "\n",
    "                if new_valid_state_ is not None:\n",
    "                    feed_dict={graph_dict['x']: batch_valid_dataset,\n",
    "                               graph_dict['x_lenarr']: batch_valid_lenarr,\n",
    "                               graph_dict['init_state']: new_valid_state_}\n",
    "                else:\n",
    "                    feed_dict={graph_dict['x']: batch_valid_dataset,\n",
    "                               graph_dict['x_lenarr']: batch_valid_lenarr}\n",
    "\n",
    "                valid_prediction_, new_valid_state_ = sess.run([graph_dict['prediction'],\n",
    "                                                                graph_dict['new_state']], \n",
    "                                                                feed_dict)\n",
    "\n",
    "            acc_valid = self.accuracy(valid_prediction_, batch_valid_labels)\n",
    "            print ('accuracy of the validation batch %d is: '%cdoc_no, acc_valid)\n",
    "                        \n",
    "                        \n",
    "    def train_network(self, graph_dict, num_batches, epochs=1, verbose=None ):\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "            for epoch in np.arange(epochs):\n",
    "                new_state_ = None\n",
    "                training_loss = 0\n",
    "                print ('')\n",
    "                print ('training training training, training training training, training training training training training')\n",
    "                for no in np.arange(num_batches):\n",
    "                    with open(self.train_batch_dir+'batch'+str(no)+'.pickle', 'rb') as f:\n",
    "                        dataset = pickle.load(f)\n",
    "                        \n",
    "                        batch_train_dataset = dataset['batch_train_dataset']\n",
    "                        batch_train_labels = dataset['batch_train_labels']\n",
    "                        batch_train_lenarr = dataset['batch_train_lenarr']\n",
    "                        \n",
    "                        feed_dict= {graph_dict['x']: batch_train_dataset, \n",
    "                                    graph_dict['y']: batch_train_labels,\n",
    "                                    graph_dict['x_lenarr']: batch_train_lenarr}\n",
    "            \n",
    "                        if new_state_ is not None:\n",
    "                            feed_dict[graph_dict['init_state']] = new_state_\n",
    "\n",
    "                        new_state_, loss_, opt, tp = sess.run([graph_dict['new_state'],\n",
    "                                                            graph_dict['loss'],\n",
    "                                                            graph_dict['optimizer'],\n",
    "                                                            graph_dict['prediction']], \n",
    "                                                            feed_dict=feed_dict)\n",
    "                        \n",
    "                        training_loss += loss_\n",
    "                        acc = self.accuracy(tp, batch_train_labels)\n",
    "                    \n",
    "                        \n",
    "                        print ('accuracy of the training batch %d is: '%no, acc)\n",
    "                        \n",
    "                        \n",
    "                        if (no%50==0 and no!=0):\n",
    "                            print ('Average Loss till the training batch %d is: '%no, training_loss/no)\n",
    "                            print ('')\n",
    "                            print ('crossvalid crossvalid crossvalid, crossvalid crossvalid crossvalid, crossvalid crossvalid crossvalid')\n",
    "                            self.cross_valid(sess,graph_dict)\n",
    "                            print ('')\n",
    "                            print ('training training training, training training training, training training training training training')\n",
    "                            \n",
    "                print ('All %d Batches Done.................................'%num_batches)\n",
    "                print ('')\n",
    "                print ('')\n",
    "                \n",
    "                if verbose:\n",
    "                    print(\"Average training loss for Epoch\", epoch, \":\", training_loss/num_batches)\n",
    "                        \n",
    "#                     \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The num of hidden unit is:  300\n",
      "The Vocab size is:  17156\n",
      "The momentum is:  0.9\n",
      "The learning_rate is:  0.1\n",
      "The shape of embed_to_hid_wghts is:  (17156, 300)\n",
      "The shape of embed_to_hid_layer is:  (?, ?, 300)\n",
      "\n",
      "training training training, training training training, training training training training training\n",
      "Average Loss for the batch 0 is:  10.2398\n",
      "accuracy of the training batch 0 is:  0.0\n",
      "Average Loss for the batch 1 is:  9.37134\n",
      "accuracy of the training batch 1 is:  0.578703703704\n",
      "Average Loss for the batch 2 is:  9.34096\n",
      "accuracy of the training batch 2 is:  1.08695652174\n",
      "Average Loss for the batch 3 is:  11.0334\n",
      "accuracy of the training batch 3 is:  2.69607843137\n",
      "Average Loss for the batch 4 is:  10.5861\n",
      "accuracy of the training batch 4 is:  0.0351123595506\n"
     ]
    }
   ],
   "source": [
    "obj_Train = Train()\n",
    "graph_dict =  dynamic_RNN_model(num_hid_units=300, vocab_size = obj_Train.vocab_size+1,)\n",
    "obj_Train.train_network(graph_dict, num_batches = 500, epochs=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
