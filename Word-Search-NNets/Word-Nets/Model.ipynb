{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import urllib.request\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.models.rnn.ptb import reader\n",
    "from gensim import corpora\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "    BEFORE RUNNING THIS CODE YOU SHOULD FIRST RUN THE \"DATA_BUILDER.PY\" TO FIRST EXTRACT, CLEAN AND LOAD THE DATA\n",
    "    INTO PICKLE FILES AND THEN THIS CODE WILL PART COME IN HANDY.\n",
    "\"\"\"\n",
    "\n",
    "############################  NOTES:: ###########################\n",
    "\"\"\"\n",
    "    1. ENBEDDING(INPUT) LAYER OPERATION\n",
    "       --> embed_to_hid_wghts = tf.get_variable('embedding_matrix', [vocab_size, num_hid_units]):\n",
    "           embed_to_hid_layer = tf.nn.embedding_lookup(embed_to_hid_wghts, x)\n",
    "           # Normally we convert the input vector into a one hot matrix and then multiply it to the embedded weights, \n",
    "           When we do so, we get the same embed weight corresponding to 1's in the one-hot vector but in a different \n",
    "           shape. The above operation does all that in a single shot. Basically, embed_to_hid_wghts defines a matrix \n",
    "           with weights going form all vacab to hiddenunits,and embed_to_hid_layer pulls the vectors from embedding_matrix\n",
    "           (embed_to_hid_wghts) corresponding to the idices entries of x for all the batch. \n",
    "           So the matrix embed_to_hid_layer = [batch_size x num_sequences x num_hid_units]\n",
    "\n",
    "    2. HIDDEN LAYER OPERATION\n",
    "       --> The output from dynamic_rnn \"rnn_output\" is a Tensor of shape of [Batch_size x num_sequence x num_hid_units] and,\n",
    "           The hid_to_output_wght is in the shape of [num_hid_units x num_classes]\n",
    "           And We want an output with shape [Batch_size x num_sequence x num_classes]\n",
    "           We horizontlly stack all the batches to form a matrix of [(Batch_size x num_sequence]) x num_classes]\n",
    "       --> In the dynamic_run we provide the \"sequence_length\", this would say the RNN that the batch are padded after\n",
    "           after the given size. Therefore the RNN doesnt consider the padded sequences while calculating the RNN output.\n",
    "           When the actual sequence length is given then the RNN would simply consider the rnn_output as 0 for the padded\n",
    "           sequence\n",
    "\n",
    "    3. OUTPUT LAYER OPERATION\n",
    "       --> sparse_softmax_cross_entropy_with_logits automatically converts the y's into on hot vectors and perform \n",
    "           the softmax operation When using softmax_cross_entropy_with_logits, we have to first convert the y's \n",
    "           into one-hot vector\n",
    "\n",
    "    4. MASK THE LOSES:\n",
    "       --> We can calculate the loss directly as we do for every batch. Normally to calculate the loss we do:\n",
    "           loss_CE = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(hid_to_ouptut_layer, tf.reshape(y, [-1])))\n",
    "           But here we dont do the complete stuff at once because we have zero (0) padded all the sequences in a batch for \n",
    "           equal size . Naively calculating the loss at each time step doesn’t work because that would take into account\n",
    "           the padded positions. So the solution is to create a weight matrix that “masks out” the losses at padded positions.\n",
    "       --> Intuition for why do we do mask. \"softmax_opt\" will give a array of size [(num_sequence*batch_size) * 0],\n",
    "           and reduced_mean just sums up all the element of the softmax and divides it with the array size.\n",
    "           And we know that many of the sequences here are padded with 0's whose softmax output should not be considered\n",
    "           while performing the reduced_mean. Hence masking converts the softmax of those padded 0's to 0 and calculate\n",
    "           the mean of all training example in a batch separetely. And finally computes the reduced mean. \n",
    "       \n",
    "       \n",
    "    5. VERY IMPORTANT NOTE:\n",
    "       --> After we receive the dictionary from the gensim dictionary we need to add 1. Because the gensim dictionary \n",
    "           is build from index 0 to n. But our training or testing batch consist of index from 1 to n+1 (achieved in data_builder.py)\n",
    "           . The addition of 1 is imperative because the embedding_matrix internally builds a corpus of (n*num_hidden_unit)\n",
    "           and it builds from 0 so if we dont add 1 then the last word in the dictionarry will have a nan corresponding to its\n",
    "           output and our loss will be nan too. Just to recap, we add 1 because we do zero (0) padding of the sequence length for a batch. \n",
    "        \n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "################  Some interesting facts we observe: ###############\n",
    "\"\"\"\n",
    "1. We apply both multilayer and single layer approach with RNN. We found that using all variable\n",
    "   num_hid_units. Single layer gave better accuracy that multilayer.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_graph():  # Reset the graph\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dynamic_RNN_model(\n",
    "    num_hid_units = 3,\n",
    "    vocab_size = 7,\n",
    "    momentum = 0.9,\n",
    "    learning_rate = 0.1,\n",
    "    num_layers = None\n",
    "    ):\n",
    "    print ('The num of hidden unit is: ', num_hid_units)\n",
    "    print ('The Vocab size is: ', vocab_size)\n",
    "    print ('The momentum is: ', momentum)\n",
    "    print ('The learning_rate is: ', learning_rate)\n",
    "    \n",
    "    \n",
    "    num_classes = vocab_size\n",
    "\n",
    "    reset_graph()\n",
    "    \n",
    "    x = tf.placeholder(tf.int32, shape = [None, None], name='input_placeholder')\n",
    "    y = tf.placeholder(tf.int32, shape = [None, None], name='output_placeholdr')\n",
    "    x_lenarr = tf.placeholder(tf.float32, shape = [None], name='output_placeholdr')\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    \n",
    "\n",
    "    # ENBEDDING(INPUT) LAYER OPERATION\n",
    "    # Creating an Embedding matrix with a random weight for all vacab to hidden_matrix\n",
    "    embed_to_hid_wghts = tf.get_variable('embedding_matrix', [vocab_size, num_hid_units])\n",
    "    embed_to_hid_layer = tf.nn.embedding_lookup(embed_to_hid_wghts, x)\n",
    "    print ('The shape of embed_to_hid_wghts is: ', embed_to_hid_wghts.get_shape())\n",
    "    print ('The shape of embed_to_hid_layer is: ', embed_to_hid_layer.get_shape())\n",
    "\n",
    "\n",
    "    # HIDDEN LAYER OPERATION\n",
    "    rnn_cell = tf.nn.rnn_cell.LSTMCell(num_hid_units, state_is_tuple=True)\n",
    "    if num_layers:\n",
    "        rnn_cell = tf.nn.rnn_cell.MultiRNNCell([rnn_cell] * num_layers, state_is_tuple=True)\n",
    "    init_state = rnn_cell.zero_state(batch_size, tf.float32)  # Each sequence will hava a state that it passes to its next sequence\n",
    "    rnn_outputs, new_state = tf.nn.dynamic_rnn(cell=rnn_cell,\n",
    "                                               sequence_length=x_lenarr,\n",
    "                                               initial_state=init_state,\n",
    "                                               inputs=embed_to_hid_layer,\n",
    "                                               dtype=tf.float32)\n",
    "    \n",
    "\n",
    "    # OUTPUT LAYER OPERATION\n",
    "    # Initialize the weight and biases for the output layer. We use variable scope because we would like to share the weights \n",
    "    with tf.variable_scope('output_layer'):\n",
    "        hid_to_output_wght = tf.get_variable('hid_to_output_wght', [num_hid_units, num_classes],\n",
    "                                            initializer = tf.random_normal_initializer())\n",
    "        output_bias = tf.get_variable('output_bias', [num_classes],\n",
    "                                      initializer=tf.random_normal_initializer())\n",
    "    \n",
    "    rnn_outputs = tf.reshape(rnn_outputs, [-1, num_hid_units])\n",
    "    hid_to_ouptut_layer = tf.matmul(rnn_outputs, hid_to_output_wght) +  output_bias  \n",
    "    # Also use tf.batch_matmul(rnn_outputs, hid_to_output_wght) +  output_bias  \n",
    "    output_state = tf.nn.softmax(hid_to_ouptut_layer, name=None)\n",
    " \n",
    "    \n",
    "    # SOFTMAX OUTPUT\n",
    "    y_reshaped = tf.reshape(y, [-1])\n",
    "    softmax_opt = tf.nn.sparse_softmax_cross_entropy_with_logits(hid_to_ouptut_layer, y_reshaped)\n",
    "    \n",
    "    # MASK THE LOSES\n",
    "    mask = tf.sign(tf.to_float(y_reshaped))\n",
    "    masked_loss = mask * softmax_opt\n",
    "    masked_loss = tf.reshape(masked_loss,  tf.shape(y))\n",
    "    mean_loss_by_example = tf.reduce_sum(masked_loss, reduction_indices=1) / x_lenarr\n",
    "    mean_loss = tf.reduce_mean(mean_loss_by_example)\n",
    "\n",
    "    \n",
    "    # OPTIMIZING THE LOSS FUNCTION\n",
    "#     optimizer = tf.train.AdamOptimizer(learning_rate).minimize(mean_loss)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, \n",
    "                                            momentum, \n",
    "                                            use_locking=False, \n",
    "                                            name='Momentum', \n",
    "                                            use_nesterov=True).minimize(mean_loss)\n",
    "\n",
    "    # Returns graph objects\n",
    "    return dict(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        x_lenarr=x_lenarr,\n",
    "        batch_size = batch_size,\n",
    "        init_state = init_state,\n",
    "        new_state = new_state,\n",
    "        loss = mean_loss,\n",
    "        optimizer = optimizer,\n",
    "        prediction = output_state\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Train():\n",
    "    def __init__(self, valid_num_batches=10):\n",
    "        self.train_batch_dir = '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Word-Search-NNets/Word-Nets/training_batch/'\n",
    "        self.valid_batch_dir = '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Word-Search-NNets/Word-Nets/crossvalid_batch/'\n",
    "        dictionary_dir = '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Word-Search-NNets/Word-Nets/dictionary.txt'\n",
    "        self.vocab_size = len(corpora.Dictionary.load_from_text(dictionary_dir))\n",
    "        \n",
    "        # Randomly select group of 5 batches from the cross valid dataset to test after every 20 batches\n",
    "        self.cdoc = np.random.choice(np.arange(30), valid_num_batches) \n",
    "\n",
    "    def accuracy(self, predictions, labels, labels_one_hot = None):\n",
    "        # The input labels are a One-Hot Vector\n",
    "        if labels_one_hot:\n",
    "            return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "                  / predictions.shape[0])\n",
    "        else:\n",
    "            return (100.0 * np.sum(np.argmax(predictions, 1) == np.reshape(labels, [-1]))\n",
    "                  / predictions.shape[0])\n",
    "        \n",
    "    def cross_valid(self, sess, graph_dict):\n",
    "        new_valid_state_ = None\n",
    "        for cdoc_no in self.cdoc:\n",
    "            with open(self.valid_batch_dir+'batch'+str(cdoc_no)+'.pickle', 'rb') as f1:\n",
    "                dataset = pickle.load(f1)\n",
    "\n",
    "                batch_valid_dataset = dataset['batch_valid_dataset']\n",
    "                batch_valid_labels = dataset['batch_valid_labels']\n",
    "                batch_valid_lenarr = dataset['batch_valid_lenarr']\n",
    "\n",
    "                if new_valid_state_ is not None:\n",
    "                    feed_dict={graph_dict['x']: batch_valid_dataset,\n",
    "                               graph_dict['x_lenarr']: batch_valid_lenarr,\n",
    "                               graph_dict['init_state']: new_valid_state_}\n",
    "                else:\n",
    "                    feed_dict={graph_dict['x']: batch_valid_dataset,\n",
    "                               graph_dict['x_lenarr']: batch_valid_lenarr}\n",
    "\n",
    "                valid_prediction_, new_valid_state_ = sess.run([graph_dict['prediction'],\n",
    "                                                                graph_dict['new_state']], \n",
    "                                                                feed_dict)\n",
    "\n",
    "            acc_valid = self.accuracy(valid_prediction_, batch_valid_labels)\n",
    "            print ('accuracy of the validation batch %d is: '%cdoc_no, acc_valid)\n",
    "                        \n",
    "                        \n",
    "    def train_network(self, graph_dict, num_batches, \n",
    "                      when_valid, epochs=1, verbose=None, \n",
    "                      path_save_model=False):\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "            for epoch in np.arange(epochs):\n",
    "                new_state_ = None\n",
    "                training_loss = 0\n",
    "                print ('')\n",
    "                print ('training training training, training training training, training training training training training')\n",
    "                for no in np.arange(num_batches):\n",
    "                    with open(self.train_batch_dir+'batch'+str(no)+'.pickle', 'rb') as f:\n",
    "                        dataset = pickle.load(f)\n",
    "                        \n",
    "                        batch_train_dataset = dataset['batch_train_dataset']\n",
    "                        batch_train_labels = dataset['batch_train_labels']\n",
    "                        batch_train_lenarr = dataset['batch_train_lenarr']\n",
    "                        \n",
    "                        feed_dict= {graph_dict['x']: batch_train_dataset, \n",
    "                                    graph_dict['y']: batch_train_labels,\n",
    "                                    graph_dict['x_lenarr']: batch_train_lenarr}\n",
    "            \n",
    "                        if new_state_ is not None:\n",
    "                            feed_dict[graph_dict['init_state']] = new_state_\n",
    "\n",
    "                        new_state_, loss_, opt, tp = sess.run([graph_dict['new_state'],\n",
    "                                                            graph_dict['loss'],\n",
    "                                                            graph_dict['optimizer'],\n",
    "                                                            graph_dict['prediction']], \n",
    "                                                            feed_dict=feed_dict)\n",
    "                        \n",
    "                        training_loss += loss_\n",
    "                        acc = self.accuracy(tp, batch_train_labels)\n",
    "                    \n",
    "                        \n",
    "                        print ('accuracy of the training batch %d is: '%no, acc)\n",
    "                        \n",
    "                        \n",
    "                        if (no%when_valid==0 and no!=0):\n",
    "                            print ('Average Loss till the training batch %d is: '%no, training_loss/no)\n",
    "                            print ('')\n",
    "                            print ('crossvalid crossvalid crossvalid, crossvalid crossvalid crossvalid, crossvalid crossvalid crossvalid')\n",
    "                            self.cross_valid(sess,graph_dict)\n",
    "                            print ('')\n",
    "                            print ('training training training, training training training, training training training training training')\n",
    "                            \n",
    "                print ('All %d Batches Done.................................'%num_batches)\n",
    "                print ('')\n",
    "                print ('')\n",
    "                \n",
    "                if verbose:\n",
    "                    print(\"Average training loss for Epoch\", epoch, \":\", training_loss/num_batches)\n",
    "            \n",
    "            if isinstance(path_save_model, str):\n",
    "                graph_dict['saver'].save(sess, path_save)\n",
    "#                     \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The num of hidden unit is:  300\n",
      "The Vocab size is:  17156\n",
      "The momentum is:  0.9\n",
      "The learning_rate is:  0.1\n",
      "The shape of embed_to_hid_wghts is:  (17156, 300)\n",
      "The shape of embed_to_hid_layer is:  (?, ?, 300)\n",
      "\n",
      "training training training, training training training, training training training training training\n",
      "accuracy of the training batch 0 is:  0.0\n",
      "accuracy of the training batch 1 is:  1.37867647059\n",
      "accuracy of the training batch 2 is:  1.99854651163\n",
      "Average Loss till the training batch 2 is:  14.5978646278\n",
      "\n",
      "crossvalid crossvalid crossvalid, crossvalid crossvalid crossvalid, crossvalid crossvalid crossvalid\n",
      "accuracy of the validation batch 24 is:  1.67763157895\n",
      "accuracy of the validation batch 2 is:  3.57142857143\n",
      "accuracy of the validation batch 13 is:  3.18509615385\n",
      "accuracy of the validation batch 20 is:  2.52976190476\n",
      "accuracy of the validation batch 27 is:  2.78846153846\n",
      "accuracy of the validation batch 9 is:  2.26362179487\n",
      "accuracy of the validation batch 9 is:  2.28365384615\n",
      "accuracy of the validation batch 29 is:  2.59712837838\n",
      "accuracy of the validation batch 25 is:  1.36542792793\n",
      "accuracy of the validation batch 16 is:  1.7578125\n",
      "\n",
      "training training training, training training training, training training training training training\n",
      "accuracy of the training batch 3 is:  1.20738636364\n",
      "accuracy of the training batch 4 is:  0.0532670454545\n",
      "Average Loss till the training batch 4 is:  13.4100189209\n",
      "\n",
      "crossvalid crossvalid crossvalid, crossvalid crossvalid crossvalid, crossvalid crossvalid crossvalid\n",
      "accuracy of the validation batch 24 is:  0.0\n",
      "accuracy of the validation batch 2 is:  0.0\n",
      "accuracy of the validation batch 13 is:  0.0\n",
      "accuracy of the validation batch 20 is:  0.0\n",
      "accuracy of the validation batch 27 is:  0.0\n",
      "accuracy of the validation batch 9 is:  0.0\n",
      "accuracy of the validation batch 9 is:  0.0\n",
      "accuracy of the validation batch 29 is:  0.0\n",
      "accuracy of the validation batch 25 is:  0.0\n",
      "accuracy of the validation batch 16 is:  0.0\n",
      "\n",
      "training training training, training training training, training training training training training\n",
      "accuracy of the training batch 5 is:  0.0952743902439\n",
      "accuracy of the training batch 6 is:  1.15131578947\n",
      "Average Loss till the training batch 6 is:  12.3069698016\n",
      "\n",
      "crossvalid crossvalid crossvalid, crossvalid crossvalid crossvalid, crossvalid crossvalid crossvalid\n",
      "accuracy of the validation batch 24 is:  2.25328947368\n",
      "accuracy of the validation batch 2 is:  4.17729591837\n",
      "accuracy of the validation batch 13 is:  4.17668269231\n",
      "accuracy of the validation batch 20 is:  3.1001984127\n",
      "accuracy of the validation batch 27 is:  3.58173076923\n",
      "accuracy of the validation batch 9 is:  2.88461538462\n",
      "accuracy of the validation batch 9 is:  2.88461538462\n",
      "accuracy of the validation batch 29 is:  3.10388513514\n",
      "accuracy of the validation batch 25 is:  2.23817567568\n",
      "accuracy of the validation batch 16 is:  2.94744318182\n",
      "\n",
      "training training training, training training training, training training training training training\n",
      "accuracy of the training batch 7 is:  4.09226190476\n",
      "accuracy of the training batch 8 is:  3.54352678571\n",
      "Average Loss till the training batch 8 is:  11.5460824966\n",
      "\n",
      "crossvalid crossvalid crossvalid, crossvalid crossvalid crossvalid, crossvalid crossvalid crossvalid\n",
      "accuracy of the validation batch 24 is:  1.64473684211\n",
      "accuracy of the validation batch 2 is:  2.80612244898\n",
      "accuracy of the validation batch 13 is:  2.25360576923\n",
      "accuracy of the validation batch 20 is:  2.20734126984\n",
      "accuracy of the validation batch 27 is:  2.04326923077\n",
      "accuracy of the validation batch 9 is:  1.68269230769\n",
      "accuracy of the validation batch 9 is:  1.68269230769\n",
      "accuracy of the validation batch 29 is:  1.83699324324\n",
      "accuracy of the validation batch 25 is:  1.02759009009\n",
      "accuracy of the validation batch 16 is:  1.10085227273\n",
      "\n",
      "training training training, training training training, training training training training training\n",
      "accuracy of the training batch 9 is:  2.17803030303\n",
      "All 10 Batches Done.................................\n",
      "\n",
      "\n",
      "Average training loss for Epoch 0 : 10.2937021255\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'saver'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-9cd8e3d94559>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                                 num_layers = None)\n\u001b[1;32m      8\u001b[0m obj_Train.train_network(graph_dict, num_batches = 10, \n\u001b[0;32m----> 9\u001b[0;31m                         when_valid=2, epochs=1, verbose=True, path_save_model=path_save_model)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-c32724d0fb71>\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(self, graph_dict, num_batches, when_valid, epochs, verbose, path_save_model)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_save_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                 \u001b[0mgraph_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'saver'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_save\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'saver'"
     ]
    }
   ],
   "source": [
    "path_save_model=\"/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Word-Search-NNets/Word-Nets/singlelayerLSTM_300\"\n",
    "obj_Train = Train()\n",
    "graph_dict =  dynamic_RNN_model(num_hid_units=300, \n",
    "                                vocab_size = obj_Train.vocab_size+1,\n",
    "                                momentum = 0.9,\n",
    "                                learning_rate=0.1,\n",
    "                                num_layers = None)\n",
    "obj_Train.train_network(graph_dict, num_batches = 10, \n",
    "                        when_valid=2, epochs=1, verbose=True, path_save_model=path_save_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The num of hidden unit is:  300\n",
      "The Vocab size is:  17156\n",
      "The momentum is:  0.9\n",
      "The learning_rate is:  0.1\n",
      "The shape of embed_to_hid_wghts is:  (17156, 300)\n",
      "The shape of embed_to_hid_layer is:  (?, ?, 300)\n",
      "\n",
      "training training training, training training training, training training training training training\n",
      "accuracy of the training batch 0 is:  0.0\n",
      "accuracy of the training batch 1 is:  0.761959876543\n",
      "accuracy of the training batch 2 is:  1.01902173913\n",
      "accuracy of the training batch 3 is:  2.14460784314\n",
      "accuracy of the training batch 4 is:  1.00070224719\n",
      "accuracy of the training batch 5 is:  0.347222222222\n",
      "accuracy of the training batch 6 is:  0.696790540541\n",
      "accuracy of the training batch 7 is:  0.457974137931\n",
      "accuracy of the training batch 8 is:  0.475543478261\n",
      "accuracy of the training batch 9 is:  2.14213709677\n",
      "accuracy of the training batch 10 is:  0.0\n",
      "Average Loss till the training batch 10 is:  14.1746023178\n",
      "\n",
      "crossvalid crossvalid crossvalid, crossvalid crossvalid crossvalid, crossvalid crossvalid crossvalid\n",
      "accuracy of the validation batch 32 is:  1.37746710526\n",
      "accuracy of the validation batch 18 is:  1.42780172414\n",
      "accuracy of the validation batch 44 is:  1.3916015625\n",
      "accuracy of the validation batch 39 is:  1.3427734375\n",
      "accuracy of the validation batch 28 is:  1.2033045977\n",
      "accuracy of the validation batch 12 is:  1.513671875\n",
      "accuracy of the validation batch 16 is:  1.35416666667\n",
      "accuracy of the validation batch 11 is:  1.23487903226\n",
      "accuracy of the validation batch 27 is:  0.380735759494\n",
      "accuracy of the validation batch 39 is:  1.26953125\n",
      "\n",
      "training training training, training training training, training training training training training\n",
      "accuracy of the training batch 11 is:  1.33023648649\n",
      "accuracy of the training batch 12 is:  1.02634803922\n",
      "accuracy of the training batch 13 is:  1.88679245283\n",
      "accuracy of the training batch 14 is:  0.270432692308\n",
      "accuracy of the training batch 15 is:  0.284090909091\n",
      "accuracy of the training batch 16 is:  0.11772260274\n",
      "accuracy of the training batch 17 is:  0.0\n",
      "accuracy of the training batch 18 is:  0.430253623188\n",
      "accuracy of the training batch 19 is:  2.18160377358\n",
      "accuracy of the training batch 20 is:  1.81525735294\n",
      "Average Loss till the training batch 20 is:  14.4125275135\n",
      "\n",
      "crossvalid crossvalid crossvalid, crossvalid crossvalid crossvalid, crossvalid crossvalid crossvalid\n",
      "accuracy of the validation batch 32 is:  1.74753289474\n",
      "accuracy of the validation batch 18 is:  2.18211206897\n",
      "accuracy of the validation batch 44 is:  1.7578125\n",
      "accuracy of the validation batch 39 is:  1.7578125\n",
      "accuracy of the validation batch 28 is:  1.54454022989\n",
      "accuracy of the validation batch 12 is:  1.904296875\n",
      "accuracy of the validation batch 16 is:  1.52083333333\n",
      "accuracy of the validation batch 11 is:  1.71370967742\n",
      "accuracy of the validation batch 27 is:  0.440071202532\n",
      "accuracy of the validation batch 39 is:  1.7578125\n",
      "\n",
      "training training training, training training training, training training training training training\n",
      "accuracy of the training batch 21 is:  1.23161764706\n",
      "accuracy of the training batch 22 is:  2.40162037037\n",
      "accuracy of the training batch 23 is:  1.60361842105\n",
      "accuracy of the training batch 24 is:  2.34375\n",
      "accuracy of the training batch 25 is:  1.93452380952\n",
      "accuracy of the training batch 26 is:  2.07555970149\n",
      "accuracy of the training batch 27 is:  2.05965909091\n",
      "accuracy of the training batch 28 is:  2.42788461538\n",
      "accuracy of the training batch 29 is:  2.55408653846\n",
      "accuracy of the training batch 30 is:  2.29640151515\n",
      "Average Loss till the training batch 30 is:  12.5666729927\n",
      "\n",
      "crossvalid crossvalid crossvalid, crossvalid crossvalid crossvalid, crossvalid crossvalid crossvalid\n",
      "accuracy of the validation batch 32 is:  1.62417763158\n",
      "accuracy of the validation batch 18 is:  2.02047413793\n",
      "accuracy of the validation batch 44 is:  2.34375\n",
      "accuracy of the validation batch 39 is:  1.6357421875\n",
      "accuracy of the validation batch 28 is:  1.5625\n",
      "accuracy of the validation batch 12 is:  1.9775390625\n",
      "accuracy of the validation batch 16 is:  1.66666666667\n",
      "accuracy of the validation batch 11 is:  1.66330645161\n",
      "accuracy of the validation batch 27 is:  0.543908227848\n",
      "accuracy of the validation batch 39 is:  1.6357421875\n",
      "\n",
      "training training training, training training training, training training training training training\n",
      "accuracy of the training batch 31 is:  1.42663043478\n",
      "accuracy of the training batch 32 is:  1.92434210526\n",
      "accuracy of the training batch 33 is:  1.40931372549\n",
      "accuracy of the training batch 34 is:  1.54296875\n",
      "accuracy of the training batch 35 is:  0.452488687783\n",
      "accuracy of the training batch 36 is:  1.75438596491\n",
      "accuracy of the training batch 37 is:  1.97233606557\n",
      "accuracy of the training batch 38 is:  2.09099264706\n",
      "accuracy of the training batch 39 is:  1.95932539683\n",
      "accuracy of the training batch 40 is:  1.30476804124\n",
      "Average Loss till the training batch 40 is:  11.5399411917\n",
      "\n",
      "crossvalid crossvalid crossvalid, crossvalid crossvalid crossvalid, crossvalid crossvalid crossvalid\n",
      "accuracy of the validation batch 32 is:  1.68585526316\n",
      "accuracy of the validation batch 18 is:  2.18211206897\n",
      "accuracy of the validation batch 44 is:  1.7578125\n",
      "accuracy of the validation batch 39 is:  1.7578125\n",
      "accuracy of the validation batch 28 is:  1.54454022989\n",
      "accuracy of the validation batch 12 is:  1.904296875\n",
      "accuracy of the validation batch 16 is:  1.52083333333\n",
      "accuracy of the validation batch 11 is:  1.71370967742\n",
      "accuracy of the validation batch 27 is:  0.440071202532\n",
      "accuracy of the validation batch 39 is:  1.7578125\n",
      "\n",
      "training training training, training training training, training training training training training\n",
      "accuracy of the training batch 41 is:  1.5625\n",
      "accuracy of the training batch 42 is:  2.26004464286\n",
      "accuracy of the training batch 43 is:  1.83423913043\n",
      "accuracy of the training batch 44 is:  1.45716292135\n",
      "accuracy of the training batch 45 is:  1.63352272727\n",
      "accuracy of the training batch 46 is:  1.87088815789\n",
      "accuracy of the training batch 47 is:  1.58390410959\n",
      "accuracy of the training batch 48 is:  1.8798828125\n",
      "accuracy of the training batch 49 is:  2.45150862069\n",
      "All 50 Batches Done.................................\n",
      "\n",
      "\n",
      "Average training loss for Epoch 0 : 10.7309052086\n"
     ]
    }
   ],
   "source": [
    "obj_Train = Train()\n",
    "graph_dict =  dynamic_RNN_model(num_hid_units=300, \n",
    "                                vocab_size = obj_Train.vocab_size+1,\n",
    "                                momentum = 0.9,\n",
    "                                learning_rate=0.5,\n",
    "                                num_layers=3)\n",
    "obj_Train.train_network(graph_dict, num_batches = 50, when_valid=10, epochs=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
