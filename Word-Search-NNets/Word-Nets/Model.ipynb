{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNotes:\\n    1. ENBEDDING(INPUT) LAYER OPERATION\\n       --> embed_to_hid_wghts = tf.get_variable(\\'embedding_matrix\\', [vocab_size, num_hid_units]):\\n           embed_to_hid_layer = tf.nn.embedding_lookup(embed_to_hid_wghts, x)\\n           # Normally we convert the input vector into a one hot matrix and then multiply it to the embedded weights, \\n           When we do so, we get the same embed weight corresponding to 1\\'s in the one-hot vector but in a different \\n           shape. The above operation does all that in a single shot. Basically, embed_to_hid_wghts defines a matrix \\n           with weights going form all vacab to hiddenunits,and embed_to_hid_layer pulls the vectors from embedding_matrix\\n           (embed_to_hid_wghts) corresponding to the idices entries of x for all the batch. \\n           So the matrix embed_to_hid_layer = [batch_size x num_sequences x num_hid_units]\\n\\n    2. HIDDEN LAYER OPERATION\\n       --> The output from dynamic_rnn \"rnn_output\" is a Tensor of shape of [Batch_size x num_sequence x num_hid_units] and,\\n           The hid_to_output_wght is in the shape of [num_hid_units x num_classes]\\n           And We want an output with shape [Batch_size x num_sequence x num_classes]\\n           We horizontlly stack all the batches to form a matrix of [(Batch_size x num_sequence]) x num_classes]\\n       --> In the dynamic_run we provide the \"sequence_length\", this would say the RNN that the batch are padded after\\n           after the given size. Therefore the RNN doesnt consider the padded sequences while calculating the RNN output.\\n           When the actual sequence length is given then the RNN would simply consider the rnn_output as 0 for the padded\\n           sequence\\n\\n    3. OUTPUT LAYER OPERATION\\n       --> sparse_softmax_cross_entropy_with_logits automatically converts the y\\'s into on hot vectors and perform \\n           the softmax operation When using softmax_cross_entropy_with_logits, we have to first convert the y\\'s \\n           into one-hot vector\\n\\n    4. MASK THE LOSES:\\n       --> We can calculate the loss directly as we do for every batch. Normally to calculate the loss we do:\\n           loss_CE = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(hid_to_ouptut_layer, tf.reshape(y, [-1])))\\n           But here we dont do the complete stuff at once because we have zero (0) padded all the sequences in a batch for \\n           equal size . Naively calculating the loss at each time step doesn’t work because that would take into account\\n           the padded positions. So the solution is to create a weight matrix that “masks out” the losses at padded positions.\\n\\n    5. VERY IMPORTANT NOTE:\\n       --> After we receive the dictionary from the gensim dictionary we need to add 1. Because the gensim dictionary \\n           is build from index 0 to n. But our training or testing batch consist of index from 1 to n+1 (achieved in data_builder.py)\\n           . The addition of 1 is imperative because the embedding_matrix internally builds a corpus of (n*num_hidden_unit)\\n           and it builds from 0 so if we dont add 1 then the last word in the dictionarry will have a nan corresponding to its\\n           output and our loss will be nan too. Just to recap, we add 1 because we do zero (0) padding of the sequence length for a batch. \\n        \\n    \\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "    BEFORE RUNNING THIS CODE YOU SHOULD FIRST RUN THE \"DATA_BUILDER.PY\" TO FIRST EXTRACT, CLEAN AND LOAD THE DATA\n",
    "    INTO PICKLE FILES AND THEN THIS CODE WILL PART COME IN HANDY.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Notes:\n",
    "    1. ENBEDDING(INPUT) LAYER OPERATION\n",
    "       --> embed_to_hid_wghts = tf.get_variable('embedding_matrix', [vocab_size, num_hid_units]):\n",
    "           embed_to_hid_layer = tf.nn.embedding_lookup(embed_to_hid_wghts, x)\n",
    "           # Normally we convert the input vector into a one hot matrix and then multiply it to the embedded weights, \n",
    "           When we do so, we get the same embed weight corresponding to 1's in the one-hot vector but in a different \n",
    "           shape. The above operation does all that in a single shot. Basically, embed_to_hid_wghts defines a matrix \n",
    "           with weights going form all vacab to hiddenunits,and embed_to_hid_layer pulls the vectors from embedding_matrix\n",
    "           (embed_to_hid_wghts) corresponding to the idices entries of x for all the batch. \n",
    "           So the matrix embed_to_hid_layer = [batch_size x num_sequences x num_hid_units]\n",
    "\n",
    "    2. HIDDEN LAYER OPERATION\n",
    "       --> The output from dynamic_rnn \"rnn_output\" is a Tensor of shape of [Batch_size x num_sequence x num_hid_units] and,\n",
    "           The hid_to_output_wght is in the shape of [num_hid_units x num_classes]\n",
    "           And We want an output with shape [Batch_size x num_sequence x num_classes]\n",
    "           We horizontlly stack all the batches to form a matrix of [(Batch_size x num_sequence]) x num_classes]\n",
    "       --> In the dynamic_run we provide the \"sequence_length\", this would say the RNN that the batch are padded after\n",
    "           after the given size. Therefore the RNN doesnt consider the padded sequences while calculating the RNN output.\n",
    "           When the actual sequence length is given then the RNN would simply consider the rnn_output as 0 for the padded\n",
    "           sequence\n",
    "\n",
    "    3. OUTPUT LAYER OPERATION\n",
    "       --> sparse_softmax_cross_entropy_with_logits automatically converts the y's into on hot vectors and perform \n",
    "           the softmax operation When using softmax_cross_entropy_with_logits, we have to first convert the y's \n",
    "           into one-hot vector\n",
    "\n",
    "    4. MASK THE LOSES:\n",
    "       --> We can calculate the loss directly as we do for every batch. Normally to calculate the loss we do:\n",
    "           loss_CE = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(hid_to_ouptut_layer, tf.reshape(y, [-1])))\n",
    "           But here we dont do the complete stuff at once because we have zero (0) padded all the sequences in a batch for \n",
    "           equal size . Naively calculating the loss at each time step doesn’t work because that would take into account\n",
    "           the padded positions. So the solution is to create a weight matrix that “masks out” the losses at padded positions.\n",
    "\n",
    "    5. VERY IMPORTANT NOTE:\n",
    "       --> After we receive the dictionary from the gensim dictionary we need to add 1. Because the gensim dictionary \n",
    "           is build from index 0 to n. But our training or testing batch consist of index from 1 to n+1 (achieved in data_builder.py)\n",
    "           . The addition of 1 is imperative because the embedding_matrix internally builds a corpus of (n*num_hidden_unit)\n",
    "           and it builds from 0 so if we dont add 1 then the last word in the dictionarry will have a nan corresponding to its\n",
    "           output and our loss will be nan too. Just to recap, we add 1 because we do zero (0) padding of the sequence length for a batch. \n",
    "        \n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import urllib.request\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.models.rnn.ptb import reader\n",
    "from gensim import corpora\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_graph():  # Reset the graph\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dynamic_RNN_model(\n",
    "    num_hid_units = 3,\n",
    "    vocab_size = 7,\n",
    "    momentum = 0.9,\n",
    "    learning_rate = 0.01,\n",
    "    output_activation_init = 'RELU' \n",
    "    ):\n",
    "    print ('The num of hidden unit is: ', num_hid_units)\n",
    "    print ('The Vocab size is: ', vocab_size)\n",
    "    print ('The momentum is: ', momentum)\n",
    "    print ('The learning_rate is: ', learning_rate)\n",
    "    \n",
    "    \n",
    "    num_classes = vocab_size\n",
    "\n",
    "    reset_graph()\n",
    "    \n",
    "    x = tf.placeholder(tf.int32, shape = [None, None], name='input_placeholder')\n",
    "    y = tf.placeholder(tf.int32, shape = [None, None], name='output_placeholdr')\n",
    "    x_lenarr = tf.placeholder(tf.float32, shape = [None], name='output_placeholdr')\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    \n",
    "\n",
    "    # ENBEDDING(INPUT) LAYER OPERATION\n",
    "    # Creating an Embedding matrix with a random weight for all vacab to hidden_matrix\n",
    "    embed_to_hid_wghts = tf.get_variable('embedding_matrix', [vocab_size, num_hid_units])\n",
    "    embed_to_hid_layer = tf.nn.embedding_lookup(embed_to_hid_wghts, x)\n",
    "    print ('The shape of embed_to_hid_wghts is: ', embed_to_hid_wghts.get_shape())\n",
    "    print ('The shape of embed_to_hid_layer is: ', embed_to_hid_layer.get_shape())\n",
    "\n",
    "\n",
    "    # HIDDEN LAYER OPERATION\n",
    "    rnn_cell = tf.nn.rnn_cell.LSTMCell(num_hid_units, state_is_tuple=True)\n",
    "    init_state = rnn_cell.zero_state(batch_size, tf.float32)  # Each sequence will hava a state that it passes to its next sequence\n",
    "    rnn_outputs, new_state = tf.nn.dynamic_rnn(cell=rnn_cell,\n",
    "                                               sequence_length=x_lenarr,\n",
    "                                               initial_state=init_state,\n",
    "                                               inputs=embed_to_hid_layer,\n",
    "                                               dtype=tf.float32)\n",
    "    \n",
    "\n",
    "    # OUTPUT LAYER OPERATION\n",
    "    # Initialize the weight and biases for the output layer. We use variable scope because we would like to share the weights \n",
    "    with tf.variable_scope('output_layer'):\n",
    "        hid_to_output_wght = tf.get_variable('hid_to_output_wght', [num_hid_units, num_classes],\n",
    "                                            initializer = tf.random_normal_initializer())\n",
    "        output_bias = tf.get_variable('output_bias', [num_classes],\n",
    "                                      initializer=tf.random_normal_initializer())\n",
    "    \n",
    "    rnn_outputs = tf.reshape(rnn_outputs, [-1, num_hid_units])\n",
    "    hid_to_ouptut_layer = tf.matmul(rnn_outputs, hid_to_output_wght) +  output_bias  \n",
    "    # Also use tf.batch_matmul(rnn_outputs, hid_to_output_wght) +  output_bias  \n",
    "    output_state = tf.nn.softmax(hid_to_ouptut_layer, name=None)\n",
    " \n",
    "    \n",
    "    # CALCULATING LOSS\n",
    "    y_reshaped = tf.reshape(y, [-1])\n",
    "    softmax_opt = tf.nn.sparse_softmax_cross_entropy_with_logits(hid_to_ouptut_layer, y_reshaped)\n",
    "    \n",
    "    # MASK THE LOSES\n",
    "    mask = tf.sign(tf.to_float(y_reshaped))\n",
    "    masked_loss = mask * softmax_opt\n",
    " \n",
    "    # Bring back to [B, T] shape\n",
    "    masked_loss = tf.reshape(masked_loss,  tf.shape(y))\n",
    "\n",
    "    # Calculate mean loss\n",
    "    mean_loss_by_example = tf.reduce_sum(masked_loss, reduction_indices=1) / x_lenarr\n",
    "    mean_loss = tf.reduce_mean(mean_loss_by_example)\n",
    "\n",
    "    # The sparse_softmax uses dtype as int32 or int64\n",
    "    \n",
    "    # OPTIMIZING THE LOSS FUNCTION\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(mean_loss)\n",
    "#     optimizer = tf.train.MomentumOptimizer(learning_rate, \n",
    "#                                             momentum, \n",
    "#                                             use_locking=False, \n",
    "#                                             name='Momentum', \n",
    "#                                             use_nesterov=True).minimize(loss_CE)\n",
    "\n",
    "    # Returns graph objects\n",
    "    return dict(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        x_lenarr=x_lenarr,\n",
    "        batch_size = batch_size,\n",
    "        init_state = init_state,\n",
    "        new_state = new_state,\n",
    "        loss = mean_loss,\n",
    "        optimizer = optimizer,\n",
    "        training_prediction = output_state\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Train():\n",
    "    def __init__(self):\n",
    "        self.train_batch_dir = '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Word-Search-NNets/Word-Nets/training_batch/'\n",
    "        dictionary_dir = '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Word-Search-NNets/Word-Nets/dictionary.txt'\n",
    "        self.vocab_size = len(corpora.Dictionary.load_from_text(dictionary_dir))\n",
    "\n",
    "    def accuracy(self, predictions, labels, labels_one_hot = None):\n",
    "        # The input labels are a One-Hot Vector\n",
    "        if labels_one_hot:\n",
    "            return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "                  / predictions.shape[0])\n",
    "        else:\n",
    "            return (100.0 * np.sum(np.argmax(predictions, 1) == np.reshape(labels, [-1]))\n",
    "                  / predictions.shape[0])\n",
    "        \n",
    "\n",
    "    def train_network(self, graph_dict, num_batches, epochs=1, verbose=None ):\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "#             cdoc = np.random.choice(np.arange(20), 5) # Randomly select group of 5 batches from the cross valid dataset to test after every 20 batches\n",
    "            for epoch in np.arange(epochs):\n",
    "                new_state_ = None\n",
    "                training_loss = 0\n",
    "                \n",
    "                for no in np.arange(num_batches):\n",
    "                    with open(self.train_batch_dir+'batch'+str(no)+'.pickle', 'rb') as f:\n",
    "                        dataset = pickle.load(f)\n",
    "                        \n",
    "                        batch_train_dataset = dataset['batch_train_dataset']\n",
    "                        batch_train_labels = dataset['batch_train_labels']\n",
    "                        batch_train_lenarr = dataset['batch_train_lenarr']\n",
    "#                         print (batch_train_lenarr)\n",
    "#                         print ('')\n",
    "#                         print ([len(i[np.where(i!=0)[0]]) for i in batch_train_dataset])\n",
    "                        \n",
    "                        feed_dict= {graph_dict['x']: batch_train_dataset, \n",
    "                                    graph_dict['y']: batch_train_labels,\n",
    "                                    graph_dict['x_lenarr']: batch_train_lenarr}\n",
    "            \n",
    "                        if new_state_ is not None:\n",
    "                            print ('Using the new RNN State')\n",
    "                            feed_dict[graph_dict['init_state']] = new_state_\n",
    "\n",
    "                        bs, new_state_, loss_, opt, tp = sess.run([graph_dict['batch_size'],\n",
    "                                                        graph_dict['new_state'],\n",
    "                                                        graph_dict['loss'],\n",
    "                                                        graph_dict['optimizer'],\n",
    "                                                        graph_dict['training_prediction']], \n",
    "                                                        feed_dict=feed_dict)\n",
    "                        \n",
    "                        training_loss += loss_\n",
    "#                         print (len(tp[0]))\n",
    "#                         print ([ii for ii in tp[0]])\n",
    "                        \n",
    "                        acc = self.accuracy(tp, batch_train_labels)\n",
    "\n",
    "                        print ('accuracy of the batch %d is: '%no, acc)\n",
    "                        print ('')\n",
    "                        print ('Average Loss for the batch %d is: '%no, loss_)\n",
    "                        print ('')\n",
    "                \n",
    "                print ('All %d Batches Done..'%num_batches)\n",
    "                print ('')\n",
    "                \n",
    "                if verbose:\n",
    "                    print(\"Average training loss for Epoch\", epoch, \":\", training_loss/num_batches)\n",
    "                        \n",
    "#                     if (num_batches%20 ==0 and num_batches!=0):\n",
    "#                         print ('Evaluating cross validation dataset ')\n",
    "#                         for cdoc_no in cdoc:\n",
    "#                             with open(self.valid_batch_dir+'batch'+str(cdoc_no)+'.pickle', 'rb') as f1:\n",
    "#                                 dataset = pickle.load(f)\n",
    "                            \n",
    "#                                 batch_valid_dataset = dataset['batch_valid_dataset']\n",
    "#                                 batch_valid_labels = dataset['batch_valid_labels']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The num of hidden unit is:  3\n",
      "The Vocab size is:  17156\n",
      "The momentum is:  0.9\n",
      "The learning_rate is:  0.01\n",
      "The shape of embed_to_hid_wghts is:  (17156, 3)\n",
      "The shape of embed_to_hid_layer is:  (?, ?, 3)\n",
      "accuracy of the batch 0 is:  0.0\n",
      "\n",
      "Average Loss for the batch 0 is:  10.1119\n",
      "\n",
      "Using the new RNN State\n",
      "accuracy of the batch 1 is:  0.0137061403509\n",
      "\n",
      "Average Loss for the batch 1 is:  10.0953\n",
      "\n",
      "Using the new RNN State\n",
      "accuracy of the batch 2 is:  0.0130208333333\n",
      "\n",
      "Average Loss for the batch 2 is:  9.97455\n",
      "\n",
      "Using the new RNN State\n",
      "accuracy of the batch 3 is:  0.0120192307692\n",
      "\n",
      "Average Loss for the batch 3 is:  10.0192\n",
      "\n",
      "Using the new RNN State\n",
      "accuracy of the batch 4 is:  0.0137061403509\n",
      "\n",
      "Average Loss for the batch 4 is:  10.0228\n",
      "\n",
      "Using the new RNN State\n",
      "accuracy of the batch 5 is:  0.0\n",
      "\n",
      "Average Loss for the batch 5 is:  9.95868\n",
      "\n",
      "Using the new RNN State\n",
      "accuracy of the batch 6 is:  0.0\n",
      "\n",
      "Average Loss for the batch 6 is:  9.93488\n",
      "\n",
      "Using the new RNN State\n",
      "accuracy of the batch 7 is:  0.0\n",
      "\n",
      "Average Loss for the batch 7 is:  9.87667\n",
      "\n",
      "Using the new RNN State\n",
      "accuracy of the batch 8 is:  0.0118371212121\n",
      "\n",
      "Average Loss for the batch 8 is:  9.86049\n",
      "\n",
      "Using the new RNN State\n",
      "accuracy of the batch 9 is:  0.0\n",
      "\n",
      "Average Loss for the batch 9 is:  9.91041\n",
      "\n",
      "Using the new RNN State\n",
      "accuracy of the batch 10 is:  0.00877808988764\n",
      "\n",
      "Average Loss for the batch 10 is:  9.84657\n",
      "\n",
      "Using the new RNN State\n",
      "accuracy of the batch 11 is:  0.0102796052632\n",
      "\n",
      "Average Loss for the batch 11 is:  9.88475\n",
      "\n",
      "Using the new RNN State\n",
      "accuracy of the batch 12 is:  0.0\n",
      "\n",
      "Average Loss for the batch 12 is:  9.80685\n",
      "\n",
      "Using the new RNN State\n",
      "accuracy of the batch 13 is:  0.0\n",
      "\n",
      "Average Loss for the batch 13 is:  9.80787\n",
      "\n",
      "Using the new RNN State\n",
      "accuracy of the batch 14 is:  0.0113224637681\n",
      "\n",
      "Average Loss for the batch 14 is:  9.74808\n",
      "\n",
      "Using the new RNN State\n",
      "accuracy of the batch 15 is:  0.0139508928571\n",
      "\n",
      "Average Loss for the batch 15 is:  9.7339\n",
      "\n",
      "Using the new RNN State\n",
      "accuracy of the batch 16 is:  0.00952743902439\n",
      "\n",
      "Average Loss for the batch 16 is:  9.68966\n",
      "\n",
      "Using the new RNN State\n",
      "accuracy of the batch 17 is:  0.0\n",
      "\n",
      "Average Loss for the batch 17 is:  9.67368\n",
      "\n",
      "Using the new RNN State\n",
      "accuracy of the batch 18 is:  0.00730140186916\n",
      "\n",
      "Average Loss for the batch 18 is:  9.63492\n",
      "\n",
      "Using the new RNN State\n",
      "accuracy of the batch 19 is:  0.00600961538462\n",
      "\n",
      "Average Loss for the batch 19 is:  9.62467\n",
      "\n",
      "All 20 Batches Done..\n",
      "\n",
      "Average training loss for Epoch 0 : 9.86079249382\n"
     ]
    }
   ],
   "source": [
    "obj_Train = Train()\n",
    "graph_dict =  dynamic_RNN_model(vocab_size = obj_Train.vocab_size+1,)\n",
    "obj_Train.train_network(graph_dict, num_batches = 20, epochs=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-66-58db6a2f57dc>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-66-58db6a2f57dc>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    0.0\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "accuracy \n",
    " 0.0\n",
    "\n",
    "popopopopopopopoop\n",
    "\n",
    "Using the new RNN State\n",
    "accuracy \n",
    " 55.5921052632\n",
    "\n",
    "popopopopopopopoop\n",
    "\n",
    "Using the new RNN State\n",
    "accuracy \n",
    " 61.8489583333\n",
    "\n",
    "popopopopopopopoop\n",
    "\n",
    "Using the new RNN State\n",
    "accuracy \n",
    " 59.6514423077\n",
    "\n",
    "popopopopopopopoop\n",
    "\n",
    "Using the new RNN State\n",
    "accuracy \n",
    " 54.5915570175\n",
    "\n",
    "popopopopopopopoop\n",
    "\n",
    "Using the new RNN State\n",
    "accuracy \n",
    " 64.4301470588\n",
    "\n",
    "popopopopopopopoop\n",
    "\n",
    "Using the new RNN State\n",
    "accuracy \n",
    " 61.6153492647\n",
    "\n",
    "popopopopopopopoop\n",
    "\n",
    "Using the new RNN State\n",
    "accuracy \n",
    " 81.9647606383\n",
    "\n",
    "popopopopopopopoop\n",
    "\n",
    "Using the new RNN State\n",
    "accuracy \n",
    " 63.6126893939\n",
    "\n",
    "popopopopopopopoop\n",
    "\n",
    "Using the new RNN State\n",
    "accuracy \n",
    " 78.9463141026\n",
    "\n",
    "popopopopopopopoop\n",
    "\n",
    "Using the new RNN State\n",
    "accuracy \n",
    " 69.6716994382\n",
    "\n",
    "popopopopopopopoop\n",
    "\n",
    "Using the new RNN State\n",
    "accuracy \n",
    " 62.4280427632\n",
    "\n",
    "popopopopopopopoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Batch size\n",
    "B = 4\n",
    "# (Maximum) number of time steps in this batch\n",
    "T = 8\n",
    "RNN_DIM = 128\n",
    "NUM_CLASSES = 10\n",
    " \n",
    "# The *acutal* length of the examples\n",
    "example_len = [1, 2, 3, 8]\n",
    " \n",
    "# The classes of the examples at each step (between 1 and 9, 0 means padding)\n",
    "y = np.random.randint(1, 10, [B, T])\n",
    "for i, length in enumerate(example_len):\n",
    "    y[i, length:] = 0  \n",
    "     \n",
    "# The RNN outputs\n",
    "rnn_outputs = tf.convert_to_tensor(np.random.randn(B, T, RNN_DIM), dtype=tf.float32)\n",
    " \n",
    "# Output layer weights\n",
    "W = tf.get_variable(\n",
    "    name=\"W\",\n",
    "    initializer=tf.random_normal_initializer(),\n",
    "    shape=[RNN_DIM, NUM_CLASSES])\n",
    " \n",
    "# Calculate logits and probs\n",
    "# Reshape so we can calculate them all at once\n",
    "rnn_outputs_flat = tf.reshape(rnn_outputs, [-1, RNN_DIM])\n",
    "logits_flat = tf.batch_matmul(rnn_outputs_flat, W)\n",
    "probs_flat = tf.nn.softmax(logits_flat)\n",
    " \n",
    "# Calculate the losses \n",
    "y_flat =  tf.reshape(y, [-1])\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits_flat, y_flat)\n",
    " \n",
    "# Mask the losses\n",
    "mask = tf.sign(tf.to_float(y_flat))\n",
    "masked_losses = mask * losses\n",
    " \n",
    "# Bring back to [B, T] shape\n",
    "masked_losses = tf.reshape(masked_losses,  tf.shape(y))\n",
    " \n",
    "# Calculate mean loss\n",
    "mean_loss_by_example = tf.reduce_sum(masked_losses, reduction_indices=1) / example_len\n",
    "mean_loss = tf.reduce_mean(mean_loss_by_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
