{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    BEFORE RUNNING THIS CODE YOU SHOULD FIRST RUN THE \"DATA_BUILDER.PY\" TO FIRST EXTRACT, CLEAN AND LOAD THE DATA\n",
    "    INTO PICKLE FILES AND THEN THIS CODE WILL PART COME IN HANDY.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import urllib.request\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.models.rnn.ptb import reader\n",
    "from gensim import corpora\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_graph():  # Reset the graph\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dynamic_RNN_model(\n",
    "    num_hid_units = 3,\n",
    "    vocab_size = 7,\n",
    "    momentum = 0.9,\n",
    "    learning_rate = 0.01\n",
    "    ):\n",
    "    print ('The num of hidden unit is: ', num_hid_units)\n",
    "    print ('The Vocab size is: ', vocab_size)\n",
    "    print ('The momentum is: ', momentum)\n",
    "    print ('The learning_rate is: ', learning_rate)\n",
    "    \n",
    "    \n",
    "    num_classes = vocab_size\n",
    "\n",
    "    reset_graph()\n",
    "    \n",
    "    x = tf.placeholder(tf.int32, shape = [None, None], name='input_placeholder')\n",
    "    y = tf.placeholder(tf.int32, shape = [None, None], name='output_placeholdr')\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    \n",
    "\n",
    "    # ENBEDDING(INPUT) LAYER OPERATION\n",
    "    # Creating an Embedding matrix with a random weight for all vacab to hidden_matrix\n",
    "    embed_to_hid_wghts = tf.get_variable('embedding_matrix', [vocab_size, num_hid_units])\n",
    "    embed_to_hid_layer = tf.nn.embedding_lookup(embed_to_hid_wghts, x)\n",
    "    print ('The shape of embed_to_hid_wghts is: ', embed_to_hid_wghts.get_shape())\n",
    "    print ('The shape of embed_to_hid_layer is: ', embed_to_hid_layer.get_shape())\n",
    "\n",
    "\n",
    "    # HIDDEN LAYER OPERATION\n",
    "    rnn_cell = tf.nn.rnn_cell.LSTMCell(num_hid_units, state_is_tuple=True)\n",
    "    init_state = rnn_cell.zero_state(batch_size, tf.float32)  # Each sequence will hava a state that it passes to its next sequence\n",
    "    rnn_outputs, new_state = tf.nn.dynamic_rnn(cell=rnn_cell,\n",
    "                                            # sequence_length=X_lengths,\n",
    "                                            initial_state=init_state,\n",
    "                                            inputs=embed_to_hid_layer)\n",
    "    \n",
    "\n",
    "    # OUTPUT LAYER OPERATION\n",
    "    # Initialize the weight and biases for the output layer. We use variable scope because we would like to share the weights \n",
    "    with tf.variable_scope('output_layer'):\n",
    "        hid_to_output_wght = tf.get_variable('hid_to_output_wght', [num_hid_units, num_classes])\n",
    "        output_bias = tf.get_variable('output_bias',\n",
    "                                      [num_classes],\n",
    "                                      initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "    rnn_outputs = tf.reshape(rnn_outputs, [-1, num_hid_units])  \n",
    "    hid_to_ouptut_layer = tf.matmul(rnn_outputs, hid_to_output_wght) +  output_bias\n",
    "    output_state = tf.nn.softmax(hid_to_ouptut_layer, name=None)\n",
    " \n",
    "    \n",
    "    \n",
    "    # CALCULATING LOSS, OPTIMIZING THE COST FUNCTION, MEASURING ACCURACY\n",
    "    y_reshaped = tf.reshape(y, [-1])\n",
    "    loss_CE = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(hid_to_ouptut_layer, y_reshaped))\n",
    "    \n",
    "    # The sparse_softmax uses dtype as int32 or int64\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, \n",
    "                                            momentum, \n",
    "                                            use_locking=False, \n",
    "                                            name='Momentum', \n",
    "                                            use_nesterov=True).minimize(loss_CE)\n",
    "    # y_ = tf.reshape(y, [-1])\n",
    "    # correct_prediction = tf.equal(tf.arg_max(output_state,1), tf.arg_max(y_ ,1))\n",
    "    # accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "    \n",
    "\n",
    "    # Returns graph objects\n",
    "    return dict(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        batch_size = batch_size,\n",
    "        init_state = init_state,\n",
    "        new_state = new_state,\n",
    "        loss_CE = loss_CE,\n",
    "        optimizer = optimizer,\n",
    "        training_prediction = output_state\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Train():\n",
    "    def __init__(self):\n",
    "        self.train_batch_dir = '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Word-Search-NNets/Word-Nets/training_batch/'\n",
    "        dictionary_dir = '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Word-Search-NNets/Word-Nets/dictionary.txt'\n",
    "        self.vocab_size = len(corpora.Dictionary.load_from_text(dictionary_dir))\n",
    "\n",
    "    def accuracy(self, predictions, labels, labels_one_hot = None):\n",
    "        # The input labels are a One-Hot Vector\n",
    "        if labels_one_hot:\n",
    "            return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "                  / predictions.shape[0])\n",
    "        else:\n",
    "            return (100.0 * np.sum(np.argmax(predictions, 1) == np.reshape(labels, [-1]))\n",
    "                  / predictions.shape[0])\n",
    "        \n",
    "\n",
    "    def train_network(self, graph_dict, num_batches, epochs=1, verbose=None ):\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "            \n",
    "            for epoch in np.arange(epochs):\n",
    "                new_hid_layer_state = None\n",
    "                training_loss = 0\n",
    "                \n",
    "                for no in np.arange(num_batches):\n",
    "                    with open(self.train_batch_dir+'batch'+str(no)+'.pickle', 'rb') as f:\n",
    "                        dataset = pickle.load(f)\n",
    "                        \n",
    "                        batch_train_dataset = dataset['batch_train_dataset']\n",
    "                        batch_train_labels = dataset['batch_train_labels']\n",
    "                        \n",
    "                        if not new_hid_layer_state:\n",
    "                            print ('Using the zero init RNN State')\n",
    "                            feed_dict= {graph_dict['x']: batch_train_dataset, \n",
    "                                        graph_dict['y']: batch_train_labels}\n",
    "                        else:\n",
    "                            print ('Using the new RNN State')\n",
    "                            feed_dict= {graph_dict['x']: batch_train_dataset, \n",
    "                                        graph_dict['y']: batch_train_labels,\n",
    "                                        graph_dict['init_state'] : new_hid_layer_state}\n",
    "\n",
    "                        bs, nwst, loss, opt, tp = sess.run([graph_dict['batch_size'],\n",
    "                                                        graph_dict['new_state'],\n",
    "                                                        graph_dict['loss_CE'],\n",
    "                                                        graph_dict['optimizer'],\n",
    "                                                        graph_dict['training_prediction']], \n",
    "                                                        feed_dict=feed_dict)\n",
    "                        new_hid_layer_state = nwst\n",
    "                        training_loss += loss\n",
    "                        acc = self.accuracy(tp, batch_train_labels)\n",
    "\n",
    "                        print ('accuracy of the batch %d is: '%no, acc)\n",
    "                        print ('')\n",
    "                        print ('Average Loss for the batch %d is: '%no, loss)\n",
    "                        print ('')\n",
    "                \n",
    "                print ('All %d Batches Done..'%num_batches)\n",
    "                print ('')\n",
    "                \n",
    "                if verbose:\n",
    "                    print(\"Average training loss for Epoch\", epoch, \":\", training_loss/num_batches)\n",
    "                        \n",
    "#                     if (num_batches%20 ==0 and num_batches!=0):\n",
    "#                         print ('Evaluating cross validation dataset ')\n",
    "#                         for cdoc_no in cdoc:\n",
    "#                             with open(self.valid_batch_dir+'batch'+str(cdoc_no)+'.pickle', 'rb') as f1:\n",
    "#                                 dataset = pickle.load(f)\n",
    "                            \n",
    "#                                 batch_valid_dataset = dataset['batch_valid_dataset']\n",
    "#                                 batch_valid_labels = dataset['batch_valid_labels']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The num of hidden unit is:  100\n",
      "The Vocab size is:  17155\n",
      "The batch_size is:  128\n",
      "The learning_rate is:  0.0001\n",
      "[[17153  2636 16270 ...,     0     0     0]\n",
      " [17153  1044 10072 ...,     0     0     0]\n",
      " [17153 14404  2686 ...,     0     0     0]\n",
      " ..., \n",
      " [17153 11839  1932 ...,     0     0     0]\n",
      " [17153 13398 14528 ...,     0     0     0]\n",
      " [17153  2636 11642 ...,     0     0     0]]\n",
      "All 5 Batches Done..\n",
      "\n",
      "Average training loss for Epoch 0 : 0.0\n"
     ]
    }
   ],
   "source": [
    "obj_Train = Train()\n",
    "graph_dict =  dynamic_RNN_model(num_classes = obj_Train.vocab_size,)\n",
    "obj_Train.train_network(graph_dict, num_batches = 5, epochs=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy \n",
    " 0.0\n",
    "\n",
    "popopopopopopopoop\n",
    "\n",
    "Using the new RNN State\n",
    "accuracy \n",
    " 55.5921052632\n",
    "\n",
    "popopopopopopopoop\n",
    "\n",
    "Using the new RNN State\n",
    "accuracy \n",
    " 61.8489583333\n",
    "\n",
    "popopopopopopopoop\n",
    "\n",
    "Using the new RNN State\n",
    "accuracy \n",
    " 59.6514423077\n",
    "\n",
    "popopopopopopopoop\n",
    "\n",
    "Using the new RNN State\n",
    "accuracy \n",
    " 54.5915570175\n",
    "\n",
    "popopopopopopopoop\n",
    "\n",
    "Using the new RNN State\n",
    "accuracy \n",
    " 64.4301470588\n",
    "\n",
    "popopopopopopopoop\n",
    "\n",
    "Using the new RNN State\n",
    "accuracy \n",
    " 61.6153492647\n",
    "\n",
    "popopopopopopopoop\n",
    "\n",
    "Using the new RNN State\n",
    "accuracy \n",
    " 81.9647606383\n",
    "\n",
    "popopopopopopopoop\n",
    "\n",
    "Using the new RNN State\n",
    "accuracy \n",
    " 63.6126893939\n",
    "\n",
    "popopopopopopopoop\n",
    "\n",
    "Using the new RNN State\n",
    "accuracy \n",
    " 78.9463141026\n",
    "\n",
    "popopopopopopopoop\n",
    "\n",
    "Using the new RNN State\n",
    "accuracy \n",
    " 69.6716994382\n",
    "\n",
    "popopopopopopopoop\n",
    "\n",
    "Using the new RNN State\n",
    "accuracy \n",
    " 62.4280427632\n",
    "\n",
    "popopopopopopopoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
