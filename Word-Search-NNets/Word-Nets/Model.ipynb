{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "    BEFORE RUNNING THIS CODE YOU SHOULD FIRST RUN THE \"DATA_BUILDER.PY\" TO FIRST EXTRACT, CLEAN AND LOAD THE DATA\n",
    "    INTO PICKLE FILES AND THEN THIS CODE WILL PART COME IN HANDY.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Notes:\n",
    "    1. ENBEDDING(INPUT) LAYER OPERATION\n",
    "       --> embed_to_hid_wghts = tf.get_variable('embedding_matrix', [vocab_size, num_hid_units]):\n",
    "           embed_to_hid_layer = tf.nn.embedding_lookup(embed_to_hid_wghts, x)\n",
    "           # Normally we convert the input vector into a one hot matrix and then multiply it to the embedded weights, \n",
    "           When we do so, we get the same embed weight corresponding to 1's in the one-hot vector but in a different \n",
    "           shape. The above operation does all that in a single shot. Basically, embed_to_hid_wghts defines a matrix \n",
    "           with weights going form all vacab to hiddenunits,and embed_to_hid_layer pulls the vectors from embedding_matrix\n",
    "           (embed_to_hid_wghts) corresponding to the idices entries of x for all the batch. \n",
    "           So the matrix embed_to_hid_layer = [batch_size x num_sequences x num_hid_units]\n",
    "\n",
    "    2. HIDDEN LAYER OPERATION\n",
    "       --> The output from dynamic_rnn \"rnn_output\" is a Tensor of shape of [Batch_size x num_sequence x num_hid_units] and,\n",
    "           The hid_to_output_wght is in the shape of [num_hid_units x num_classes]\n",
    "           And We want an output with shape [Batch_size x num_sequence x num_classes]\n",
    "           We horizontlly stack all the batches to form a matrix of [(Batch_size x num_sequence]) x num_classes]\n",
    "       --> In the dynamic_run we provide the \"sequence_length\", this would say the RNN that the batch are padded after\n",
    "           after the given size. Therefore the RNN doesnt consider the padded sequences while calculating the RNN output.\n",
    "           When the actual sequence length is given then the RNN would simply consider the rnn_output as 0 for the padded\n",
    "           sequence\n",
    "\n",
    "    3. OUTPUT LAYER OPERATION\n",
    "       --> sparse_softmax_cross_entropy_with_logits automatically converts the y's into on hot vectors and perform \n",
    "           the softmax operation When using softmax_cross_entropy_with_logits, we have to first convert the y's \n",
    "           into one-hot vector\n",
    "\n",
    "    4. MASK THE LOSES:\n",
    "       --> We can calculate the loss directly as we do for every batch. Normally to calculate the loss we do:\n",
    "           loss_CE = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(hid_to_ouptut_layer, tf.reshape(y, [-1])))\n",
    "           But here we dont do the complete stuff at once because we have zero (0) padded all the sequences in a batch for \n",
    "           equal size . Naively calculating the loss at each time step doesn’t work because that would take into account\n",
    "           the padded positions. So the solution is to create a weight matrix that “masks out” the losses at padded positions.\n",
    "       --> Intuition for why do we do mask. \"softmax_opt\" will give a array of size [(num_sequence*batch_size) * 0],\n",
    "           and reduced_mean just sums up all the element of the softmax and divides it with the array size.\n",
    "           And we know that many of the sequences here are padded with 0's whose softmax output should not be considered\n",
    "           while performing the reduced_mean. Hence masking converts the softmax of those padded 0's to 0 and calculate\n",
    "           the mean of all training example in a batch separetely. And finally computes the reduced mean. \n",
    "       \n",
    "       \n",
    "    5. VERY IMPORTANT NOTE:\n",
    "       --> After we receive the dictionary from the gensim dictionary we need to add 1. Because the gensim dictionary \n",
    "           is build from index 0 to n. But our training or testing batch consist of index from 1 to n+1 (achieved in data_builder.py)\n",
    "           . The addition of 1 is imperative because the embedding_matrix internally builds a corpus of (n*num_hidden_unit)\n",
    "           and it builds from 0 so if we dont add 1 then the last word in the dictionarry will have a nan corresponding to its\n",
    "           output and our loss will be nan too. Just to recap, we add 1 because we do zero (0) padding of the sequence length for a batch. \n",
    "        \n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import urllib.request\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.models.rnn.ptb import reader\n",
    "from gensim import corpora\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_graph():  # Reset the graph\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dynamic_RNN_model(\n",
    "    num_hid_units = 3,\n",
    "    vocab_size = 7,\n",
    "    momentum = 0.9,\n",
    "    learning_rate = 0.1,\n",
    "    output_activation_init = 'RELU' \n",
    "    ):\n",
    "    print ('The num of hidden unit is: ', num_hid_units)\n",
    "    print ('The Vocab size is: ', vocab_size)\n",
    "    print ('The momentum is: ', momentum)\n",
    "    print ('The learning_rate is: ', learning_rate)\n",
    "    \n",
    "    \n",
    "    num_classes = vocab_size\n",
    "\n",
    "    reset_graph()\n",
    "    \n",
    "    x = tf.placeholder(tf.int32, shape = [None, None], name='input_placeholder')\n",
    "    y = tf.placeholder(tf.int32, shape = [None, None], name='output_placeholdr')\n",
    "    x_lenarr = tf.placeholder(tf.float32, shape = [None], name='output_placeholdr')\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    \n",
    "\n",
    "    # ENBEDDING(INPUT) LAYER OPERATION\n",
    "    # Creating an Embedding matrix with a random weight for all vacab to hidden_matrix\n",
    "    embed_to_hid_wghts = tf.get_variable('embedding_matrix', [vocab_size, num_hid_units])\n",
    "    embed_to_hid_layer = tf.nn.embedding_lookup(embed_to_hid_wghts, x)\n",
    "    print ('The shape of embed_to_hid_wghts is: ', embed_to_hid_wghts.get_shape())\n",
    "    print ('The shape of embed_to_hid_layer is: ', embed_to_hid_layer.get_shape())\n",
    "\n",
    "\n",
    "    # HIDDEN LAYER OPERATION\n",
    "    rnn_cell = tf.nn.rnn_cell.LSTMCell(num_hid_units, state_is_tuple=True)\n",
    "    init_state = rnn_cell.zero_state(batch_size, tf.float32)  # Each sequence will hava a state that it passes to its next sequence\n",
    "    rnn_outputs, new_state = tf.nn.dynamic_rnn(cell=rnn_cell,\n",
    "                                               sequence_length=x_lenarr,\n",
    "                                               initial_state=init_state,\n",
    "                                               inputs=embed_to_hid_layer,\n",
    "                                               dtype=tf.float32)\n",
    "    \n",
    "\n",
    "    # OUTPUT LAYER OPERATION\n",
    "    # Initialize the weight and biases for the output layer. We use variable scope because we would like to share the weights \n",
    "    with tf.variable_scope('output_layer'):\n",
    "        hid_to_output_wght = tf.get_variable('hid_to_output_wght', [num_hid_units, num_classes],\n",
    "                                            initializer = tf.random_normal_initializer())\n",
    "        output_bias = tf.get_variable('output_bias', [num_classes],\n",
    "                                      initializer=tf.random_normal_initializer())\n",
    "    \n",
    "    rnn_outputs = tf.reshape(rnn_outputs, [-1, num_hid_units])\n",
    "    hid_to_ouptut_layer = tf.matmul(rnn_outputs, hid_to_output_wght) +  output_bias  \n",
    "    # Also use tf.batch_matmul(rnn_outputs, hid_to_output_wght) +  output_bias  \n",
    "    output_state = tf.nn.softmax(hid_to_ouptut_layer, name=None)\n",
    " \n",
    "    \n",
    "    # SOFTMAX OUTPUT\n",
    "    y_reshaped = tf.reshape(y, [-1])\n",
    "    softmax_opt = tf.nn.sparse_softmax_cross_entropy_with_logits(hid_to_ouptut_layer, y_reshaped)\n",
    "    \n",
    "    # MASK THE LOSES\n",
    "    mask = tf.sign(tf.to_float(y_reshaped))\n",
    "    masked_loss = mask * softmax_opt\n",
    "    masked_loss = tf.reshape(masked_loss,  tf.shape(y))\n",
    "    mean_loss_by_example = tf.reduce_sum(masked_loss, reduction_indices=1) / x_lenarr\n",
    "    mean_loss = tf.reduce_mean(mean_loss_by_example)\n",
    "\n",
    "    \n",
    "    # OPTIMIZING THE LOSS FUNCTION\n",
    "#     optimizer = tf.train.AdamOptimizer(learning_rate).minimize(mean_loss)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, \n",
    "                                            momentum, \n",
    "                                            use_locking=False, \n",
    "                                            name='Momentum', \n",
    "                                            use_nesterov=True).minimize(mean_loss)\n",
    "\n",
    "    # Returns graph objects\n",
    "    return dict(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        x_lenarr=x_lenarr,\n",
    "        batch_size = batch_size,\n",
    "        init_state = init_state,\n",
    "        new_state = new_state,\n",
    "        loss = mean_loss,\n",
    "        optimizer = optimizer,\n",
    "        prediction = output_state\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Train():\n",
    "    def __init__(self, valid_num_batches=10):\n",
    "        self.train_batch_dir = '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Word-Search-NNets/Word-Nets/training_batch/'\n",
    "        self.valid_batch_dir = '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Word-Search-NNets/Word-Nets/crossvalid_batch/'\n",
    "        dictionary_dir = '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Word-Search-NNets/Word-Nets/dictionary.txt'\n",
    "        self.vocab_size = len(corpora.Dictionary.load_from_text(dictionary_dir))\n",
    "        \n",
    "        # Randomly select group of 5 batches from the cross valid dataset to test after every 20 batches\n",
    "        self.cdoc = np.random.choice(np.arange(50), valid_num_batches) \n",
    "\n",
    "    def accuracy(self, predictions, labels, labels_one_hot = None):\n",
    "        # The input labels are a One-Hot Vector\n",
    "        if labels_one_hot:\n",
    "            return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "                  / predictions.shape[0])\n",
    "        else:\n",
    "            return (100.0 * np.sum(np.argmax(predictions, 1) == np.reshape(labels, [-1]))\n",
    "                  / predictions.shape[0])\n",
    "        \n",
    "    def cross_valid(self, sess, graph_dict):\n",
    "        new_valid_state_ = None\n",
    "        for cdoc_no in self.cdoc:\n",
    "            with open(self.valid_batch_dir+'batch'+str(cdoc_no)+'.pickle', 'rb') as f1:\n",
    "                dataset = pickle.load(f1)\n",
    "\n",
    "                batch_valid_dataset = dataset['batch_valid_dataset']\n",
    "                batch_valid_labels = dataset['batch_valid_labels']\n",
    "                batch_valid_lenarr = dataset['batch_valid_lenarr']\n",
    "\n",
    "                if new_valid_state_ is not None:\n",
    "                    feed_dict={graph_dict['x']: batch_valid_dataset,\n",
    "                               graph_dict['x_lenarr']: batch_valid_lenarr,\n",
    "                               graph_dict['init_state']: new_valid_state_}\n",
    "                else:\n",
    "                    feed_dict={graph_dict['x']: batch_valid_dataset,\n",
    "                               graph_dict['x_lenarr']: batch_valid_lenarr}\n",
    "\n",
    "                valid_prediction_, new_valid_state_ = sess.run([graph_dict['prediction'],\n",
    "                                                                graph_dict['new_state']], \n",
    "                                                                feed_dict)\n",
    "\n",
    "            acc_valid = self.accuracy(valid_prediction_, batch_valid_labels)\n",
    "            print ('accuracy of the validation batch %d is: '%cdoc_no, acc_valid)\n",
    "                        \n",
    "                        \n",
    "    def train_network(self, graph_dict, num_batches, epochs=1, verbose=None ):\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "            for epoch in np.arange(epochs):\n",
    "                new_state_ = None\n",
    "                training_loss = 0\n",
    "                print ('')\n",
    "                print ('training training training, training training training, training training training training training')\n",
    "                for no in np.arange(num_batches):\n",
    "                    with open(self.train_batch_dir+'batch'+str(no)+'.pickle', 'rb') as f:\n",
    "                        dataset = pickle.load(f)\n",
    "                        \n",
    "                        batch_train_dataset = dataset['batch_train_dataset']\n",
    "                        batch_train_labels = dataset['batch_train_labels']\n",
    "                        batch_train_lenarr = dataset['batch_train_lenarr']\n",
    "                        \n",
    "                        feed_dict= {graph_dict['x']: batch_train_dataset, \n",
    "                                    graph_dict['y']: batch_train_labels,\n",
    "                                    graph_dict['x_lenarr']: batch_train_lenarr}\n",
    "            \n",
    "                        if new_state_ is not None:\n",
    "                            feed_dict[graph_dict['init_state']] = new_state_\n",
    "\n",
    "                        new_state_, loss_, opt, tp = sess.run([graph_dict['new_state'],\n",
    "                                                            graph_dict['loss'],\n",
    "                                                            graph_dict['optimizer'],\n",
    "                                                            graph_dict['prediction']], \n",
    "                                                            feed_dict=feed_dict)\n",
    "                        \n",
    "                        training_loss += loss_\n",
    "                        acc = self.accuracy(tp, batch_train_labels)\n",
    "                    \n",
    "                        \n",
    "                        print ('accuracy of the training batch %d is: '%no, acc)\n",
    "                        \n",
    "                        \n",
    "                        if (no%50==0 and no!=0):\n",
    "                            print ('Average Loss till the training batch %d is: '%no, training_loss/no)\n",
    "                            print ('')\n",
    "                            print ('crossvalid crossvalid crossvalid, crossvalid crossvalid crossvalid, crossvalid crossvalid crossvalid')\n",
    "                            self.cross_valid(sess,graph_dict)\n",
    "                            print ('')\n",
    "                            print ('training training training, training training training, training training training training training')\n",
    "                            \n",
    "                print ('All %d Batches Done.................................'%num_batches)\n",
    "                print ('')\n",
    "                print ('')\n",
    "                \n",
    "                if verbose:\n",
    "                    print(\"Average training loss for Epoch\", epoch, \":\", training_loss/num_batches)\n",
    "                        \n",
    "#                     \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The num of hidden unit is:  300\n",
      "The Vocab size is:  17156\n",
      "The momentum is:  0.9\n",
      "The learning_rate is:  0.1\n",
      "The shape of embed_to_hid_wghts is:  (17156, 300)\n",
      "The shape of embed_to_hid_layer is:  (?, ?, 300)\n",
      "\n",
      "training training training, training training training, training training training training training\n",
      "accuracy of the training batch 0 is:  0.0\n",
      "accuracy of the training batch 1 is:  0.906635802469\n",
      "accuracy of the training batch 2 is:  1.63043478261\n",
      "accuracy of the training batch 3 is:  0.582107843137\n",
      "accuracy of the training batch 4 is:  0.0\n",
      "accuracy of the training batch 5 is:  1.85185185185\n",
      "accuracy of the training batch 6 is:  2.68158783784\n",
      "accuracy of the training batch 7 is:  1.69719827586\n",
      "accuracy of the training batch 8 is:  1.47192028986\n",
      "accuracy of the training batch 9 is:  1.86491935484\n",
      "accuracy of the training batch 10 is:  2.25290697674\n",
      "accuracy of the training batch 11 is:  1.39358108108\n",
      "accuracy of the training batch 12 is:  0.842524509804\n",
      "accuracy of the training batch 13 is:  3.15448113208\n",
      "accuracy of the training batch 14 is:  3.24519230769\n",
      "accuracy of the training batch 15 is:  2.41477272727\n",
      "accuracy of the training batch 16 is:  1.64811643836\n",
      "accuracy of the training batch 17 is:  3.45394736842\n",
      "accuracy of the training batch 18 is:  3.28351449275\n",
      "accuracy of the training batch 19 is:  4.18632075472\n",
      "accuracy of the training batch 20 is:  3.26286764706\n",
      "accuracy of the training batch 21 is:  2.40808823529\n",
      "accuracy of the training batch 22 is:  4.6875\n",
      "accuracy of the training batch 23 is:  3.88569078947\n",
      "accuracy of the training batch 24 is:  4.43638392857\n",
      "accuracy of the training batch 25 is:  5.28273809524\n",
      "accuracy of the training batch 26 is:  4.31436567164\n",
      "accuracy of the training batch 27 is:  3.50378787879\n",
      "accuracy of the training batch 28 is:  4.13461538462\n",
      "accuracy of the training batch 29 is:  5.19831730769\n",
      "accuracy of the training batch 30 is:  4.33238636364\n",
      "accuracy of the training batch 31 is:  3.17028985507\n",
      "accuracy of the training batch 32 is:  3.30592105263\n",
      "accuracy of the training batch 33 is:  2.87990196078\n",
      "accuracy of the training batch 34 is:  3.61328125\n",
      "accuracy of the training batch 35 is:  1.83116515837\n",
      "accuracy of the training batch 36 is:  5.09868421053\n",
      "accuracy of the training batch 37 is:  4.55942622951\n",
      "accuracy of the training batch 38 is:  4.13602941176\n",
      "accuracy of the training batch 39 is:  3.94345238095\n",
      "accuracy of the training batch 40 is:  3.10889175258\n",
      "accuracy of the training batch 41 is:  3.96634615385\n",
      "accuracy of the training batch 42 is:  5.55245535714\n",
      "accuracy of the training batch 43 is:  4.25724637681\n",
      "accuracy of the training batch 44 is:  3.2654494382\n",
      "accuracy of the training batch 45 is:  3.94176136364\n",
      "accuracy of the training batch 46 is:  4.37911184211\n",
      "accuracy of the training batch 47 is:  4.34503424658\n",
      "accuracy of the training batch 48 is:  5.2001953125\n",
      "accuracy of the training batch 49 is:  5.49568965517\n",
      "accuracy of the training batch 50 is:  4.18836805556\n",
      "Average Loss till the training batch 50 is:  8.76393228531\n",
      "\n",
      "crossvalid crossvalid crossvalid, crossvalid crossvalid crossvalid, crossvalid crossvalid crossvalid\n",
      "accuracy of the validation batch 44 is:  4.8828125\n",
      "accuracy of the validation batch 49 is:  5.44181034483\n",
      "accuracy of the validation batch 20 is:  3.70639534884\n",
      "accuracy of the validation batch 9 is:  3.60704787234\n",
      "accuracy of the validation batch 15 is:  4.45015822785\n",
      "accuracy of the validation batch 21 is:  6.34375\n",
      "accuracy of the validation batch 11 is:  4.63709677419\n",
      "accuracy of the validation batch 41 is:  5.09159482759\n",
      "accuracy of the validation batch 1 is:  5.72916666667\n",
      "accuracy of the validation batch 17 is:  4.70950704225\n",
      "\n",
      "training training training, training training training, training training training training training\n",
      "accuracy of the training batch 51 is:  5.74882075472\n",
      "accuracy of the training batch 52 is:  4.16666666667\n",
      "accuracy of the training batch 53 is:  5.06552419355\n",
      "accuracy of the training batch 54 is:  4.01785714286\n",
      "accuracy of the training batch 55 is:  3.82179054054\n",
      "accuracy of the training batch 56 is:  3.98897058824\n",
      "accuracy of the training batch 57 is:  4.75961538462\n",
      "accuracy of the training batch 58 is:  3.76090116279\n",
      "accuracy of the training batch 59 is:  3.02278037383\n",
      "accuracy of the training batch 60 is:  4.83940972222\n",
      "accuracy of the training batch 61 is:  4.46428571429\n",
      "accuracy of the training batch 62 is:  4.23259493671\n",
      "accuracy of the training batch 63 is:  6.21995192308\n",
      "accuracy of the training batch 64 is:  4.91431451613\n",
      "accuracy of the training batch 65 is:  4.62890625\n",
      "accuracy of the training batch 66 is:  5.91947115385\n",
      "accuracy of the training batch 67 is:  2.95138888889\n",
      "accuracy of the training batch 68 is:  6.49671052632\n",
      "accuracy of the training batch 69 is:  3.5426980198\n",
      "accuracy of the training batch 70 is:  3.14094387755\n",
      "accuracy of the training batch 71 is:  3.8300304878\n",
      "accuracy of the training batch 72 is:  6.48148148148\n",
      "accuracy of the training batch 73 is:  5.45634920635\n",
      "accuracy of the training batch 74 is:  5.43269230769\n",
      "accuracy of the training batch 75 is:  4.31338028169\n",
      "accuracy of the training batch 76 is:  3.69318181818\n",
      "accuracy of the training batch 77 is:  4.52302631579\n",
      "accuracy of the training batch 78 is:  6.1875\n",
      "accuracy of the training batch 79 is:  4.1266025641\n",
      "accuracy of the training batch 80 is:  4.51600609756\n",
      "accuracy of the training batch 81 is:  5.04981884058\n",
      "accuracy of the training batch 82 is:  5.44270833333\n",
      "accuracy of the training batch 83 is:  5.078125\n",
      "accuracy of the training batch 84 is:  3.50654069767\n",
      "accuracy of the training batch 85 is:  6.00754310345\n",
      "accuracy of the training batch 86 is:  4.75171232877\n",
      "accuracy of the training batch 87 is:  3.125\n",
      "accuracy of the training batch 88 is:  4.77807971014\n",
      "accuracy of the training batch 89 is:  6.50201612903\n",
      "accuracy of the training batch 90 is:  5.82386363636\n",
      "accuracy of the training batch 91 is:  5.38461538462\n",
      "accuracy of the training batch 92 is:  0.812718914186\n",
      "accuracy of the training batch 93 is:  4.88911290323\n",
      "accuracy of the training batch 94 is:  7.28058510638\n",
      "accuracy of the training batch 95 is:  4.20386904762\n",
      "accuracy of the training batch 96 is:  3.125\n",
      "accuracy of the training batch 97 is:  5.02403846154\n",
      "accuracy of the training batch 98 is:  5.20833333333\n",
      "accuracy of the training batch 99 is:  2.48724489796\n",
      "accuracy of the training batch 100 is:  5.04981884058\n",
      "Average Loss till the training batch 100 is:  8.18598852634\n",
      "\n",
      "crossvalid crossvalid crossvalid, crossvalid crossvalid crossvalid, crossvalid crossvalid crossvalid\n",
      "accuracy of the validation batch 44 is:  5.7861328125\n",
      "accuracy of the validation batch 49 is:  5.89978448276\n",
      "accuracy of the validation batch 20 is:  3.99709302326\n",
      "accuracy of the validation batch 9 is:  3.88962765957\n",
      "accuracy of the validation batch 15 is:  4.50949367089\n",
      "accuracy of the validation batch 21 is:  6.65625\n",
      "accuracy of the validation batch 11 is:  5.14112903226\n",
      "accuracy of the validation batch 41 is:  5.68426724138\n",
      "accuracy of the validation batch 1 is:  6.14035087719\n",
      "accuracy of the validation batch 17 is:  5.50176056338\n",
      "\n",
      "training training training, training training training, training training training training training\n",
      "accuracy of the training batch 101 is:  4.39814814815\n",
      "accuracy of the training batch 102 is:  4.47115384615\n",
      "accuracy of the training batch 103 is:  6.14035087719\n",
      "accuracy of the training batch 104 is:  4.8046875\n",
      "accuracy of the training batch 105 is:  5.52455357143\n",
      "accuracy of the training batch 106 is:  7.04787234043\n",
      "accuracy of the training batch 107 is:  4.66720779221\n",
      "accuracy of the training batch 108 is:  3.45873786408\n",
      "accuracy of the training batch 109 is:  4.77430555556\n",
      "accuracy of the training batch 110 is:  3.15656565657\n",
      "accuracy of the training batch 111 is:  5.94846491228\n",
      "accuracy of the training batch 112 is:  2.92440878378\n",
      "accuracy of the training batch 113 is:  4.14870689655\n",
      "accuracy of the training batch 114 is:  5.15873015873\n",
      "accuracy of the training batch 115 is:  6.11049107143\n",
      "accuracy of the training batch 116 is:  4.70950704225\n",
      "accuracy of the training batch 117 is:  3.04347826087\n",
      "accuracy of the training batch 118 is:  6.1731557377\n",
      "accuracy of the training batch 119 is:  3.0517578125\n",
      "accuracy of the training batch 120 is:  5.14964788732\n",
      "accuracy of the training batch 121 is:  3.22353603604\n",
      "accuracy of the training batch 122 is:  5.23897058824\n",
      "accuracy of the training batch 123 is:  4.96199324324\n",
      "accuracy of the training batch 124 is:  3.97213855422\n",
      "accuracy of the training batch 125 is:  7.45442708333\n",
      "accuracy of the training batch 126 is:  7.69886363636\n",
      "accuracy of the training batch 127 is:  6.30296610169\n",
      "accuracy of the training batch 128 is:  3.21428571429\n",
      "accuracy of the training batch 129 is:  3.1631097561\n",
      "accuracy of the training batch 130 is:  5.75657894737\n",
      "accuracy of the training batch 131 is:  6.51483050847\n",
      "accuracy of the training batch 132 is:  2.28587962963\n",
      "accuracy of the training batch 133 is:  4.02057926829\n",
      "accuracy of the training batch 134 is:  5.00422297297\n",
      "accuracy of the training batch 135 is:  5.53075396825\n",
      "accuracy of the training batch 136 is:  5.2734375\n",
      "accuracy of the training batch 137 is:  1.93452380952\n",
      "accuracy of the training batch 138 is:  5.23200757576\n",
      "accuracy of the training batch 139 is:  3.29569327731\n",
      "accuracy of the training batch 140 is:  3.5046728972\n",
      "accuracy of the training batch 141 is:  5.63446969697\n",
      "accuracy of the training batch 142 is:  4.05890804598\n",
      "accuracy of the training batch 143 is:  3.72596153846\n",
      "accuracy of the training batch 144 is:  4.02298850575\n",
      "accuracy of the training batch 145 is:  2.69230769231\n",
      "accuracy of the training batch 146 is:  5.04166666667\n",
      "accuracy of the training batch 147 is:  3.27445652174\n",
      "accuracy of the training batch 148 is:  5.56640625\n",
      "accuracy of the training batch 149 is:  6.30387931034\n",
      "accuracy of the training batch 150 is:  2.97015765766\n",
      "Average Loss till the training batch 150 is:  7.89560133616\n",
      "\n",
      "crossvalid crossvalid crossvalid, crossvalid crossvalid crossvalid, crossvalid crossvalid crossvalid\n",
      "accuracy of the validation batch 44 is:  6.3720703125\n",
      "accuracy of the validation batch 49 is:  6.22306034483\n",
      "accuracy of the validation batch 20 is:  4.34229651163\n",
      "accuracy of the validation batch 9 is:  4.08909574468\n",
      "accuracy of the validation batch 15 is:  4.78639240506\n",
      "accuracy of the validation batch 21 is:  7.28125\n",
      "accuracy of the validation batch 11 is:  5.26713709677\n",
      "accuracy of the validation batch 41 is:  5.49568965517\n",
      "accuracy of the validation batch 1 is:  6.68859649123\n",
      "accuracy of the validation batch 17 is:  5.72183098592\n",
      "\n",
      "training training training, training training training, training training training training training\n",
      "accuracy of the training batch 151 is:  5.53385416667\n",
      "accuracy of the training batch 152 is:  4.45015822785\n",
      "accuracy of the training batch 153 is:  4.34451219512\n",
      "accuracy of the training batch 154 is:  5.48245614035\n",
      "accuracy of the training batch 155 is:  6.65509259259\n",
      "accuracy of the training batch 156 is:  5.60515873016\n",
      "accuracy of the training batch 157 is:  0.7975\n",
      "accuracy of the training batch 158 is:  3.57481060606\n",
      "accuracy of the training batch 159 is:  5.3036971831\n",
      "accuracy of the training batch 160 is:  4.18595679012\n",
      "accuracy of the training batch 161 is:  5.60096153846\n",
      "accuracy of the training batch 162 is:  7.38031914894\n",
      "accuracy of the training batch 163 is:  5.33203125\n",
      "accuracy of the training batch 164 is:  3.955078125\n",
      "accuracy of the training batch 165 is:  4.31463068182\n",
      "accuracy of the training batch 166 is:  3.97321428571\n",
      "accuracy of the training batch 167 is:  3.48668981481\n",
      "accuracy of the training batch 168 is:  4.14930555556\n",
      "accuracy of the training batch 169 is:  3.51890756303\n",
      "accuracy of the training batch 170 is:  4.24632352941\n",
      "accuracy of the training batch 171 is:  7.5625\n",
      "accuracy of the training batch 172 is:  4.04647435897\n",
      "accuracy of the training batch 173 is:  6.08173076923\n",
      "accuracy of the training batch 174 is:  4.48529411765\n",
      "accuracy of the training batch 175 is:  5.75810185185\n",
      "accuracy of the training batch 176 is:  5.68426724138\n",
      "accuracy of the training batch 177 is:  7.98233695652\n",
      "accuracy of the training batch 178 is:  3.61426767677\n",
      "accuracy of the training batch 179 is:  4.06901041667\n",
      "accuracy of the training batch 180 is:  6.2752016129\n",
      "accuracy of the training batch 181 is:  6.005859375\n",
      "accuracy of the training batch 182 is:  5.75396825397\n",
      "accuracy of the training batch 183 is:  5.48041044776\n",
      "accuracy of the training batch 184 is:  4.35505319149\n",
      "accuracy of the training batch 185 is:  6.54481132075\n",
      "accuracy of the training batch 186 is:  7.89930555556\n",
      "accuracy of the training batch 187 is:  5.21537162162\n",
      "accuracy of the training batch 188 is:  7.01704545455\n",
      "accuracy of the training batch 189 is:  4.296875\n",
      "accuracy of the training batch 190 is:  6.82091346154\n",
      "accuracy of the training batch 191 is:  7.11805555556\n",
      "accuracy of the training batch 192 is:  7.25\n",
      "accuracy of the training batch 193 is:  4.46428571429\n",
      "accuracy of the training batch 194 is:  5.3125\n",
      "accuracy of the training batch 195 is:  6.65322580645\n",
      "accuracy of the training batch 196 is:  4.44711538462\n",
      "accuracy of the training batch 197 is:  6.67892156863\n",
      "accuracy of the training batch 198 is:  4.38218390805\n",
      "accuracy of the training batch 199 is:  8.11887254902\n",
      "accuracy of the training batch 200 is:  3.8699127907\n",
      "Average Loss till the training batch 200 is:  7.71185467958\n",
      "\n",
      "crossvalid crossvalid crossvalid, crossvalid crossvalid crossvalid, crossvalid crossvalid crossvalid\n",
      "accuracy of the validation batch 44 is:  6.396484375\n",
      "accuracy of the validation batch 49 is:  6.38469827586\n",
      "accuracy of the validation batch 20 is:  4.57848837209\n",
      "accuracy of the validation batch 9 is:  4.53789893617\n",
      "accuracy of the validation batch 15 is:  5.22151898734\n",
      "accuracy of the validation batch 21 is:  7.53125\n",
      "accuracy of the validation batch 11 is:  5.64516129032\n",
      "accuracy of the validation batch 41 is:  5.9536637931\n",
      "accuracy of the validation batch 1 is:  7.15460526316\n",
      "accuracy of the validation batch 17 is:  6.27200704225\n",
      "\n",
      "training training training, training training training, training training training training training\n",
      "accuracy of the training batch 201 is:  5.51819620253\n",
      "accuracy of the training batch 202 is:  7.17592592593\n",
      "accuracy of the training batch 203 is:  5.25\n",
      "accuracy of the training batch 204 is:  6.32944915254\n",
      "accuracy of the training batch 205 is:  5.3171641791\n",
      "accuracy of the training batch 206 is:  4.296875\n",
      "accuracy of the training batch 207 is:  7.35294117647\n",
      "accuracy of the training batch 208 is:  5.36057692308\n",
      "accuracy of the training batch 209 is:  4.7637195122\n",
      "accuracy of the training batch 210 is:  5.8302238806\n",
      "accuracy of the training batch 211 is:  6.05769230769\n",
      "accuracy of the training batch 212 is:  7.59803921569\n",
      "accuracy of the training batch 213 is:  2.61101973684\n",
      "accuracy of the training batch 214 is:  6.80970149254\n",
      "accuracy of the training batch 215 is:  4.11278735632\n",
      "accuracy of the training batch 216 is:  5.53728070175\n",
      "accuracy of the training batch 217 is:  7.47549019608\n",
      "accuracy of the training batch 218 is:  5.32483552632\n",
      "accuracy of the training batch 219 is:  7.33418367347\n",
      "accuracy of the training batch 220 is:  5.00422297297\n",
      "accuracy of the training batch 221 is:  3.73579545455\n",
      "accuracy of the training batch 222 is:  7.58928571429\n",
      "accuracy of the training batch 223 is:  5.94758064516\n",
      "accuracy of the training batch 224 is:  6.2756147541\n",
      "accuracy of the training batch 225 is:  6.78066037736\n",
      "accuracy of the training batch 226 is:  4.30126404494\n",
      "accuracy of the training batch 227 is:  4.91898148148\n",
      "accuracy of the training batch 228 is:  6.45161290323\n",
      "accuracy of the training batch 229 is:  7.01650943396\n",
      "accuracy of the training batch 230 is:  5.83333333333\n",
      "accuracy of the training batch 231 is:  2.23615269461\n",
      "accuracy of the training batch 232 is:  4.92647058824\n",
      "accuracy of the training batch 233 is:  4.36046511628\n",
      "accuracy of the training batch 234 is:  6.36160714286\n",
      "accuracy of the training batch 235 is:  6.97737068966\n",
      "accuracy of the training batch 236 is:  3.47222222222\n",
      "accuracy of the training batch 237 is:  4.38829787234\n",
      "accuracy of the training batch 238 is:  4.35505319149\n",
      "accuracy of the training batch 239 is:  8.57638888889\n",
      "accuracy of the training batch 240 is:  5.65068493151\n",
      "accuracy of the training batch 241 is:  6.19212962963\n",
      "accuracy of the training batch 242 is:  7.25235849057\n",
      "accuracy of the training batch 243 is:  5.41666666667\n",
      "accuracy of the training batch 244 is:  4.541015625\n",
      "accuracy of the training batch 245 is:  4.55013736264\n",
      "accuracy of the training batch 246 is:  5.9814453125\n",
      "accuracy of the training batch 247 is:  5.00351123596\n",
      "accuracy of the training batch 248 is:  3.34272540984\n",
      "accuracy of the training batch 249 is:  5.52226027397\n",
      "accuracy of the training batch 250 is:  5.25\n",
      "Average Loss till the training batch 250 is:  7.57659057999\n",
      "\n",
      "crossvalid crossvalid crossvalid, crossvalid crossvalid crossvalid, crossvalid crossvalid crossvalid\n",
      "accuracy of the validation batch 44 is:  6.5673828125\n",
      "accuracy of the validation batch 49 is:  6.14224137931\n",
      "accuracy of the validation batch 20 is:  4.52398255814\n",
      "accuracy of the validation batch 9 is:  4.17220744681\n",
      "accuracy of the validation batch 15 is:  5.06329113924\n",
      "accuracy of the validation batch 21 is:  7.75\n",
      "accuracy of the validation batch 11 is:  6.12399193548\n",
      "accuracy of the validation batch 41 is:  6.11530172414\n",
      "accuracy of the validation batch 1 is:  6.99013157895\n",
      "accuracy of the validation batch 17 is:  6.00792253521\n",
      "\n",
      "training training training, training training training, training training training training training\n",
      "accuracy of the training batch 251 is:  5.88107638889\n",
      "accuracy of the training batch 252 is:  6.396484375\n",
      "accuracy of the training batch 253 is:  4.53464673913\n",
      "accuracy of the training batch 254 is:  7.32230392157\n",
      "accuracy of the training batch 255 is:  4.66994382022\n",
      "accuracy of the training batch 256 is:  4.70486111111\n",
      "accuracy of the training batch 257 is:  4.40512048193\n",
      "accuracy of the training batch 258 is:  4.78515625\n",
      "accuracy of the training batch 259 is:  6.85763888889\n",
      "accuracy of the training batch 260 is:  6.3446969697\n",
      "accuracy of the training batch 261 is:  7.15144230769\n",
      "accuracy of the training batch 262 is:  7.07720588235\n",
      "accuracy of the training batch 263 is:  2.72108843537\n",
      "accuracy of the training batch 264 is:  5.42652027027\n",
      "accuracy of the training batch 265 is:  6.69270833333\n",
      "accuracy of the training batch 266 is:  8.45588235294\n",
      "accuracy of the training batch 267 is:  7.10227272727\n",
      "accuracy of the training batch 268 is:  5.67307692308\n",
      "accuracy of the training batch 269 is:  3.39543269231\n",
      "accuracy of the training batch 270 is:  6.00961538462\n",
      "accuracy of the training batch 271 is:  4.75852272727\n",
      "accuracy of the training batch 272 is:  3.97995283019\n",
      "accuracy of the training batch 273 is:  4.27442528736\n",
      "accuracy of the training batch 274 is:  3.92045454545\n",
      "accuracy of the training batch 275 is:  6.45833333333\n",
      "accuracy of the training batch 276 is:  6.02355072464\n",
      "accuracy of the training batch 277 is:  5.89583333333\n",
      "accuracy of the training batch 278 is:  8.6277173913\n",
      "accuracy of the training batch 279 is:  5.5625\n",
      "accuracy of the training batch 280 is:  5.07373595506\n",
      "accuracy of the training batch 281 is:  6.83262711864\n",
      "accuracy of the training batch 282 is:  9.30397727273\n",
      "accuracy of the training batch 283 is:  5.32962328767\n",
      "accuracy of the training batch 284 is:  7.48922413793\n",
      "accuracy of the training batch 285 is:  3.08626033058\n",
      "accuracy of the training batch 286 is:  7.64627659574\n",
      "accuracy of the training batch 287 is:  6.96721311475\n",
      "accuracy of the training batch 288 is:  3.31196581197\n",
      "accuracy of the training batch 289 is:  5.02873563218\n",
      "accuracy of the training batch 290 is:  3.80764563107\n",
      "accuracy of the training batch 291 is:  5.15422077922\n",
      "accuracy of the training batch 292 is:  4.51807228916\n",
      "accuracy of the training batch 293 is:  7.34649122807\n",
      "accuracy of the training batch 294 is:  6.84523809524\n",
      "accuracy of the training batch 295 is:  6.75117924528\n",
      "accuracy of the training batch 296 is:  4.65073529412\n",
      "accuracy of the training batch 297 is:  7.24431818182\n",
      "accuracy of the training batch 298 is:  6.65246212121\n",
      "accuracy of the training batch 299 is:  5.95703125\n",
      "accuracy of the training batch 300 is:  6.6162109375\n",
      "Average Loss till the training batch 300 is:  7.46389214675\n",
      "\n",
      "crossvalid crossvalid crossvalid, crossvalid crossvalid crossvalid, crossvalid crossvalid crossvalid\n",
      "accuracy of the validation batch 44 is:  6.982421875\n",
      "accuracy of the validation batch 49 is:  6.35775862069\n",
      "accuracy of the validation batch 20 is:  4.66933139535\n",
      "accuracy of the validation batch 9 is:  4.65425531915\n",
      "accuracy of the validation batch 15 is:  5.41930379747\n",
      "accuracy of the validation batch 21 is:  8.09375\n",
      "accuracy of the validation batch 11 is:  6.62802419355\n",
      "accuracy of the validation batch 41 is:  6.7349137931\n",
      "accuracy of the validation batch 1 is:  7.53837719298\n",
      "accuracy of the validation batch 17 is:  6.38204225352\n",
      "\n",
      "training training training, training training training, training training training training training\n",
      "accuracy of the training batch 301 is:  5.51948051948\n",
      "accuracy of the training batch 302 is:  5.94618055556\n",
      "accuracy of the training batch 303 is:  5.01077586207\n",
      "accuracy of the training batch 304 is:  7.1875\n",
      "accuracy of the training batch 305 is:  7.18201754386\n",
      "accuracy of the training batch 306 is:  6.4208984375\n",
      "accuracy of the training batch 307 is:  7.12719298246\n",
      "accuracy of the training batch 308 is:  7.27459016393\n",
      "accuracy of the training batch 309 is:  5.22203947368\n",
      "accuracy of the training batch 310 is:  6.85911016949\n",
      "accuracy of the training batch 311 is:  4.93222891566\n",
      "accuracy of the training batch 312 is:  6.19959677419\n",
      "accuracy of the training batch 313 is:  7.2337962963\n",
      "accuracy of the training batch 314 is:  6.03813559322\n",
      "accuracy of the training batch 315 is:  5.59111445783\n",
      "accuracy of the training batch 316 is:  5.08868243243\n",
      "accuracy of the training batch 317 is:  3.92219387755\n",
      "accuracy of the training batch 318 is:  5.93297101449\n",
      "accuracy of the training batch 319 is:  7.421875\n",
      "accuracy of the training batch 320 is:  6.96271929825\n",
      "accuracy of the training batch 321 is:  6.20535714286\n",
      "accuracy of the training batch 322 is:  5.34048507463\n",
      "accuracy of the training batch 323 is:  7.75240384615\n",
      "accuracy of the training batch 324 is:  6.22596153846\n",
      "accuracy of the training batch 325 is:  3.39781746032\n",
      "accuracy of the training batch 326 is:  6.22888513514\n",
      "accuracy of the training batch 327 is:  2.85183566434\n",
      "accuracy of the training batch 328 is:  7.31770833333\n",
      "accuracy of the training batch 329 is:  5.44819078947\n",
      "accuracy of the training batch 330 is:  6.59090909091\n",
      "accuracy of the training batch 331 is:  7.875\n",
      "accuracy of the training batch 332 is:  8.14302884615\n",
      "accuracy of the training batch 333 is:  7.41525423729\n",
      "accuracy of the training batch 334 is:  8.47355769231\n",
      "accuracy of the training batch 335 is:  7.16594827586\n",
      "accuracy of the training batch 336 is:  6.52281746032\n",
      "accuracy of the training batch 337 is:  5.95034246575\n",
      "accuracy of the training batch 338 is:  7.1875\n",
      "accuracy of the training batch 339 is:  8.42633928571\n",
      "accuracy of the training batch 340 is:  5.04518072289\n",
      "accuracy of the training batch 341 is:  8.73724489796\n",
      "accuracy of the training batch 342 is:  5.35996835443\n",
      "accuracy of the training batch 343 is:  6.86961206897\n",
      "accuracy of the training batch 344 is:  5.17314189189\n",
      "accuracy of the training batch 345 is:  7.91843220339\n",
      "accuracy of the training batch 346 is:  6.15671641791\n",
      "accuracy of the training batch 347 is:  7.46173469388\n",
      "accuracy of the training batch 348 is:  6.74479166667\n",
      "accuracy of the training batch 349 is:  7.94719827586\n",
      "accuracy of the training batch 350 is:  4.93328651685\n",
      "Average Loss till the training batch 350 is:  7.37438412121\n",
      "\n",
      "crossvalid crossvalid crossvalid, crossvalid crossvalid crossvalid, crossvalid crossvalid crossvalid\n",
      "accuracy of the validation batch 44 is:  6.93359375\n",
      "accuracy of the validation batch 49 is:  6.68103448276\n",
      "accuracy of the validation batch 20 is:  4.70566860465\n",
      "accuracy of the validation batch 9 is:  4.50465425532\n",
      "accuracy of the validation batch 15 is:  5.51819620253\n",
      "accuracy of the validation batch 21 is:  8.21875\n",
      "accuracy of the validation batch 11 is:  6.42641129032\n",
      "accuracy of the validation batch 41 is:  6.51939655172\n",
      "accuracy of the validation batch 1 is:  7.51096491228\n",
      "accuracy of the validation batch 17 is:  6.44806338028\n",
      "\n",
      "training training training, training training training, training training training training training\n",
      "accuracy of the training batch 351 is:  6.1731557377\n",
      "accuracy of the training batch 352 is:  7.42924528302\n",
      "accuracy of the training batch 353 is:  7.04495614035\n",
      "accuracy of the training batch 354 is:  5.34420289855\n",
      "accuracy of the training batch 355 is:  5.58978873239\n",
      "accuracy of the training batch 356 is:  4.21401515152\n",
      "accuracy of the training batch 357 is:  6.71875\n",
      "accuracy of the training batch 358 is:  7.83854166667\n",
      "accuracy of the training batch 359 is:  6.17788461538\n",
      "accuracy of the training batch 360 is:  6.85763888889\n",
      "accuracy of the training batch 361 is:  7.06623134328\n",
      "accuracy of the training batch 362 is:  7.72879464286\n",
      "accuracy of the training batch 363 is:  7.25446428571\n",
      "accuracy of the training batch 364 is:  7.37390350877\n",
      "accuracy of the training batch 365 is:  5.89285714286\n",
      "accuracy of the training batch 366 is:  8.70949074074\n",
      "accuracy of the training batch 367 is:  7.51201923077\n",
      "accuracy of the training batch 368 is:  7.92824074074\n",
      "accuracy of the training batch 369 is:  6.02992957746\n",
      "accuracy of the training batch 370 is:  7.30064655172\n",
      "accuracy of the training batch 371 is:  3.28246124031\n",
      "accuracy of the training batch 372 is:  6.07876712329\n",
      "accuracy of the training batch 373 is:  7.64508928571\n",
      "accuracy of the training batch 374 is:  6.00961538462\n",
      "accuracy of the training batch 375 is:  6.38586956522\n",
      "accuracy of the training batch 376 is:  5.04954268293\n",
      "accuracy of the training batch 377 is:  2.69976265823\n",
      "accuracy of the training batch 378 is:  5.29725609756\n",
      "accuracy of the training batch 379 is:  7.65881147541\n",
      "accuracy of the training batch 380 is:  8.0078125\n",
      "accuracy of the training batch 381 is:  4.44834183673\n",
      "accuracy of the training batch 382 is:  5.50176056338\n",
      "accuracy of the training batch 383 is:  6.15808823529\n",
      "accuracy of the training batch 384 is:  5.69490131579\n",
      "accuracy of the training batch 385 is:  8.203125\n",
      "accuracy of the training batch 386 is:  6.31996268657\n",
      "accuracy of the training batch 387 is:  8.96226415094\n",
      "accuracy of the training batch 388 is:  3.675\n",
      "accuracy of the training batch 389 is:  4.88079896907\n",
      "accuracy of the training batch 390 is:  6.07394366197\n",
      "accuracy of the training batch 391 is:  3.73475609756\n",
      "accuracy of the training batch 392 is:  7.41477272727\n",
      "accuracy of the training batch 393 is:  6.67162698413\n",
      "accuracy of the training batch 394 is:  7.29166666667\n",
      "accuracy of the training batch 395 is:  7.21354166667\n",
      "accuracy of the training batch 396 is:  6.81612318841\n",
      "accuracy of the training batch 397 is:  7.11495535714\n",
      "accuracy of the training batch 398 is:  6.2744140625\n",
      "accuracy of the training batch 399 is:  6.88004032258\n",
      "accuracy of the training batch 400 is:  5.98958333333\n",
      "Average Loss till the training batch 400 is:  7.29732568622\n",
      "\n",
      "crossvalid crossvalid crossvalid, crossvalid crossvalid crossvalid, crossvalid crossvalid crossvalid\n",
      "accuracy of the validation batch 44 is:  6.982421875\n",
      "accuracy of the validation batch 49 is:  6.7349137931\n",
      "accuracy of the validation batch 20 is:  4.88735465116\n",
      "accuracy of the validation batch 9 is:  4.50465425532\n",
      "accuracy of the validation batch 15 is:  5.71598101266\n",
      "accuracy of the validation batch 21 is:  8.6875\n",
      "accuracy of the validation batch 11 is:  6.55241935484\n",
      "accuracy of the validation batch 41 is:  6.76185344828\n",
      "accuracy of the validation batch 1 is:  7.94956140351\n",
      "accuracy of the validation batch 17 is:  6.42605633803\n",
      "\n",
      "training training training, training training training, training training training training training\n",
      "accuracy of the training batch 401 is:  4.8981741573\n",
      "accuracy of the training batch 402 is:  3.92127403846\n",
      "accuracy of the training batch 403 is:  6.94159836066\n",
      "accuracy of the training batch 404 is:  5.44921875\n",
      "accuracy of the training batch 405 is:  8.47355769231\n",
      "accuracy of the training batch 406 is:  5.34156976744\n",
      "accuracy of the training batch 407 is:  6.70021186441\n",
      "accuracy of the training batch 408 is:  7.35795454545\n",
      "accuracy of the training batch 409 is:  5.01453488372\n",
      "accuracy of the training batch 410 is:  1.86323221757\n",
      "accuracy of the training batch 411 is:  4.97485632184\n",
      "accuracy of the training batch 412 is:  6.70362903226\n",
      "accuracy of the training batch 413 is:  6.8115234375\n",
      "accuracy of the training batch 414 is:  4.29276315789\n",
      "accuracy of the training batch 415 is:  6.54871323529\n",
      "accuracy of the training batch 416 is:  8.66745283019\n",
      "accuracy of the training batch 417 is:  3.37540064103\n",
      "accuracy of the training batch 418 is:  7.08333333333\n",
      "accuracy of the training batch 419 is:  7.6171875\n",
      "accuracy of the training batch 420 is:  5.62876506024\n",
      "accuracy of the training batch 421 is:  7.5161637931\n",
      "accuracy of the training batch 422 is:  5.14481707317\n",
      "accuracy of the training batch 423 is:  5.04261363636\n",
      "accuracy of the training batch 424 is:  6.25\n",
      "accuracy of the training batch 425 is:  5.20198170732\n",
      "accuracy of the training batch 426 is:  6.0302734375\n",
      "accuracy of the training batch 427 is:  5.0828313253\n",
      "accuracy of the training batch 428 is:  6.86475409836\n",
      "accuracy of the training batch 429 is:  5.24929775281\n",
      "accuracy of the training batch 430 is:  7.98611111111\n",
      "accuracy of the training batch 431 is:  5.08261494253\n",
      "accuracy of the training batch 432 is:  5.8277027027\n",
      "accuracy of the training batch 433 is:  9.65277777778\n",
      "accuracy of the training batch 434 is:  6.5625\n",
      "accuracy of the training batch 435 is:  6.25\n",
      "accuracy of the training batch 436 is:  8.48651960784\n",
      "accuracy of the training batch 437 is:  5.01736111111\n",
      "accuracy of the training batch 438 is:  4.00309917355\n",
      "accuracy of the training batch 439 is:  7.05492424242\n",
      "accuracy of the training batch 440 is:  6.36837121212\n",
      "accuracy of the training batch 441 is:  4.8704954955\n",
      "accuracy of the training batch 442 is:  5.546875\n",
      "accuracy of the training batch 443 is:  8.54779411765\n",
      "accuracy of the training batch 444 is:  7.56138392857\n",
      "accuracy of the training batch 445 is:  7.07097457627\n",
      "accuracy of the training batch 446 is:  8.51403061224\n",
      "accuracy of the training batch 447 is:  5.54887820513\n",
      "accuracy of the training batch 448 is:  7.72879464286\n",
      "accuracy of the training batch 449 is:  6.94444444444\n",
      "accuracy of the training batch 450 is:  5.15625\n",
      "Average Loss till the training batch 450 is:  7.22687611474\n",
      "\n",
      "crossvalid crossvalid crossvalid, crossvalid crossvalid crossvalid, crossvalid crossvalid crossvalid\n",
      "accuracy of the validation batch 44 is:  7.470703125\n",
      "accuracy of the validation batch 49 is:  6.95043103448\n",
      "accuracy of the validation batch 20 is:  4.86918604651\n",
      "accuracy of the validation batch 9 is:  4.80385638298\n",
      "accuracy of the validation batch 15 is:  5.83465189873\n",
      "accuracy of the validation batch 21 is:  8.5625\n",
      "accuracy of the validation batch 11 is:  6.82963709677\n",
      "accuracy of the validation batch 41 is:  6.68103448276\n",
      "accuracy of the validation batch 1 is:  7.7850877193\n",
      "accuracy of the validation batch 17 is:  6.88820422535\n",
      "\n",
      "training training training, training training training, training training training training training\n",
      "accuracy of the training batch 451 is:  5.13649425287\n",
      "accuracy of the training batch 452 is:  6.27297794118\n",
      "accuracy of the training batch 453 is:  4.7214673913\n",
      "accuracy of the training batch 454 is:  3.78151260504\n",
      "accuracy of the training batch 455 is:  3.13616071429\n",
      "accuracy of the training batch 456 is:  8.35227272727\n",
      "accuracy of the training batch 457 is:  5.84415584416\n",
      "accuracy of the training batch 458 is:  6.77083333333\n",
      "accuracy of the training batch 459 is:  7.24897540984\n",
      "accuracy of the training batch 460 is:  4.54545454545\n",
      "accuracy of the training batch 461 is:  7.14699074074\n",
      "accuracy of the training batch 462 is:  4.64199029126\n",
      "accuracy of the training batch 463 is:  3.56971153846\n",
      "accuracy of the training batch 464 is:  5.79509493671\n",
      "accuracy of the training batch 465 is:  7.84375\n",
      "accuracy of the training batch 466 is:  6.79824561404\n",
      "accuracy of the training batch 467 is:  7.42872807018\n",
      "accuracy of the training batch 468 is:  8.56370192308\n",
      "accuracy of the training batch 469 is:  6.83302238806\n",
      "accuracy of the training batch 470 is:  7.22336065574\n",
      "accuracy of the training batch 471 is:  7.73305084746\n",
      "accuracy of the training batch 472 is:  6.5673828125\n",
      "accuracy of the training batch 473 is:  9.78125\n",
      "accuracy of the training batch 474 is:  8.08189655172\n",
      "accuracy of the training batch 475 is:  2.14049796748\n",
      "accuracy of the training batch 476 is:  7.52314814815\n",
      "accuracy of the training batch 477 is:  5.30873493976\n",
      "accuracy of the training batch 478 is:  6.396484375\n",
      "accuracy of the training batch 479 is:  5.64123376623\n",
      "accuracy of the training batch 480 is:  5.54366438356\n",
      "accuracy of the training batch 481 is:  6.89338235294\n",
      "accuracy of the training batch 482 is:  6.47007042254\n",
      "accuracy of the training batch 483 is:  7.41525423729\n",
      "accuracy of the training batch 484 is:  5.9335443038\n",
      "accuracy of the training batch 485 is:  7.83898305085\n",
      "accuracy of the training batch 486 is:  5.20833333333\n",
      "accuracy of the training batch 487 is:  5.15422077922\n",
      "accuracy of the training batch 488 is:  6.62076271186\n",
      "accuracy of the training batch 489 is:  5.58894230769\n",
      "accuracy of the training batch 490 is:  7.1533203125\n",
      "accuracy of the training batch 491 is:  5.40441176471\n",
      "accuracy of the training batch 492 is:  5.96955128205\n",
      "accuracy of the training batch 493 is:  7.96618852459\n",
      "accuracy of the training batch 494 is:  4.79525862069\n",
      "accuracy of the training batch 495 is:  4.71698113208\n",
      "accuracy of the training batch 496 is:  6.57894736842\n",
      "accuracy of the training batch 497 is:  6.54438405797\n",
      "accuracy of the training batch 498 is:  5.12576219512\n",
      "accuracy of the training batch 499 is:  5.60897435897\n",
      "All 500 Batches Done.................................\n",
      "\n",
      "\n",
      "Average training loss for Epoch 0 : 7.15461982536\n"
     ]
    }
   ],
   "source": [
    "obj_Train = Train()\n",
    "graph_dict =  dynamic_RNN_model(num_hid_units=300, vocab_size = obj_Train.vocab_size+1,)\n",
    "obj_Train.train_network(graph_dict, num_batches = 500, epochs=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
