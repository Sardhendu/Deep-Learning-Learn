{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import urllib.request\n",
    "from six.moves import cPickle as pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.models.rnn.ptb import reader\n",
    "from gensim import corpora\n",
    "\n",
    "\n",
    "from model import  dynamic_RNN_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_graph():  # Reset the graph\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dynamic_RNN_model(\n",
    "    num_hid_units = 3,\n",
    "    vocab_size = 7,\n",
    "    momentum = 0.9,\n",
    "    learning_rate = 0.01\n",
    "    ):\n",
    "    \n",
    "    num_classes = vocab_size\n",
    "\n",
    "    reset_graph()\n",
    "    \n",
    "    x = tf.placeholder(tf.int32, shape = [None, None], name='input_placeholder')\n",
    "    y = tf.placeholder(tf.int32, shape = [None, None], name='output_placeholder')\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    \n",
    "\n",
    "    # ENBEDDING(INPUT) LAYER OPERATION\n",
    "    # Creating an Embedding matrix with a random weight for all vacab to hidden_matrix\n",
    "    embed_to_hid_wghts = tf.get_variable('embedding_matrix', [vocab_size, num_hid_units])\n",
    "    embed_to_hid_layer = tf.nn.embedding_lookup(embed_to_hid_wghts, x)\n",
    "\n",
    "\n",
    "    # HIDDEN LAYER OPERATION\n",
    "    rnn_cell = tf.nn.rnn_cell.LSTMCell(num_hid_units, state_is_tuple=True)\n",
    "    init_state = rnn_cell.zero_state(batch_size, tf.float32)  # Each sequence will hava a state that it passes to its next sequence\n",
    "    rnn_outputs, new_state = tf.nn.dynamic_rnn(\n",
    "                                        cell=rnn_cell,\n",
    "                                        # sequence_length=X_lengths,\n",
    "                                        initial_state=init_state,\n",
    "                                        inputs=embed_to_hid_layer)\n",
    "    \n",
    "\n",
    "    # OUTPUT LAYER OPERATION\n",
    "    # Initialize the weight and biases for the output layer. We use variable scope because we would like to share the weights \n",
    "    with tf.variable_scope('output_layer'):\n",
    "        hid_to_output_wght = tf.get_variable('hid_to_output_wght', \n",
    "                                                 [num_hid_units, num_classes], \n",
    "                                                 initializer = tf.random_normal_initializer())\n",
    "        output_bias = tf.get_variable('output_bias',\n",
    "                                      [num_classes],\n",
    "                                      initializer = tf.random_normal_initializer())\n",
    "    \n",
    "    rnn_outputs = tf.reshape(rnn_outputs, [-1, num_hid_units])  \n",
    "    hid_to_ouptut_layer = tf.matmul(rnn_outputs, hid_to_output_wght) +  output_bias\n",
    "    output_state = tf.nn.softmax(hid_to_ouptut_layer, name=None)\n",
    " \n",
    "    \n",
    "    \n",
    "    # CALCULATING LOSS, OPTIMIZING THE COST FUNCTION, MEASURING ACCURACY\n",
    "    loss_CE = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(hid_to_ouptut_layer, tf.reshape(y, [-1])))\n",
    "    \n",
    "    # The sparse_softmax uses dtype as int32 or int64\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, \n",
    "                                            momentum, \n",
    "                                            use_locking=False, \n",
    "                                            name='Momentum', \n",
    "                                            use_nesterov=True).minimize(loss_CE)\n",
    "    # y_ = tf.reshape(y, [-1])\n",
    "    # correct_prediction = tf.equal(tf.arg_max(output_state,1), tf.arg_max(y_ ,1))\n",
    "    # accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "    \n",
    "\n",
    "    # Returns graph objects\n",
    "    return dict(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        batch_size = batch_size,\n",
    "        init_state = init_state,\n",
    "        new_state = new_state,\n",
    "        loss_CE = loss_CE,\n",
    "        optimizer = optimizer,\n",
    "        training_prediction = output_state\n",
    "        # accuracy = tf.Variable([[1,2,3]])\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Train():\n",
    "    def __init__(self):\n",
    "        self.num_hid_units = 3,\n",
    "        self.momentum = 0.9,\n",
    "        self.learning_rate = 0.5\n",
    "        self.train_batch_dir = '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Word-Search-NNets/Word-Nets/training_batch/'\n",
    "        dictionary_dir = '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Word-Search-NNets/Word-Nets/dictionary.txt'\n",
    "        self.vocab_size = len(corpora.Dictionary.load_from_text(dictionary_dir))\n",
    "\n",
    "    def accuracy(self, predictions, labels, labels_one_hot = None):\n",
    "        # The input labels are a One-Hot Vector\n",
    "        if labels_one_hot:\n",
    "            return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "                  / predictions.shape[0])\n",
    "        else:\n",
    "            return (100.0 * np.sum(np.argmax(predictions, 1) == np.reshape(labels, [-1]))\n",
    "                  / predictions.shape[0])\n",
    "        \n",
    "\n",
    "    def train_network(self, num_batches, graph_dict):\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "            \n",
    "            aa = np.array([[1,2,3,4,5], [1,3,4,0,5], [2,3,4,5,6]])  # , [2,3,4,5,6], [2,1,5,4,3]\n",
    "            bb = np.array([[2,3,4,5,2], [3,4,0,5,2], [3,4,5,6,2]])  # , [3,4,5,6,2], [1,5,4,3,2]\n",
    "            cc = np.array([[1,4,2,2,1,3], [1,4,3,3,0,0]])\n",
    "            dd = np.array([[4,2,2,5,3,2], [4,3,0,5,3,0]])\n",
    "            epochs = 1\n",
    "            for epoch in np.arange(epochs):\n",
    "                new_hid_layer_state = None\n",
    "                print (graph_dict)\n",
    "\n",
    "                for no in np.arange(num_batches):#[[aa,bb],[cc,dd]]:#np.arange(2):\n",
    "                    with open(self.train_batch_dir+'batch'+str(no)+'.pickle', 'rb') as f:\n",
    "                        dataset = pickle.load(f)\n",
    "                        \n",
    "                        batch_train_dataset = dataset['batch_train_dataset']\n",
    "                        batch_train_labels = dataset['batch_train_labels']\n",
    "\n",
    "                        if not new_hid_layer_state: \n",
    "                            feed_dict= {graph_dict['x']: batch_train_dataset, \n",
    "                                        graph_dict['y']: batch_train_labels}\n",
    "                                        # graph_dict['batch_size']: batch_size}\n",
    "                        else:\n",
    "                            print ('Using the new RNN State')\n",
    "                            feed_dict= {graph_dict['x']: batch_train_dataset, \n",
    "                                        graph_dict['y']: batch_train_labels,\n",
    "                                        graph_dict['init_state'] : new_hid_layer_state}\n",
    "\n",
    "                        bs, nwst, loss, opt, tp = sess.run([graph_dict['batch_size'],\n",
    "                                                        graph_dict['new_state'],\n",
    "                                                        graph_dict['loss_CE'],\n",
    "                                                        graph_dict['optimizer'],\n",
    "                                                        graph_dict['training_prediction']], \n",
    "                                                        feed_dict=feed_dict)\n",
    "                        new_hid_layer_state = nwst\n",
    "\n",
    "                        acc = self.accuracy(tp, batch_train_labels)\n",
    "\n",
    "                        # print ('Batch size \\n', bs)\n",
    "                        # print ('')\n",
    "                        # print ('New State \\n', nwst)\n",
    "                        # print ('')\n",
    "                        # print ('Avg Loss \\n',  loss)\n",
    "                        # print ('')\n",
    "                        # print ('optimizer \\n', opt)\n",
    "                        # print ('')\n",
    "                        # print ('training_prediction \\n', tp)\n",
    "                        # print ('')\n",
    "                        print ('accuracy \\n', acc)\n",
    "                        print ('')\n",
    "                        print ('')\n",
    "                        print ('popopopopopopopoop')\n",
    "                        print ('')\n",
    "                        print ('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (<ipython-input-12-1cf8568353a7>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-1cf8568353a7>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    obj_Train.train_network(num_batches = 2, graph_dict)\u001b[0m\n\u001b[0m                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "obj_Train = Train()\n",
    "graph_dict =  dynamic_RNN_model(num_hid_units = 3,\n",
    "                                vocab_size = obj_Train.vocab_size,\n",
    "                                momentum = 0.9,\n",
    "                                learning_rate = 0.01)\n",
    "obj_Train.train_network(num_batches = 2, graph_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
