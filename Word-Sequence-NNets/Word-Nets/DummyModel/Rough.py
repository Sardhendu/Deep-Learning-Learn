The num of hidden unit is:  3
The Vocab size is:  7
The momentum is:  0.9
The learning_rate is:  0.001
The shape of embed_to_hid_wghts is:  (7, 3)
The shape of embed_to_hid_layer is:  (?, ?, 3)
[[1 2 3 4 5]
 [1 3 4 0 0]]

[[2 3 4 5 6]
 [3 4 6 0 0]]

[5 3]

x_lenarr =  [ 5.  3.]

batch_size =  2

init_state =  LSTMStateTuple(c=array([[ 0.,  0.,  0.],
       [ 0.,  0.,  0.]], dtype=float32), h=array([[ 0.,  0.,  0.],
       [ 0.,  0.,  0.]], dtype=float32))

new_state =  LSTMStateTuple(c=array([[-0.17453107,  0.18170479,  0.13560715],
       [-0.14755413,  0.30341703,  0.05169982]], dtype=float32), h=array([[-0.08735172,  0.09806846,  0.0721207 ],
       [-0.08132361,  0.16405913,  0.02307099]], dtype=float32))

rnn_output =  [[-0.11240847  0.02237382 -0.01146052]
 [-0.09266036  0.08729238 -0.02846817]
 [-0.03471814  0.18594502  0.00067257]
 [-0.09410399  0.2101521   0.01817178]
 [-0.08735172  0.09806846  0.0721207 ]
 [-0.11240847  0.02237382 -0.01146052]
 [-0.0359637   0.12716477  0.0099439 ]
 [-0.08132361  0.16405913  0.02307099]
 [ 0.          0.          0.        ]
 [ 0.          0.          0.        ]]

hid_to_ouptut_layer =  [[-0.11464401  1.32737565 -0.34149152 -0.55111301 -1.08583295  1.5553484
   1.41647005]
 [-0.10227735  1.19219267 -0.22348496 -0.57343334 -0.93944943  1.5632515
   1.30682516]
 [-0.1130424   1.08792579  0.05954307 -0.679474   -0.6834172   1.54642546
   1.21449101]
 [-0.15113324  1.03349686  0.05380943 -0.6454677  -0.67877626  1.57004809
   1.17502534]
 [-0.17495266  1.3293438  -0.06951168 -0.66564476 -0.89330006  1.53157938
   1.40861666]
 [-0.11464401  1.32737565 -0.34149152 -0.55111301 -1.08583295  1.5553484
   1.41647005]
 [-0.11346136  1.21104324 -0.03514823 -0.66965723 -0.80354923  1.53417492
   1.31319988]
 [-0.14451718  1.13476276 -0.0069485  -0.64862341 -0.76218897  1.55539048
   1.25506699]
 [-0.07781408  1.45492578 -0.23185977 -0.65937197 -1.03767884  1.50072896
   1.50725806]
 [-0.07781408  1.45492578 -0.23185977 -0.65937197 -1.03767884  1.50072896
   1.50725806]]

output_state =  [[ 0.05886969  0.2489734   0.04692164  0.03804833  0.02228998  0.31272331
   0.2721737 ]
 [ 0.06260613  0.22845364  0.05545964  0.03908377  0.02710427  0.33109063
   0.25620189]
 [ 0.06380478  0.21204451  0.0758239   0.03621221  0.0360697   0.33539063
   0.2406543 ]
 [ 0.06228752  0.20364766  0.07645512  0.03799394  0.03674927  0.34825763
   0.23460887]
 [ 0.05526669  0.24875458  0.06141238  0.03383441  0.02694567  0.30450952
   0.26927674]
 [ 0.05886969  0.2489734   0.04692164  0.03804833  0.02228998  0.31272331
   0.2721737 ]
 [ 0.06143956  0.23103251  0.06644449  0.03522859  0.0308139   0.31915924
   0.25588167]
 [ 0.06093203  0.2189928   0.06991831  0.0368057   0.03285444  0.3335079
   0.24698883]
 [ 0.05828327  0.26990092  0.04996233  0.03258191  0.02231927  0.28255072
   0.28440157]
 [ 0.05828327  0.26990092  0.04996233  0.03258191  0.02231927  0.28255072
   0.28440157]]

softmax_opt =  [ 3.05927634  3.24204803  3.32230186  1.05481279  1.31201565  3.26889801
  3.47978926  1.39841223  2.84244013  2.84244013]

loss_CE =  2.58224

optimizer =  None

training_prediction =  [[ 0.05886969  0.2489734   0.04692164  0.03804833  0.02228998  0.31272331
   0.2721737 ]
 [ 0.06260613  0.22845364  0.05545964  0.03908377  0.02710427  0.33109063
   0.25620189]
 [ 0.06380478  0.21204451  0.0758239   0.03621221  0.0360697   0.33539063
   0.2406543 ]
 [ 0.06228752  0.20364766  0.07645512  0.03799394  0.03674927  0.34825763
   0.23460887]
 [ 0.05526669  0.24875458  0.06141238  0.03383441  0.02694567  0.30450952
   0.26927674]
 [ 0.05886969  0.2489734   0.04692164  0.03804833  0.02228998  0.31272331
   0.2721737 ]
 [ 0.06143956  0.23103251  0.06644449  0.03522859  0.0308139   0.31915924
   0.25588167]
 [ 0.06093203  0.2189928   0.06991831  0.0368057   0.03285444  0.3335079
   0.24698883]
 [ 0.05828327  0.26990092  0.04996233  0.03258191  0.02231927  0.28255072
   0.28440157]
 [ 0.05828327  0.26990092  0.04996233  0.03258191  0.02231927  0.28255072
   0.28440157]]


popopopopopopopooppopopopopopopopooppopopopopopopopooppopopopopopopopoop


[[1 4 2 3 4 5]
 [1 3 4 0 0 0]]

[[4 2 3 4 5 6]
 [3 4 6 0 0 0]]

[6 3]

Using the new RNN State
x_lenarr =  [ 6.  3.]

batch_size =  2

init_state =  LSTMStateTuple(c=array([[-0.17453107,  0.18170479,  0.13560715],
       [-0.14755413,  0.30341703,  0.05169982]], dtype=float32), h=array([[-0.08735172,  0.09806846,  0.0721207 ],
       [-0.08132361,  0.16405913,  0.02307099]], dtype=float32))

new_state =  LSTMStateTuple(c=array([[-0.22182368,  0.19751309,  0.15687557],
       [-0.27444318,  0.403438  ,  0.07357191]], dtype=float32), h=array([[-0.11071506,  0.10662023,  0.08244694],
       [-0.15024152,  0.21589187,  0.03204491]], dtype=float32))

rnn_output =  [[-0.18548395  0.07938705  0.03904057]
 [-0.16878608  0.10478021  0.02739687]
 [-0.14259121  0.14118735 -0.00059091]
 [-0.07556446  0.22202508  0.01999066]
 [-0.12828277  0.23167242  0.03366933]
 [-0.11071506  0.10662023  0.08244694]
 [-0.19588223  0.13703665  0.01079514]
 [-0.11209948  0.20594102  0.02260025]
 [-0.15024152  0.21589187  0.03204491]
 [ 0.          0.          0.        ]
 [ 0.          0.          0.        ]
 [ 0.          0.          0.        ]]

hid_to_ouptut_layer =  [[-0.18512765  1.25265515 -0.26187307 -0.53990859 -1.01715314  1.57869518
   1.35960424]
 [-0.17324293  1.1975863  -0.21133746 -0.55281347 -0.95353234  1.57956481
   1.31446385]
 [-0.14709023  1.1027199  -0.15057637 -0.56395179 -0.86269116  1.58338571
   1.23759317]
 [-0.14613055  1.02406812  0.10044083 -0.66633284 -0.63751394  1.56236386
   1.16579962]
 [-0.17729157  0.99492776  0.07185397 -0.63123369 -0.6580416   1.58184159
   1.14653254]
 [-0.19163784  1.31371427 -0.0681904  -0.65374243 -0.89150143  1.53834021
   1.39781952]
 [-0.17517769  1.09548187 -0.20741302 -0.52255464 -0.91253579  1.60150957
   1.23622072]
 [-0.16003808  1.03586817  0.03120947 -0.62990987 -0.69995344  1.57419384
   1.17878449]
 [-0.18260065  1.00854695  0.01525584 -0.60496336 -0.70877922  1.58892405
   1.15985262]
 [-0.07681409  1.45392573 -0.23085979 -0.65837198 -1.03667879  1.49972892
   1.50625801]
 [-0.07681409  1.45392573 -0.23085979 -0.65837198 -1.03667879  1.49972892
   1.50625801]
 [-0.07681409  1.45392573 -0.23085979 -0.65837198 -1.03667879  1.49972892
   1.50625801]]

output_state =  [[ 0.05619436  0.23665398  0.05204303  0.03941062  0.02445393  0.32787719
   0.26336691]
 [ 0.058       0.22843967  0.05583207  0.039681    0.02657985  0.33470538
   0.25676203]
 [ 0.06145862  0.21447094  0.06124474  0.04050812  0.03004701  0.34683183
   0.24543875]
 [ 0.06272574  0.20214202  0.08026577  0.03728419  0.03837432  0.34628642
   0.23292163]
 [ 0.06123443  0.19773522  0.0785594   0.03889118  0.03786243  0.35561284
   0.23010451]
 [ 0.05462881  0.24614321  0.06180651  0.03441377  0.02713157  0.30813545
   0.26774064]
 [ 0.05977884  0.21300407  0.05788258  0.04223608  0.02859677  0.35330757
   0.24519411]
 [ 0.06171195  0.20405385  0.07471831  0.03857506  0.03596558  0.349572
   0.2354033 ]
 [ 0.06073772  0.19987907  0.07402638  0.03981334  0.03588741  0.35712636
   0.23252976]
 [ 0.05838089  0.26981279  0.05004602  0.03263648  0.02235665  0.28245848
   0.2843087 ]
 [ 0.05838089  0.26981279  0.05004602  0.03263648  0.02235665  0.28245848
   0.2843087 ]
 [ 0.05838089  0.26981279  0.05004602  0.03263648  0.02235665  0.28245848
   0.2843087 ]]

softmax_opt =  [ 3.71096444  2.88540697  3.20625281  3.26036692  1.03391266  1.31773651
  3.16448045  3.32519293  1.45873713  2.84076667  2.84076667  2.84076667]

loss_CE =  2.65711

optimizer =  None

training_prediction =  [[ 0.05619436  0.23665398  0.05204303  0.03941062  0.02445393  0.32787719
   0.26336691]
 [ 0.058       0.22843967  0.05583207  0.039681    0.02657985  0.33470538
   0.25676203]
 [ 0.06145862  0.21447094  0.06124474  0.04050812  0.03004701  0.34683183
   0.24543875]
 [ 0.06272574  0.20214202  0.08026577  0.03728419  0.03837432  0.34628642
   0.23292163]
 [ 0.06123443  0.19773522  0.0785594   0.03889118  0.03786243  0.35561284
   0.23010451]
 [ 0.05462881  0.24614321  0.06180651  0.03441377  0.02713157  0.30813545
   0.26774064]
 [ 0.05977884  0.21300407  0.05788258  0.04223608  0.02859677  0.35330757
   0.24519411]
 [ 0.06171195  0.20405385  0.07471831  0.03857506  0.03596558  0.349572
   0.2354033 ]
 [ 0.06073772  0.19987907  0.07402638  0.03981334  0.03588741  0.35712636
   0.23252976]
 [ 0.05838089  0.26981279  0.05004602  0.03263648  0.02235665  0.28245848
   0.2843087 ]
 [ 0.05838089  0.26981279  0.05004602  0.03263648  0.02235665  0.28245848
   0.2843087 ]
 [ 0.05838089  0.26981279  0.05004602  0.03263648  0.02235665  0.28245848
   0.2843087 ]]


popopopopopopopooppopopopopopopopooppopopopopopopopooppopopopopopopopoop


[Finished in 3.5s]